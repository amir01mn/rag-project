{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daa07a69",
   "metadata": {},
   "source": [
    "### RAG Pipelines - from Data ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec964f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amirmansouri/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb6dabee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: 3681780.3697252.pdf\n",
      "  ✓ Loaded 8 pages\n",
      "\n",
      "Processing: Comprehensive Survey of RAG.pdf\n",
      "  ✓ Loaded 18 pages\n",
      "\n",
      "Processing: 2411.06037v3.pdf\n",
      "  ✓ Loaded 25 pages\n",
      "\n",
      "Total documents loaded: 51\n"
     ]
    }
   ],
   "source": [
    "### first we need to read all pdfs inside the directory\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e8b5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content=\". \\n. \\nLatest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3681780.3697252\\n. \\n. \\nRESEARCH-ARTICLE\\nAutomating Bibliometric Analysis with Sentence Transformers and\\nRetrieval-Augmented Generation (RAG): A Pilot Study in Semantic and\\nContextual Search for Customized Literature Characterization for High-\\nImpact Urban Research\\nHAOWEN XU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nXUEPING LI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJOSE TUPAYACHI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJIANMING (JAMIE) LIAN, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nOLUFEMI A OMITAOMU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\n. \\n. \\nOpen Access Support provided by:\\n. \\nOak Ridge National Laboratory\\n. \\nThe University of Tennessee, Knoxville\\n. \\nPDF Download\\n3681780.3697252.pdf\\n28 January 2026\\nTotal Citations: 2\\nTotal Downloads: 314\\n. \\n. \\nPublished: 29 October 2024\\n. \\n. \\nCitation in BibTeX format\\n. \\n. \\nSIGSPATIAL '24: The 32nd ACM\\nInternational Conference on Advances in\\nGeographic Information Systems\\nOctober 29 - November 1, 2024\\nGA, Atlanta, USA\\n. \\n. \\nConference Sponsors:\\nSIGSPATIAL\\nUrbanAI '24: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI (October 2024)\\nh\\ue03cps://doi.org/10.1145/3681780.3697252\\nISBN: 9798400711565\\n.\"),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers\\nand Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature\\nCharacterization for High-Impact Urban Research\\nHaowen Xu\\nxuh4@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nXueping Li\\nXueping.Li@utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJose Tupayachi\\njtupayac@vols.utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJianming (Jamie) Lian\\nlianj@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nOlufemi A Omitaomu\\nomitaomuoa@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nABSTRACT\\nBibliometric analysis is essential for understanding research trends,\\nscope, and impact in urban science, especially in high-impact jour-\\nnals, such Nature Portfolios. However, traditional methods, relying\\non keyword searches and basic NLP techniques, often fail to uncover\\nvaluable insights not explicitly stated in article titles or keywords.\\nThese approaches are unable to perform semantic searches and\\ncontextual understanding, limiting their effectiveness in classifying\\ntopics and characterizing studies. In this paper, we address these\\nlimitations by leveraging Generative AI models, specifically trans-\\nformers and Retrieval-Augmented Generation (RAG), to automate\\nand enhance bibliometric analysis. We developed a technical work-\\nflow that integrates a vector database, Sentence Transformers, a\\nGaussian Mixture Model (GMM), Retrieval Agent, and Large Lan-\\nguage Models (LLMs) to enable contextual search, topic ranking,\\nand characterization of research using customized prompt tem-\\nplates. A pilot study analyzing 223 urban science-related articles\\npublished in Nature Communications over the past decade high-\\nlights the effectiveness of our approach in generating insightful\\nsummary statistics on the quality, scope, and characteristics of\\npapers in high-impact journals. This study introduces a new para-\\ndigm for enhancing bibliometric analysis and knowledge retrieval\\nin urban research, positioning an AI agent as a powerful tool for\\nadvancing research evaluation and understanding.\\nThis manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-\\n00OR22725 with the US Department of Energy (DOE). The US government retains\\nand the publisher, by accepting the article for publication, acknowledges that the\\nUS government retains a nonexclusive, paid-up, irrevocable, worldwide license to\\npublish or reproduce the published form of this manuscript, or allow others to do\\nso, for US government purposes. DOE will provide public access to these results\\nof federally sponsored research in accordance with the DOE Public Access Plan\\n(http://energy.gov/downloads/doe-public-access-plan).\\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\\nauthored or co-authored by an employee, contractor or affiliate of the United States\\ngovernment. As such, the Government retains a nonexclusive, royalty-free right to\\npublish or reproduce this article, or to allow others to do so, for Government purposes\\nonly.\\nUrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1156-5/24/10. . . $15.00\\nhttps://doi.org/10.1145/3681780.3697252\\nKEYWORDS\\nBibliometrics Analysis, Large Language Models, Retrieval-Augmented\\nGeneration, Transformers\\nACM Reference Format:\\nHaowen Xu, Xueping Li, Jose Tupayachi, Jianming (Jamie) Lian, and Olufemi\\nA Omitaomu. 2024. Automating Bibliometric Analysis with Sentence Trans-\\nformers and Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature Characteri-\\nzation for High-Impact Urban Research . In 2nd ACM SIGSPATIAL In-\\nternational Workshop on Advances in Urban-AI (UrbanAI’24), October 29-\\nNovember 1 2024, Atlanta, GA, USA. ACM, Seattle, WA, USA, 7 pages.\\nhttps://doi.org/10.1145/3681780.3697252\\n1 INTRODUCTION\\nBibliometric analysis is a widely used method for evaluating and\\nmapping research trends, impact, and scope across various scien-\\ntific domains [ 2]. It provides quantitative insights by analyzing\\npublication records, citations, and other scholarly outputs, helping\\nresearchers and policymakers understand the evolution of specific\\nfields [4]. Over the past few decades, bibliometric analysis has\\nevolved from basic citation counts and keyword frequency met-\\nrics to more sophisticated approaches, incorporating co-authorship\\nnetworks, citation flows, and research topic clusters [7]. These meth-\\nods are particularly important in fields like urban science, where\\nemerging topics such as smart cities require continuous monitor-\\ning to shape the direction of future research and innovation [ 5].\\nBibliometric analysis plays a key role in identifying influential\\nworks, emerging themes, and research gaps, thus guiding strategic\\ndecision-making in urban science and smart city development [15].\\nHowever, traditional bibliometric methods face several limita-\\ntions. Most rely heavily on keyword searches and basic text mining\\ntechniques, which depend on exact matches of terminologies and\\npredefined keywords. These techniques often miss critical insights\\nthat are not explicitly captured in the titles or abstracts of research\\narticles, thereby limiting the ability to fully understand and clas-\\nsify research topics [8]. Furthermore, traditional natural language\\nprocessing (NLP) approaches, such as term frequency-inverse doc-\\nument frequency (TF-IDF) or simple word co-occurrence metrics,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nfail to capture the semantic meaning and contextual relationships\\nbetween concepts[9]. Although topic modeling methods like La-\\ntent Dirichlet Allocation (LDA) can offer significant benefits for\\nbibliometric analysis by providing deeper insights into the rela-\\ntionships and structures within research literature [ 1], they are\\nprimarily used for uncovering thematic structures and classifying\\narticle topics and are not designed for enabling semantic search or\\nproviding a contextual understanding of an article that involves\\ndeeper reasoning and interpretation. As a result, traditional biblio-\\nmetric analysis often falls short in generating deeper insights that\\nrequire a thorough review and interpretation of the article’s full\\ncontent. Relying primarily on the analysis of titles, keywords, and\\nstandard metadata, limits the ability to provide a more customized\\nand nuanced characterization of research based on the full textual\\ndata.\\nRecent advancements in generative AI models, such as large\\nlanguage models (LLMs), have opened new opportunities for en-\\nhancing research [11]. These models, including transformers and\\nRetrieval-Augmented Generation (RAG) systems, excel at seman-\\ntic understanding and contextual interpretation of complex texts,\\nmaking them highly suitable for extracting valuable insights from\\nresearch articles and technical manuals [10, 14]. In the field of urban\\ninformatics, LLMs have been increasingly applied to analyze large\\nvolumes of text, uncovering patterns and trends that traditional\\nmethods would overlook [6, 13]. In this paper, we propose a novel\\ntechnical workflow for automating and enhancing bibliometric\\nanalysis by integrating Vector Databases, Sentence Transformers,\\nGaussian Mixture Models (GMM), Retrieval Agents, and an LLM.\\nOur approach enables contextual search, topic ranking, and cus-\\ntomized characterization of research articles, which we demonstrate\\nthrough a pilot study analyzing 201 urban science-related articles\\npublished in Nature Communications over the past decade. This\\nwork addresses the limitations of traditional bibliometric meth-\\nods, introducing a new paradigm for urban research analysis and\\nknowledge retrieval through the development of AI agents with\\ncontextual understanding and reasoning capabilities.\\n2 LITERATURE REVIEW\\nTo overcome the limitations and knowledge gaps in traditional\\nbibliometric analysis, recent studies have employed generative AI\\nmodels, particularly transformer-based language models, to auto-\\nmate and enhance bibliometric methodologies.\\nFijačko et al. [3] explores the application of generative AI in\\nbibliometric analysis, focusing on 10 years of research abstracts\\nfrom the European Resuscitation Congresses (ERC). Using ChatGPT-\\n4, the study classified 2,491 abstracts into ERC guideline topics, with\\nBasic Life Support and Adult Advanced Life Support being the most\\nfrequent. The research highlights the potential of large language\\nmodels like ChatGPT-4 in categorizing and analyzing scientific\\nliterature and identifying trends. However, challenges included\\npotential misclassification, the limited use of abstract titles rather\\nthan full-text, and heavy reliance on the model’s capabilities. These\\nconstraints highlight the challenges of automating bibliometric\\nanalysis in the absence of comprehensive datasets. However, the\\nstudy effectively showcases the potential of AI to significantly\\nimprove bibliometric methodologies despite these limitations.\\nWeng et al. [12] introduces a methodology for detecting and\\nvisualizing key research topics using GPT-3 embeddings and the\\nHDBSCAN clustering algorithm on 593 abstracts related to urban\\nstudies and machine learning. By clustering abstracts based on\\nsemantic similarity and extracting keywords using the Maximal\\nMarginal Relevance (MMR) algorithm, the study provides an in-\\nteractive tool for exploring abstract clusters and their associated\\ntopics. Challenges included optimizing clustering parameters and\\nrelying solely on abstracts, which may not fully represent the re-\\nsearch. Some clusters contained outliers or minimal data, affecting\\naccuracy. Despite these limitations, the study demonstrates the\\npotential of transformer-based models in facilitating unsupervised\\nbibliometric analysis, though refinement is needed.\\nBoth articles emphasize the benefits of transformer-based and\\nlarge language models for bibliometric analysis, while also address-\\ning critical limitations such as data quality, optimization challenges,\\nand input constraints when working with abstract-based datasets.\\nTo overcome these challenges, there is a need to harness recent\\nadvancements in sentence transformer models and RAG technolo-\\ngies. These innovations can enable the development of an AI agent\\ncapable of advanced contextual understanding of research articles,\\nfacilitating semantic search and providing tailored insights based on\\nuser-specific queries. This, in turn, can generate new bibliometric\\nmetrics, offering deeper and more comprehensive analysis.\\n3 METHODOLOGY\\nThis section starts by outlining the design requirements for our\\nproposed methods, then presents the conceptual workflow and its\\nimplementation, which combines Generative AI techniques with\\nstatistical models.\\n3.1 Design Requirements\\nOverall, we aim to develop an AI-agent styled tool that can interact\\nwith users, who are primarily researchers and college students,\\nthrough human nature conversations, to get their inquire on the\\ncurrent-state of cutting edge research in a specific domain, such as\\nsmart city and urban science. Based on the inquiry, our workflow\\nwill automate a sequence of procedures that leverage the unique\\ncapabilities of sentence transformers and RAG techniques on a\\nbatch of selected literature filtered and downloaded from academics\\ndatabases, such as Scopous, IEEE Xplore, and Web of Science. Aim-\\ning to shed lights on more advanced, intelligent, and automonous\\nbiblimetric analysis, our workflow aims to enable the following\\nfeatures:\\nConversational Interaction: A chatbot-style interface will\\nbe implemented, allowing users to ask questions through nat-\\nural human conversations, without the need for pre-defined\\nkeywords or technical jargon. This feature will enable users\\nto define search and filter criteria for subsets of bibliographic\\ndata (e.g., research articles, conference proceedings, techni-\\ncal reports, and manuals) that have been pre-selected and\\ndownloaded from popular academic databases. The search\\nprocess will be guided by broad categories, such as domains,\\ndisciplines, and journals, to streamline access to relevant\\nliterature.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nFigure 1: The overall design of the transformer and RAG-powered workflow.\\nSemantic and Contextual Search: Based on the user-defined\\ninquiry, this process matches and retrieves relevant research\\ndocuments or specific sections by analyzing the underly-\\ning meaning and contextual relationships between words,\\nrather than relying solely on keyword matching. The use of\\nsentence transformers and text embeddings, enables users\\nto access information and knowledge based on conceptual\\nrelevance, rather than simple term frequency. This enhances\\nthe precision of literature filtering and facilitates deeper,\\nmore insightful knowledge discovery, which will plays im-\\nportant role as the retrieval agent within the RAG paradigm\\nto benefit further analytics using Generative AI models.\\nCustomized Literature Characterization: Using the output\\nliterature from the semantic and contextual search as input,\\nGenerative Pre-trained Transformer (GPT) models will be\\nemployed for contextual understanding, reasoning, and in-\\nterpretation. These GPT models will process user inquiries\\nto generate customized characterizations and interpretations\\nof the selected literature, providing deeper insights and cre-\\nating more sophisticated metrics for advanced bibliometric\\nanalyses. This approach aims to enhance the overall un-\\nderstanding of research trends and offer tailored, in-depth\\nevaluations of the literature.\\nAs an example of our end-user capability, a user could ask the\\nchatbot, powered by our method, a question like, “What percentage\\nof research published in Computers, Environment and Urban Sys-\\ntems over the past 5 years in the urban mobility sector uses traffic\\nsimulation-based methods versus crowd-sourced data-driven meth-\\nods, and what are their spatial scales?” The semantic and contextual\\nsearch then filters and retrieves relevant articles based on the query,\\nranks them by relevance, and feeds them to the generative AI model.\\nThis enables advanced contextual understanding and reasoning to\\nprovide customized characterizations on individual research’s sim-\\nulation types and spatial scales, which involve information often\\nnot found in keywords or titles. These characterizations can be later\\nused to generate summary statics and insights to facilitate more\\ndetailed trend analysis and thematic mapping.\\n3.2 Workflow Design\\nOur workflow consists of four key procedures, as depicted in Figure\\n1. The workflow is later implemented in a Jupyter lab environment\\nusing Python-based libraries. Each procedure is detailed through\\nthe following following list.\\n3.2.1 Bibliography Selection and Data Preparation. In the first step,\\nusers select literature based on generic search criteria such as disci-\\npline, publication year, and journal name. Data is extracted from'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nacademic databases like Scopus, IEEE Xplore, and SerpApi using\\ntheir respective web services and platforms, or through custom-\\nbuilt web scrapers, such as those powered by SerpAPI. The retrieved\\ndata includes bibliographic summaries in CSV format and individ-\\nual articles in formats like PDF and HTML, which are then stored\\nin a file-based system for further processing.\\n3.2.2 Text Embedding and Data Warehousing. After retrieving the\\nessential documents, a Python script powered by PyPDF2 is used\\nto parse the bibliographic summaries, which include the list of\\ndownloaded articles along with supportive metadata (e.g., authors,\\nyear, source, citations, and h-index), as well as the PDF and HTML\\nversions of the individual articles. This parsing process is designed\\nto upload key textual information into a datastore, building the\\nknowledge base for the proposed AI agent. Unlike traditional infor-\\nmation and content management systems, our workflow utilizes\\na sentence transformer, specifically the all-MiniLM-L6-v2 model\\nfrom Hugging Face, to generate text embeddings—vector represen-\\ntations that encode the semantic and contextual meaning of the text.\\nCompared with traditional NLP methods, sentence transformers,\\nwith its unique self-attention mechanism, have superior advantages\\nin capturing semantic meaning, enabling contextual understanding,\\nhandling synonyms, and long-range dependencies between words\\nin a sentence. These embeddings facilitate more efficient semantic\\nand contextual searches in later stages of the workflow. The text\\nembeddings, along with essential metadata and article content, are\\nuploaded into the datastore. We selected Neo4j, a graph database, as\\nthe datastore for this workflow due to its graph data model, which\\nbetter represents the relationships between data entities stored as\\nnodes in the database. In our project, individual articles are repre-\\nsented as nodes within Neo4j, with associated metadata, content,\\nand text embeddings stored as properties of each node.\\n3.2.3 Semantic and Contextual Search. In the third step, the work-\\nflow enables semantic and contextual searches within the literature\\nstored in the knowledge base, leveraging the Neo4j database and\\nsentence transformers. User queries, collected through a chatbot\\ninterface, serve as input for this advanced search. The core function-\\nality compares the text embeddings of the user queries with those\\nof the article contents. We employ an enhanced cosine similarity\\nanalysis, as described in Eq. 1, to calculate a similarity score ranging\\nfrom 0 to 1, where 0 represents complete irrelevance and 1 repre-\\nsents high relevance. Our implementation extends the standard\\ncosine similarity formula by using Python to chunk the original\\narticle content into sections and paragraphs, enabling more granu-\\nlar comparisons between the query and specific parts of the article.\\nThis process is applied to each article in the database, generating a\\nsimilarity score based on semantic similarity with the user’s query.\\nAt the contextual level, the framework evaluates the query’s con-\\ntext and intelligently selects embeddings from different sections of\\nthe articles to perform a targeted and accurate search.\\nSimilarity Score = a · b\\n∥a∥ ∥b∥ (1)\\nTo draw the decision boundary based on a list of individual\\narticle’s similarity score, we employed GMM to rank and cluster\\narticles by their similarity score, which reflects their relevance. A\\nGMM is a probabilistic model that represents a distribution of data\\nas a mixture of multiple Gaussian (normal) distributions, each char-\\nacterized by its own mean and variance, making it effective for\\nmodeling complex, multimodal datasets. We employed the Akaike\\nInformation Criterion (AIC) and Bayesian Information Criterion\\n(BIC), alongside the elbow method, to determine the optimal num-\\nber of clusters for the Gaussian Mixture Model (GMM) analysis.\\nAfter the clustering analysis, the cluster with the highest average\\nsimilarity scores implies it contains the most relevant articles, which\\nare also further ranked based on its similarity score.\\nArticles in the top-ranked clusters are subsequently fed into\\ngenerative AI models, specifically GPT, to enable more in-depth\\nanalysis and interpretation. The semantic and contextual search\\nwithin this workflow is a critical component of the RAG paradigm,\\nallowing for further subsetting and refining of input information\\nto ensure more accurate and relevant results. This process also\\nhelps prevent exceeding the token limits of the GPT model context\\nwindow by optimizing the selection of input texts.\\n3.2.4 Customized Article Characterization. The top-ranked clus-\\nters, containing the most relevant articles, are then imported into a\\nGPT model as an external knowledge source to generate customized\\ncharacterizations for each article. These tailored bibliographic char-\\nacteristics serve as metrics, providing more detailed descriptions\\nand classifications of the articles. This approach uncovers valuable\\ninsights into research trends, focusing on individual articles’ topics,\\ntechnologies, methods, and contributions.\\nWe leverage the contextual reasoning capabilities of large lan-\\nguage models to classify and justify findings based on the semantic\\nmeaning of sections and paragraphs within the articles, extracting\\nuseful information without relying on precise names or keywords.\\nThis process is guided by instructional prompting strategies, where\\nwe design engineered prompt templates and feed them into the\\nGPT model along with the relevant article text segments (specific\\nsections). These segments are further refined and filtered based\\non their content relevance to ensure accurate classification and\\nextraction.\\nAt the technical level, we explored and tested the capabilities\\nof two GPT models, including a local instance of EleutherAI/gpt-\\nneo-1.3B models and the ChatGPT-3.5 Turbo API. Our experiments\\nreveals that small models with on 1.3B parameters suffer from\\nsevere hallucination, and are unable to analyze large size of tokens.\\nThe ChatGPT-3.5 API demonstrates stable performance, particularly\\nin its ability to process large text segments efficiently and produce\\nreasoned characterizations.\\n4 PILOT STUDY\\nThis pilot study aims to demonstrate the feasibility and performance\\nof our proposed methods. For this study, we compiled a dataset\\nof 223 high-impact urban research articles published in Nature\\nCommunications, obtained through the following Scopus query:\\nTITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management”\\nOR “urban planning” ) AND SRCTITLE ( “Nature Communications”\\n) AND PUBYEAR > 2013. We preprocessed the dataset by removing\\nall intermediate versions labeled as “Author Correction” or “Pub-\\nlisher Correction. ” The final dataset consists of a CSV file containing\\nbibliometric summaries with all Scopus fields selected, along with\\n223 individual PDF documents of the actual articles.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n(a) Semantic and contextual search based on user’s query.\\n(b) Customized Literature Characterization using GPT’s reasoning capability.\\nFigure 2: Demonstration of the workflow through two use cases.\\n4.1 Use Case Demonstration\\nOur first use case on semantic and contextual search is demon-\\nstrated in Figure 2a, where the user submits an inquiry to identify\\narticles related to urban green space. The chatbot responds by visu-\\nalizing a histogram of similarity scores for all articles and displaying\\nthe GMM clusters of the articles. Additionally, a link is provided to'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\ndownload a CSV file that ranks and clusters the articles based on\\ntheir relevance to the user’s query.\\nOur second use case builds on the output articles from the first\\nuse case and demonstrates the capability to generate customized\\nliterature characterizations, creating new metrics for bibliometric\\nanalysis. Through the chatbot interface, users specify requests and\\ninstructions via prompts to guide the GPT model in generating tai-\\nlored metrics. Examples of these prompts are illustrated in Figure\\n2b. Based on the prompts, the GPT processes the 67 retrieved arti-\\ncles and their critical content, leveraging its contextual reasoning\\nability to derive new literature characteristics, which can then be\\ndeveloped into metrics and summary statistics. Figure 2b also visu-\\nalizes the responses to the user’s queries using pie charts and box\\nplots. Users can submit additional questions and custom requests\\nthrough the chatbot to extract information and develop unique\\nmetrics tailored to the needs of bibliometric analysis.\\nOur major contribution lies in the development of an autonomous\\nAI agent designed to assist researchers in automating the charac-\\nterization and information extraction of large volumes of literature,\\nincluding datasets exceeding 1,000 articles. This system enables the\\ngeneration of in-depth insights for bibliometric analysis, signifi-\\ncantly enhancing the scalability and depth of literature review and\\nresearch trend identification processes. By automating these tasks,\\nthe AI agent offers a powerful tool for efficiently managing and\\nanalyzing extensive collections of scholarly articles, ultimately fa-\\ncilitating more comprehensive and insightful bibliometric analyses.\\n4.2 Limitation and Future Work\\nDeveloped as a prototype for a more advanced knowledge base and\\nmanagement system, our workflow still faces a few limitations, as\\nthe following:\\nToken Size Limitation: The current implementation using\\nthe ChatGPT API has a maximum token size limitation and\\nincurs service fees based on the number of tokens processed.\\nThis makes it less suitable for analyzing large volumes of\\nliterature.\\nDatabase Query Performance: The current datastore imple-\\nmentation using the Neo4j database may encounter chal-\\nlenges in querying and managing large volumes of embed-\\nding data, as Neo4j is not optimized as a dedicated vector\\ndatabase.\\nLack of Evaluation and Validation: The GPT-generated lit-\\nerature characteristics are not currently evaluated by human\\nexperts, which introduces uncertainty regarding their accu-\\nracy and reliability.\\nAs future work to address these limitations, we propose several\\nexperimental solutions. These include (a) deploying a local version\\nof large language models, such as GPT-Neo, to minimize service\\nfees for large-scale data analysis, (b) fine-tuning the GPT model\\nto reduce unnecessary content and instructions sent to the model,\\nthereby mitigating token size limitations, (c) transitioning our data-\\nstore implementation to dedicated vector databases, such as FAISS\\nor Pinecone, to enhance latency and accuracy, and (d) developing a\\ncomprehensive strategy to evaluate the GPT’s performance in ana-\\nlyzing and characterizing literature. Additionally, more advanced\\nbibliometric analysis methods could be integrated into the current\\nworkflow to extend its analytical capabilities.\\n5 CONCLUSION\\nIn this paper, we have presented a novel workflow that integrates\\ngenerative AI models and advanced analytical techniques through\\nthe RAG paradigm to address the limitations of traditional biblio-\\nmetric analysis methods. By leveraging the contextual reasoning\\ncapabilities of large language models and enhanced semantic search\\ntechniques, our system offers a more nuanced and insightful analy-\\nsis of research literature. This approach, demonstrated through the\\nanalysis of urban science-related articles, enables customized char-\\nacterizations and generates new metrics for bibliometric analysis,\\nproviding deeper insights into research trends, methodologies, and\\ncontributions.\\nOur pilot study demonstrates the feasibility of this workflow,\\nshowcasing its ability to facilitate advanced semantic and contex-\\ntual searches, cluster relevant articles, and produce tailored bib-\\nliographic insights through generative AI. However, the current\\nimplementation faces challenges, including token size limitations,\\ndatabase query performance issues, and the lack of expert evalua-\\ntion for the AI-generated results.\\nTo address these limitations, future work will explore the de-\\nployment of local language models, fine-tuning of GPT models to\\noptimize token usage, and transitioning to vector databases like\\nFAISS or Pinecone to improve performance. Additionally, we aim\\nto establish a comprehensive validation framework involving hu-\\nman experts to ensure the accuracy and reliability of the gener-\\nated bibliometric insights. As advancements in AI and bibliometric\\nmethodologies continue, our workflow has the potential to serve as\\na powerful and autonomous tool for researchers and policymakers\\nseeking to analyze and interpret vast bodies of scientific literature\\nmore effectively.\\n6 ACKNOWLEDGMENTS\\nThis work was supported by the U.S. Department of Energy (U.S\\nDOE), Advanced Research Projects Agency–Energy (ARPA-E) un-\\nder the project #DE-AR0001780. We thank our collaborators from\\nthe University of Tennessee Knoxville.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nREFERENCES\\n[1] Xieling Chen and Haoran Xie. 2020. A structural topic modeling-based biblio-\\nmetric study of sentiment analysis literature. Cognitive Computation 12 (2020),\\n1097–1129.\\n[2] Naveen Donthu, Satish Kumar, Debmalya Mukherjee, Nitesh Pandey, and\\nWeng Marc Lim. 2021. How to conduct a bibliometric analysis: An overview and\\nguidelines. Journal of business research 133 (2021), 285–296.\\n[3] Nino Fijačko, Ruth Masterson Creber, Benjamin S Abella, Primož Kocbek, Špela\\nMetličar, Robert Greif, and Gregor Štiglic. 2024. Using generative artificial intelli-\\ngence in bibliometric analysis: 10 years of research trends from the European\\nResuscitation Congresses. Resuscitation Plus 18 (2024), 100584.\\n[4] Ye-na Gan, Duo-duo Li, Nicola Robinson, and Jian-ping Liu. 2022. Practical guid-\\nance on bibliometric analysis and mapping knowledge domains methodology–A\\nsummary. European Journal of Integrative Medicine 56 (2022), 102203.\\n[5] Yi-Ming Guo, Zhen-Ling Huang, Ji Guo, Hua Li, Xing-Rong Guo, and Mpeoane Ju-\\ndith Nkeli. 2019. Bibliometric analysis on smart cities research. Sustainability 11,\\n13 (2019), 3606.\\n[6] Jianyuan Liang, Anqi Zhao, Shuyang Hou, Fengying Jin, and Huayi Wu. 2024. A\\nGPT-enhanced framework on knowledge extraction and reuse for geographic\\nanalysis models in Google Earth Engine. International Journal of Digital Earth\\n17, 1 (2024), 2398063.\\n[7] Luis Javier Cabeza Ramírez, Sandra M Sánchez-Cañizares, and Fernando J Fuentes-\\nGarcía. 2019. Past themes and tracking research trends in entrepreneurship: A\\nco-word, cites and usage count analysis. Sustainability 11, 11 (2019), 3121.\\n[8] Rodrigo Romero-Silva and Sander De Leeuw. 2021. Learning from the past\\nto shape the future: A comprehensive text mining analysis of OR/MS reviews.\\nOmega 100 (2021), 102388.\\n[9] Iqra Safder and Saeed-Ul Hassan. 2019. Bibliometric-enhanced information\\nretrieval: a novel deep feature engineering approach for algorithm searching\\nfrom full-text publications. Scientometrics 119 (2019), 257–277.\\n[10] Jose Tupayachi, Haowen Xu, Olufemi A Omitaomu, Mustafa Can Camur, Aliza\\nSharmin, and Xueping Li. 2024. Towards Next-Generation Urban Decision Sup-\\nport Systems through AI-Powered Construction of Scientific Ontology Using\\nLarge Language Models—A Case in Optimizing Intermodal Freight Transporta-\\ntion. Smart Cities 7, 5 (2024), 2392–2421.\\n[11] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large\\nlanguage model based autonomous agents. Frontiers of Computer Science 18, 6\\n(2024), 186345.\\n[12] Min-Hsien Weng, Shaoqun Wu, and Mark Dyer. 2022. Identification and visual-\\nization of key topics in scientific publications with transformer-based language\\nmodels and document clustering methods. Applied Sciences 12, 21 (2022), 11220.\\n[13] Haowen Xu, Femi Omitaomu, Soheil Sabri, Sisi Zlatanova, Xiao Li, and Yongze\\nSong. 2024. Leveraging Generative AI for Urban Digital Twins: A Scoping Review\\non the Autonomous Generation of Urban Data, Scenarios, Designs, and 3D City\\nModels for Smart City Advancement. arXiv preprint arXiv:2405.19464 (2024).\\nhttps://doi.org/10.48550/arXiv.2405.19464 arXiv:2405.19464 [cs.AI] Computer\\nScience > Artificial Intelligence.\\n[14] Haowen Xu, Jinghui Yuan, Anye Zhou, Guanhao Xu, Wan Li, Xinyue Ye, et al. 2024.\\nGenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities\\nand Challenges for Integrating Large Language Models (LLMs) and Retrieval-\\nAugmented Generation (RAG) with Intelligent Transportation Systems. arXiv\\npreprint arXiv:2409.00494 (2024).\\n[15] Li Zhao, Zhi-ying Tang, and Xin Zou. 2019. Mapping the knowledge domain of\\nsmart-city research: A bibliometric and scientometric analysis. Sustainability 11,\\n23 (2019), 6648.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, CurrentLandscapeandFutureDirections\\nShailjaGupta(CarnegieMellonUniversity, USA)RajeshRanjan(CarnegieMellonUniversity, USA)SuryaNarayanSingh(BITSindri, India)\\nAbstract\\nThis paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing itsevolution fromfoundational concepts to the current state of theart. RAGcombinesretrieval mechanismswith generative language modelstoenhancetheaccuracyof outputs, addressingkeylimitationsof LLMs.Thestudyexploresthebasicarchitectureof RAG, focusingonhowretrieval andgenerationareintegratedto handle knowledge-intensive tasks. Adetailed reviewof the significant technological advancements inRAG is provided, including key innovations in retrieval-augmented language models and applicationsacross various domains such as question-answering, summarization, and knowledge-based tasks.Recent research breakthroughs are discussed, highlighting novel methods for improving retrievalefficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethicalconcerns in deployment. Future research directions are proposed, with a focus on improving therobustness of RAGmodels, expanding the scope of application of RAGmodels, andaddressingsocietalimplications. This survey aims to serve as a foundational resource for researchers and practitioners inunderstandingthepotential of RAGanditstrajectoryinthefieldof natural languageprocessing.\\nFigure1: TrendsinRAGcapturedfromrecent researchpapers\\nKeywords: Retrieval-Augmented Generation (RAG), InformationRetrieval, Natural LanguageProcessing(NLP), Artificial Intelligence(AI), MachineLearning(ML), LargeLanguageModel (LLM).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Introduction\\n1.1Introductionof Natural LanguageGeneration(NLG)\\nNatural Language Processing (NLP) has become a pivotal domain within artificial intelligence (AI), withapplications ranging from simple text classification to more complex tasks such as summarization,machinetranslation, andquestionanswering. Aparticularlysignificant branchof NLPisNatural LanguageGeneration (NLG), which focuses on the production of human-like language from structured orunstructured data. NLG's goal is to enable machines to generate coherent, relevant, and context-awaretext, improvinginteractionsbetweenhumansandmachines(Gatt et. al. 2018). AsAI evolves, thedemandfor more contextually awareandfactuallygroundedgeneratedcontent hasincreased, bringingabout newchallengesandinnovationsinNLG.\\nTraditional NLG models, especially sequence-to-sequence architectures (Sutskever et al. 2014), haveexhibited significant advancements ingeneratingfluent andcoherent text. However, thesemodelstendtorely heavily on training data, often struggling when tasked with generating factually accurate orcontextually rich content for queries that require knowledge beyond their trainingset. Asaresult, modelslike GPT (Radford et al. 2019) or BERT-based (Devlin et al. 2019) text generators are prone tohallucinations, where they produceplausiblebut incorrect or non-existent information(Ji et al. 2022). Thislimitation has prompted the exploration of hybrid models that combine retrieval mechanisms withgenerative capabilities to ensure both fluency and factual correctness in outputs. There has been asignificant rise in several research papers in this field and several new methods across the RAGcomponents have been proposed. Apart from new algorithms and methods, RAGhas also seen steepadoption across various applications. However, there is a gapinasufficient surveyof thisspacetrackingtheevolutionandrecent changesinthisspace. Thecurrent surveyintendstofill thisgap.\\n1.2Overviewof Retrieval-AugmentedGeneration(RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address thelimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,which retrieves relevant documents or information from an external knowledge source, and (ii) ageneration module, which processesthisinformationtogeneratehuman-liketext (Lewiset al. 2020). Thiscombination allows RAG models to not only generate fluent text but also ground their outputs inreal-world, up-to-datedata.\\nThe retrieval module in RAG typically leverages dense vector representations to identify relevantdocuments from large datasets, such as Wikipedia or proprietary databases. Once retrieved, thesedocuments are passed to the generative module, often built using transformer-based architectures, togenerate responses grounded in the retrieved knowledge. This methodology helps mitigate thehallucination problem and ensures that the generated text is more factual and contextually appropriate(Thakur et al. 2021). Over the period, RAGmodels have seen applicationsinvariousdomains, includingopen-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), andpersonalizedrecommendations.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure2: Abasicflowof theRAGsystemalongwithitscomponent\\n1.3Evolutionof HybridModelsinNLP\\nBefore the introduction of RAG, NLPmodelsprimarilyreliedoneither retrieval or generationapproaches,each with its own set of advantages and limitations. Retrieval-based systems, such as traditionalinformation retrieval engines (Salton et al., 1975), efficiently provided relevant documents or snippets inresponse to a query but could not synthesize new information or present the results in a coherentnarrative. On the other hand, purely generative models, which became popular with the rise oftransformer architectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factualaccuracy.\\nThe development of hybrid systems combining retrieval and generation began to gain momentum asresearchers recognizedthecomplementarystrengthsof bothapproaches. Earlyeffortsinhybridmodelingcan be traced back to works like DrQA(Chen et al. 2017), which employed retrieval techniques to fetchrelevant documents for question-answering tasks. However, the generative component in such systemswas minimal, often limited to selecting text directlyfromtheretrieveddocuments. Similarly, inmodelslikeInformationRetrieval (Dai et al. 2019), retrieval wastreatedasdistinct, independent components.\\nThe real innovation came with the realization that retrieval and generation could be tightly integrated.Models like REALM (Guu et al., 2020) represented a key milestone, as they trained the retrieval andgenerative components jointly, enabling better alignment between the retrieved information and thegenerated output. RAG (Lewis et al. 2020) further extended this paradigm by using dense passageretrieval (Karpukhin et al., 2020) to fetch relevant documents and transformers like BART (Lewis et al.,2020) for a generation. This architecture provided a more seamless integration of retrieval andgeneration, allowingthemodel toanswer open-endedquestionswithbothfluencyandfactual grounding.\\n1.4Importanceof FactuallyGroundedLanguageGeneration\\nOne of the main motivations for developing RAG is the increasing demand for factually accurate,contextually relevant, and up-to-date generated content. Inmanyapplications, suchascustomer service,medical diagnostics, or legal advisory systems, the need for reliable and grounded responses isparamount. Generative models that produce hallucinated or inaccurate information can lead to seriousconsequences, suchasspreadingmisinformationor providingincorrect advice(Ji et al. 2022).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"RAG models directly address these concerns by grounding their generative process in external,up-to-date knowledge sources. This groundingimprovesthefactual accuracyof theoutput andenhancesthe relevance of responses by incorporating real-world data that is directly tiedtothequery. Additionally,RAGmodels are less likely to propagate biases present in static training data, astheycanretrievemorediverseandbalancedinformationfromexternal sources\\n1.5Applicationsof RAGModels\\nRAG models have been applied across a wide array of domains where factual accuracy and contextualunderstanding are critical. One of themost prominent applicationsisinopen-domainquestionanswering,where the model must generate answers based on a wide range of topics. RAGhas proven effective inimproving answer accuracy byretrievingrelevant informationandthengeneratingresponsesgroundedinthat data (Izacard et. al. 2021). Models like Dense PassageRetrieval (DPR) (Karpukhinet al., 2020) andFusion-in-Decoder (Izacardet. al. 2021) havebeenusedtogreat effect inthiscontext, showingsignificantimprovementsover traditional generativeor retrieval-onlymodels.\\nIn conversational AI, RAGmodels have enhanced the capabilities of dialogue systems by ensuring thatresponses are both coherent and grounded in factual information (Roller et al., 2020). For example,chatbots used in customer service can benefit fromRAG's ability to retrievespecificdetailsfromproductdatabasesor documentation, leadingtomoreaccurateanduseful responsesfor end-users.\\nOther applications include medical diagnosis systems, where RAGcan retrieve and integrate the latestresearch findings or patient-specific data togenerateaccuratediagnosticsuggestions, andlegal advisorysystems, where the model can retrieve relevant case law or statutes to provide legally sound advice.Furthermore, RAGhasfoundapplicationsinpersonalizedrecommendationsystems, whereit canretrieveuser preferencesor past interactionsandgeneratepersonalizedsuggestions.\\n1.6ChallengesandLimitationsof RAG\\nDespite the promise of RAGmodels, several challenges need attention. The retrieval mechanism, whilepowerful, can still struggle with retrieving the most relevant documents, particularly when dealing withambiguous queries or niche knowledge domains. The reliance on dense vector representations, suchasthose used in DPR, can sometimes lead to irrelevant or off-topic documents being retrieved. Efforts torefine retrieval techniques, including the incorporation of more sophisticated query expansion andcontextual disambiguation, are needed to improve performance in these areas. The integration betweenretrieval and generation, while seamless in theory, can sometimes fail in practice. For instance, thegenerative module may not always effectively incorporate the retrieved information into its responses,leading to inconsistencies or incoherence between the retrieved facts and the generated text. Researchinto better alignment mechanisms, such as improved attention models or hierarchical fusion techniques,may help alleviate these issues (Izacard et. al. 2021). Additionally, the computational overhead of RAGmodels is a concern, as they require both a retrieval and a generation step for each query. This dualprocess can be resource-intensive, particularly for large-scale applications (Borgeaud et al. 2021).Techniques such as model pruning (Han et al. 2015) or knowledge distillation (Sanh et al., 2019) mayoffer ways to reduce the computational burden without sacrificing performance. Finally, there are ethicalconcerns associated with the deployment of RAGmodels, particularly in termsof biasandtransparency.Biases in AI and LLM have been a well-researched and evolving field with researchers identifyingdifferent types of biases not limited to Gender, socio-economic class, or even educational background(Gupta et. al. 2024; Ranjan et. al., 2024). While RAG has the potential to reduce biases by retrievingmore balanced information, there is still the risk of amplifying biases present in the retrieved sources\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"(Binns, 2018). Furthermore, ensuring transparency in how retrieval results are selected and used ingenerationiscrucial for maintainingtrust inthesesystems.\\n1.7Scopeof theSurvey\\nThis paper aims to provide a comprehensive survey of RAG models, covering their evolution, keyarchitectural components, recent research in this area, current challenges and limitations of RAG, andfutureresearchdirection.\\n2: CoreComponentsandArchitectural Overviewof RAGSystems\\n2.1Overviewof RAGModels\\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments naturallanguage generation (NLG) with external retrieval mechanisms to enhance themodel'sknowledgebase.Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vastcorpora, rely entirely on their internal representations of knowledge, making themsusceptible to issueslike hallucinations—where the models generate plausible but incorrect information. Thesemodelscannotefficiently update their knowledge bases without retraining, making them less practical for dynamic,knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al.2020). Toovercometheselimitations, thepaper (Lewiset al. 2020) proposedtheRAGarchitecture, whichretrievesreal-time, relevant external documentstogroundthegeneratedtext infactual information.\\nTheRAGmodel incorporatestwokeycomponents:\\n1. Retriever: This retrieves the most relevant documents froma corpus using techniques such asdensepassageretrieval (DPR) (Karpukhinet. al. 2020) or traditional BM25algorithms.2. Generator: It synthesizes the retrieved documents into coherent, contextually relevantresponses.\\nRAG’s strength lies in its ability to leverage external knowledge dynamically, allowing it to outperformgenerative models like GPT-3andknowledge-groundedsystemslikeBERT, whichrelyonstaticdatasets.In open-domain question answering, RAG has been demonstrated to be highly effective, consistentlyretrievingrelevant informationandimprovingthefactual accuracyof thegeneratedresponses(Guu, K., etal. 2020). In addition to knowledge retrieval, RAGmodels excel at updating knowledgebases. Sincethemodel fetches external documents for each query, it requires no retraining to incorporate the latestinformation. This flexibility makes RAG models particularly suitable for domains where information isconstantly evolving, such as medical research, financial news, and legal proceedings. Furthermore,studies have shown that RAGmodels achieve superior results in a varietyof knowledge-intensivetasks,includingdocument summarizationand, knowledge-groundeddialogues\\n2.2Retriever MechanismsinRAGSystems\\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.Effective retrieval ensures that the model's output is grounded in accurate information. Several retrievalmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticatedtechniqueslikeDensePassageRetrieval (DPR).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2.2.1BM25\\nBM25 is a well-established informationretrieval algorithmthat usesthetermfrequency-inversedocumentfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25remains a strong baseline for many modern retrieval systems, including those used in RAG models.BM25 calculates therelevancescoreof adocument basedonhowfrequentlyaquerytermappearsinthedocument while adjusting for the document's length and the frequency of the term across the corpus(Robertson et. al. 2009). WhileBM25iseffectivefor keywordmatching, it haslimitationsinunderstandingsemantic meaning. For example, BM25 cannot capture the relationships between words and tends toperform poorly on more complex, natural language queries that require an understanding of context.Despite this limitation, BM25 is still widely used because of itssimplicityandefficiency. BM25iseffectivefor tasks involvingsimpler, keyword-basedqueries, althoughmoremodernretrieval modelslikeDPRtendtooutperformit insemanticallycomplextasks.\\n2.2.2DensePassageRetrieval (DPR)\\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modernapproach to information retrieval. It uses a dense vector space in which both the query and thedocuments are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, wherethe query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xionget. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query anddocuments, making it highly effective for open-domain question-answering tasks. The strength of DPRlies in its ability to retrieve relevant information based on semantic meaning rather than keywordmatching. By training the retriever on a large corpus of question-answer pairs, DPRcan finddocumentsthat are contextually related to the query, even when the query and the document do not share exactterms. Recent research has further improved DPRbyintegratingit withpre-trainedlanguagemodelsandanexampleisLLMadaptedfor thedenseRetrievAl approach(Li et. al. 2023)\\n2.2.3REALM(Retrieval-AugmentedLanguageModel)\\nAnother significant advancement in retrieval mechanisms for RAGmodels is REALM(Guu et al. (2020).REALM integrates retrieval into the language model's pre-training process, ensuring that the retriever isoptimized alongside the generator for downstreamtasks. ThekeyinnovationinREALMisthat it learnstoretrieve documents that improve the model’s performance on specific tasks, suchasquestionansweringor document summarization. During training, REALM updates both the retriever and the generator,ensuring that the retrieval process is optimized for the generation task. REALM’s retriever is trained toidentify documents that are not only relevant to the query but also helpful for generating accurate andcoherent responses. As a result, REALM significantly improves the quality of generated responses,particularly in tasks that require external knowledge. Recent studies have demonstrated that REALMoutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval istightlycoupledwithgeneration.\\nThe core of RAG lies in the quality of retrieved passages, but many current methods rely onsimilarity-based retrieval (Mallen et al. 2022). Self-RAG (Asai et al. 2023b), and REPLUG (Shi et al.,2023) have advanced by leveraging LLMs to enhance retrieval capabilities, achieving more adaptiveretrieval. After initial retrieval, cross-encoder models are used to re-rank the retrieved results by jointlyencoding the query and each retrieved document to compute relevance scores. These models providemore context-aware retrieval at the cost of higher computational overhead. Pointwise and PairwiseRanking, often based on Learning-to-Rank (LTR) algorithms, are used to assign relevance scores to\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"retrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAGsystems utilizeself-attentionwithintheLLMtomanagecontext andrelevanceacrossdifferent partsof theinput and retrieved text. Cross-attentionmechanismsareusedwhenintegratingretrievedinformationintothe generative model, ensuring that the most relevant pieces of information are emphasized duringgeneration.\\n2.3Generator MechanismsinRAGSystems\\nIn Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role inproducing the final output by integrating retrieved information with the input query. After the retrievalcomponent pulls relevant knowledge from external sources, the generator synthesizes this informationinto coherent, contextually appropriate responses. The Large Language Model (LLM) serves as thebackbone of the generator, which ensures the generated text is fluent, accurate, and aligned with theoriginal query.\\n2.3.1T5(Text-to-Text Transfer Transformer)\\nT5 (Text-to-Text Transfer Transformer) (Raffel et al. 2020) is one of the most commonlyusedmodelsforgeneration tasks in RAGsystems. T5isversatileinitsapproach, framingeveryNLPtaskasatext-to-texttask. This uniform framework allows T5 to be fine-tuned for a wide range of tasks, includingquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,T5-based RAG models have been shown to outperform traditional generative models like GPT-3 andBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.Moreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAGsystemsthat needtotackleadiverserangeof knowledge-intensivetasks.\\n2.3.2BARTBART(Bidirectional andAuto-RegressiveTransformer), introducedbyLewiset al. (2020), isanotherprominent generativemodel usedinRAGsystems. BARTisparticularlywell-suitedfor tasksinvolvingtextgenerationfromnoisyinputs, suchassummarizationandopen-domainquestionanswering. Asadenoisingautoencoder, BARTcanreconstruct corruptedtext sequences, makingit robust for tasksthatrequirethegenerationof coherent, factual outputsfromincompleteor noisydata. Whenpairedwitharetriever inaRAGsystem, BARThasbeenshowntoimprovethefactual accuracyof generatedtext bygroundingit inexternal knowledge. Studieshavedemonstratedthat BART-basedRAGmodelsachievestate-of-the-art resultsinvariousknowledge-intensivetasks, includingdialoguegenerationandnewssummarization.\\n3. Retrieval-AugmentedGenerationModelsAcrossDifferent Modalities\\n3.1 Text-Based RAG Models: Text-based RAG models represent the most mature and widelyresearched category. These models leverage textual data for both retrieval and generation tasks,enabling applications such as question-answering, summarization, and conversational agents.Transformer architectures, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), arefoundational in text-based RAG models. These models utilize self-attention mechanisms to capturecontextual relationships within text, which enhances both retrieval accuracy and generation fluency.Dense retrieval models, such as those using dense embeddings fromBERT, offer superior performancecompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leveragedense representations to retrieve relevant documents more effectively. Recent advancements focus onintegrating retrieval and generation into a single training pipeline. REALM (Guu et al., 2020) is an\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='example of such anend-to-endmodel that jointlyoptimizesretrieval andgenerationprocesses, improvingoverall taskperformance.\\n3.2 Audio-Based RAGModels: Audio-based RAGmodels extend the principles of retrieval-augmentedgeneration to the audio modality, enablingapplicationssuchasspeechrecognition, audiosummarization,and conversational agents in voice interfaces. Audiodataisoftenrepresentedusingembeddingsderivedfrom pre-trained models like Wav2Vec 2.0 (Baevski et al., 2020). These embeddings serve as input toretrieval andgenerationcomponents, enablingthemodel tohandleaudiodataeffectively.\\n3.3 Video-Based RAG Models: Video-based RAG models incorporate both visual and textualinformation to enhance performance in tasks such as video understanding, captioning, and retrieval.Video data is represented using embeddings from models like I3D (Xie et. al. 2017) or TimeSformer(Bertasius et al. 2021). These embeddings capture temporal and spatial features essential for effectiveretrieval andgeneration.\\n3.4 Multimodal RAG Models: Multimodal RAG models integrate data from multiple modalities—text,audio, video, and images—to provide a more holistic approach to retrieval andgenerationtasks. Modelslike Flamingo (Alayrac et al., 2022) integrate multiple modalities into a unified framework, enablingsimultaneous processing of text, images, and videos. Techniques for cross-modal retrieval involveretrievingrelevant informationacrossdifferent modalities(Li. et. al. 2023).\\nMultimodal capabilities enhance the versatility and efficiency of RAG across various applications.”Retrieval as generation” (Wang et. al. 2024) extends the Retrieval-Augmented Generation (RAG)framework tomultimodal applicationsbyincorporatingtext-to-imageandimage-to-text retrieval. Utilizingalarge dataset of pairedimagesandtext descriptions, thesystemacceleratesimagegenerationwhenuserqueries align with stored text descriptions (\"retrieval as generation\"). The image-to-text functionalityallowsuserstoengageindiscussionsbasedoninput images.\\nFigure3: Timelineof theevolutionof theRAGsystemanditscomponents'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='4. Recent Advancement inthefield:\\nThere has been significant advancement in this field and this section intendstocapturekeyfindingsof afewimportant recent papers. Anovel agenticRetrieval-AugmentedGeneration(RAG) framework(Ravuruet. al. 2024) employs a hierarchical, multi-agent architecturewherespecializedsub-agents, usingsmallerpre-trained language models (SLMs), are fine-tuned for specific time series tasks. The master agentdelegates tasks to these sub-agents, who retrieve relevant prompts fromasharedknowledgerepository.In this modular, multi-agent approach, the authors achieve state-of-the-art performance demonstratingimproved flexibility and effectiveness over task-specificmethodsintimeseriesanalysis. RULE(Xiaet. al.2024), a multimodal Retrieval-Augmented Generation (RAG) framework designed to improve thefactuality of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG byintroducing a calibrated selection strategy to control factuality risk, and, by developing a preferenceoptimization strategy to balance the model’s intrinsic knowledge with retrieved contexts, proving itseffectiveness in enhancing factual accuracy in Med-LVLM systems. METRAG (Gan et. al. 2024), amulti-layered, thoughts-enhanced retrieval-augmented generation framework, integratesLLMsupervisionto generate utility-oriented thoughts and combines document similarity with utility for improvedperformance. It also incorporates a task-adaptive summarizer to produce compact thoughts. Using themulti-layered thoughts from these stages, an LLM generates knowledge-augmented content,demonstrating superior performance on knowledge-intensive tasks compared to traditional approaches.Distractor document is\\nFigure4: EvolvingTrendsinRAGcapturedfromresearchpapers'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"one of the key traits of Retrieval AugmentedFine-Tuning(RAFT) (Zhanget. al. 2024) wherethemodel istrained to disregard irrelevant, distractor documents and instead cite directly fromrelevant sources. Thisprocess, combined with a chain-of-thought reasoning style, enhances themodel'sreasoningcapabilities.RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, includingPubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO(Wang et. al. 2023) , a method designed toenhancethequalityof context providedtogenerativemodelsin tasks like open-domain question answering and fact verification, addresses issues of over- orunder-relianceonretrievedpassages, whichcanleadtoproblemssuchashallucinationsinthegeneratedoutputs. The method improves context quality by identifying useful context through lexical andinformation-theoretic approaches and training context filtering models to refine retrieved contexts duringtest time. ReflectionTokenisakeyattributeof Self-reflectiveRetrieval Augmented-Generation(Self-RAG)(Asai et. al. 2023), anovel frameworkdesignedtoimprovethefactual accuracyof largelanguagemodels(LLMs) by combining retrieval with self-reflection. Unlike traditional methodsthat retrieveandincorporatea fixed number of passages, Self-RAGadaptively retrievesrelevant passagesandusesreflectiontokensto evaluate and refine its responses, allowing the model to adjust its behavior according to task-specificneeds and has shown superior performance in open-domain question-answering, reasoning, factverification, and long-formgenerationtasks. Intelligenceandeffectivenessof RAGaredependent alot onthe quality of retrieval and more meta-data understanding of the repository would enhance theeffectiveness of the RAGsystem. Anovel data-centric Retrieval-Augmented Generation (RAG) workflowadvances beyond the traditional retrieve-then-read mode and employs aprepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextuallyrelevant, time-critical, or domain-specific information. Key innovations include generating metadata,synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MKSummary)for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG(Chang et. al. 2024), a zero-shot framework that integrates community structures within KnowledgeGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances theaccuracy and contextual relevance of fact-checking by utilizing multi-hop connections within KGs,outperforming traditional methods without requiring additional domain-specific training. The RAPTORmodel (Sarthi et. al. 2024) introduces a hierarchical approach to retrieval-augmented language models,addressing limitations in traditional methods that retrieve only short, contiguous text chunks. RAPTORforms a summary tree to retrieve information at varying abstraction levels by recursively embedding,clustering, and summarizing text. Experiments demonstrate RAPTOR’s superior performance, especiallyin question-answering tasks requiring complex reasoning. When paired with GPT-4, RAPTORimprovesaccuracyontheQuALITYbenchmarkby20%.\\nThis advancement in RAGfurther provestheutilityof theRAGsystemhowever recent LLMlaunchesthatsupport long-termcontext havesignificantlyshownimprovedperformance. Arecent study(Li et. al. 2024)compared the efficiency of Retrieval Augmented Generation (RAG) and long-context (LC) LargeLanguage Models (LLMs), such as Gemini-1.5 and GPT-4. While LC models outperform RAG whenadequately resourced, RAG's cost-efficiency remains advantageous. To balance performance and cost,the paper introduces Self-Route. This method dynamically directs queries to either RAGor LCbasedonmodel self-reflection, optimizing both computation cost and performance. This study offers valuableinsights into the optimal application of RAGand LCin handling long-context tasks. Nguyen et. al., 2024introduce SFR-RAG, a small but highly efficient Retrieval AugmentedGeneration(RAG) model, whichisdesigned to enhance the integration of external contextual information into Large Language Models(LLMs) while minimizing hallucinations. LA-RAG (Li et. al., 2024), a novel Retrieval-AugmentedGeneration (RAG) paradigm designed to enhance Automatic Speech Recognition (ASR) in largelanguage models (LLMs). One of the key benefits of LA-RAG is its ability to leverage fine-grainedtoken-level speech data stores alongside a speech-to-speech retrieval mechanism, improving ASR\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"accuracy by incorporating LLMin-context learning (ICL). The studyfocusesondatasetsof Mandarinandvarious Chinese dialects, demonstrating significant accuracy improvements, particularly in managingaccent variations, which have historically been a challenge for existing speech encoders. The findingshighlight LA-RAG’s potential to advance ASR technology, offering a more robust solution for diverseacoustic conditions. Large Language Models (LLMs) face challenges in AI legal and policy contextsdueto outdated knowledge and hallucinations. HyPA-RAG(Kalra et. al., 2024), aHybridParameter-AdaptiveRetrieval-Augmented Generation system, improves accuracy by using adaptive parameter tuning andhybrid retrieval strategies. Tested on NYC Local Law144 (LL144), HyPA-RAGdemonstrates enhancedcorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG(Qianet. al.,2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome thelimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG’sdual-system architecture utilizes a lightweight long-range LLM to generate draft answers and guideretrieval tools, while a more powerful LLM refines the final output. This framework, optimized for bettercluing and memory capacity, significantly outperforms conventional RAG models across both complexand straightforward tasks. NLLB-E5 (Acharya et. al., 2024) introduces a scalable multilingual retrievalmodel aimed at addressing the challenges faced in supporting multiple languages, particularlylow-resource languages like Indiclanguages. ByleveragingtheNLLBencoder andadistillationapproachfromtheE5multilingual retriever, NLLB-E5enableszero-shot retrieval acrosslanguageswithout theneedfor multilingual training data. Evaluations on benchmarks such as Hindi-BEIR showcase its robustperformance, highlighting task-specific challenges and advancing multilingual information access forglobal inclusivity.\\n5. Current Challenges andLimitationsinRetrieval-AugmentedGeneration(RAG):\\nThissectionintendstohighlight thecurrent challengesandlimitationsof RAGconsideringthecurrentlandscapeof thesystemandthiswouldshapethefutureresearchdirectionsinthefield.\\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrievalcomponentsrelyonexternal databases, handlingvast anddynamicallygrowingdatasetsrequiresefficientretrieval algorithms. High computational costs and memory requirements also make it difficult to deployRAGmodelsinreal-timeor resource-constrainedenvironments(Shi et al. 2023), (Asai et al. 2023b).\\nRetrieval Quality and Relevance: Ensuring the quality andrelevanceof retrieveddocumentsremainsasignificant concern. Retrieval models can sometimes return irrelevant or outdated information, whichnegatively affects the accuracy of the generated output. Improving retrieval precision, especially forlong-formcontent generation, remainsanactiveareaof research(Mallenet al. 2022), (Shi et al. 2023).\\nBias and Fairness: Similar to other machine learning models, RAG systems can exhibit bias due tobiases present in the retrieved datasets. Retrieval-based models may amplifyharmful biasesinretrievedknowledge, leading to biased outputs in a generation. Developing bias mitigation techniquesfor retrievalandgenerationintandemisanongoingchallenge.\\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,contextually relevant text. The alignment between retrieved passages and the generationmodel'soutputis not always seamless, leading to inconsistencies or factual hallucinations in the final response(Ji et al.2022).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Interpretability and Transparency: Like many AI systems, RAG models are often treated as blackboxes, with limited transparency in how retrieval influences generation. Improving the interpretability ofthesemodelsiscrucial tofosteringtrust, especiallyincritical applications(Roller et al. 2020).\\n6. FutureResearchDirectionsfor Retrieval-AugmentedGeneration(RAG)\\nRetrieval-augmented generation (RAG) represents a significant advancement in natural languageprocessing and related fields by combining retrieval and generative mechanisms. This section exploreskeyareasfor futureresearch, highlightingthepotential for innovationandimprovement inRAGsystems.\\n6.1 Enhancing Multimodal Integration: The integration of text, image, audio, and video data in RAGmodels remains an evolving challenge. Future research should focus on improving multimodal fusiontechniques to enable seamless interaction between different data types. This includes developingadvanced methods for aligning and synthesizing information across modalities. Recent works (Chen et.al. 2022), (Yasunaga et. al. 2022), (Zhu et. al. 2024) have explored multimodal learning, but furtherinnovations are needed to enhance the coherence andcontextualityof multimodal outputs.Researchintocross-modal retrieval aims to improve the ability of RAGsystems to retrieve relevant information acrossdifferent modalities. For example, combining text-based queries with image or video content retrievalcould enhance applications such as visual question answering and multimedia search. This is anotherfuturedirectiontoexplorefor RAGrelatedresearch.\\n6.2 Scaling and Efficiency: As RAG models are deployed in increasingly large-scale applications,scalability becomes a critical concern. Research should focus on developing methods toefficientlyscaleretrieval and generation processes without compromising performance. Techniques such as distributedcomputing and efficient indexing methods are essential for handling large datasets. Improving theefficiency of RAG models involves optimizing both retrieval and generation components to reducecomputational resourcesandlatency.\\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrievalprocesses to cater to individual user preferences and contexts. This involves developing techniques toadapt retrieval strategies based on user history, behaviour, and preferences. Enhancing the contextualadaptation of RAGmodels by deeper understandingof thecontext andsentimentsof query(Guptaet. al.2024) and the repository of ducments is crucial for improving the relevance of generated responses.Research should explore methods for dynamic adjustment of retrieval and generation processes basedontheevolvingcontext of interactions. Thisincludesincorporatinguser feedbackandcontextual cuesintotheRAGpipeline.\\n6.4 Ethical and PrivacyConsiderations: Addressingbiases(Shresthaet. al. 2024), (Guptaet. al. 2024)in general and specifics to RAG models is a critical area for future research. As RAG systems aredeployed in diverse applications, ensuring fairness and mitigating biases in retrieved and generatedcontent is essential. Future RAG research should focus on privacy-preserving techniques to protectsensitive information during retrieval and generation. This includes developing methods for secure datahandling and privacy-aware retrieval strategies. Interpretability of model is also a critical area to focusuponasapart of ongoingresearchinimprovingRAG.\\n6.5 Cross-Lingual and Low-Resource Languages: Expanding RAG technology to support multiplelanguages ( Chirkova et. al. 2024), especially low-resource languages, is a promising direction. Future'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"research should aimtoimprovecross-lingual retrieval andgenerationcapabilitiestoprovideaccurateandrelevant results across different languages. Enhancing RAG models to effectively support low-resourcelanguages involves developing methods to retrieve and generate content with limited training data.Research should focus on techniques for transfer learning and data augmentation to improveperformanceinunderrepresentedlanguages.\\n6.6 Advanced Retrieval Mechanisms: Future RAG research should explore dynamic retrievalmechanisms that adapt to changing query patterns and content requirements. This includes developingmodels that can dynamically update their retrieval strategiesbasedonnewinformationandevolvinguserneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, suchasdenseand sparse retrieval, could enhance the effectiveness of RAGsystems. Research shouldexplorehowtointegratedifferent retrieval methodstoachieveoptimal performancefor diversetasks.\\n6.7 Integration with Emerging Technologies: Integrating RAGmodels with brain-computer interfaces(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.Research should explore how RAG systems can leverage BCI data to enhance user experience andgenerate context-aware responses.The integration of RAG with AR and VR technologies presentsopportunities for creating immersive and interactive experiences. FutureresearchshouldinvestigatehowRAG models can be used to enhance AR and VR applications by providing contextually relevantinformationandinteractions.\\n7. Conclusion\\nRetrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive researchdedicated to improving retrieval effectiveness and enhancing coherent generation to minimizehallucinations. Fromitsearlyiterationstorecent advancements, RAGhasbeeninstrumental inintegratingexternal knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. Inparticular, recent domain-specific work has showcased RAG's potential in specialized areas such aslegal, medical, and low-resource language applications, highlighting its adaptability andscope. However,despite these advances, this paper identifies clear gaps that remain unresolved. Challengessuchastheintegration of ambiguous or unstructured information, effective handling of domain-specific contexts, andthe high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments. Thefuture research directions outlined in this paper—ranging from improving retrieval mechanisms toenhancing context management and ensuring scalability—will serveasacritical guidefor thenext phaseof innovation in this space. By addressing these gaps, the next generation of RAG models has thepotential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing theboundariesof what ispossibleinretrieval-augmentedAI applications.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='References:\\nAcharya, A., Murthy, R., Kumar, V., &Sen, J. (2024). NLLB-E5: AScalable Multilingual Retrieval Model.ArXiv. /abs/2409.05401\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M.,Menick, J., Borgeaud, S., . . . Simonyan, K. (2022). Flamingo: AVisual Language Model for Few-ShotLearning. ArXiv. /abs/2204.14198\\nAsai, A., Wu, Z., Wang, Y., Sil, A., &Hajishirzi, H. (2023). Self-RAG: Learning to retrieve, generate, andcritiquethroughself-reflection. arXivpreprint arXiv:2310.11511.\\nBaevski, A., Zhou, H., Mohamed, A., &Auli, M. (2020). Wav2vec 2.0: AFramework for Self-SupervisedLearningof SpeechRepresentations. ArXiv. /abs/2006.11477\\nBertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for VideoUnderstanding?ArXiv. /abs/2102.05095\\nBinns, R. (2018). Fairness in machine learning: Lessons from political philosophy. Proceedings of the2018ConferenceonFairness, Accountability, andTransparency(pp. 149-159).\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. V., Lespiau, J.,Damoc, B., Clark, A., Casas, D. D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L.,Jones, C., Cassirer, A., . . . Sifre, L. (2021). Improving language models by retrieving from trillions oftokens. ArXiv. /abs/2112.04426\\nBrown, T., et al. (2020). \"LanguageModelsareFew-Shot Learners.\" arXivpreprint arXiv:2005.14165.\\nChang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in KnowledgeGraphsfor AdvancedRetrieval-AugmentedGenerationinFact-Checking. ArXiv. /abs/2408.08535\\nChen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to answer open-domainquestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers) (pp. 1870-1879).\\nChen, W., Hu, H., Chen, X., Verga, P., &Cohen, W. W. (2022). MuRAG: Multimodal Retrieval-AugmentedGenerator for OpenQuestionAnsweringover ImagesandText. ArXiv. /abs/2210.02928\\nChirkova, N., Rau, D., Déjean, H., Formal, T., Clinchant, S., &Nikoulina, V. (2024). Retrieval-augmentedgenerationinmultilingual settings. ArXiv. /abs/2407.01463\\nDai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For FirstStageRetrieval. ArXiv. /abs/1910.10687\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectionaltransformers for language understanding. In Proceedings of the 2019 Conference of theNorthAmericanChapter of the Association for Computational Linguistics: Human Language Technologies (pp.4171-4186).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep BidirectionalTransformersfor LanguageUnderstanding. ArXiv. /abs/1810.04805\\nGan, C., Yang, D., Hu, B., Zhang, H., Li, S., Liu, Z., Shen, Y., Ju, L., Zhang, Z., Gu, J., Liang, L., &Zhou,J. (2024). Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi LayeredThoughts. ArXiv. /abs/2405.19893\\nGatt, A., &Krahmer, &E. (2018). Surveyof thestateof theart innatural languagegeneration: Coretasks,applications, andevaluation. Journal of Artificial IntelligenceResearch, 61, 65-170.\\nGupta, S., &Ranjan, R. (2024). Evaluation of LLMs Biases TowardsEliteUniversities: APersona-BasedExploration. ArXiv. /abs/2407.12801\\nGupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: FromRule-basedtomodernLLMbasedsystem. ArXiv. /abs/2409.09989\\nGuu, J., Lee, K., & Pasupat, P. (2020). Retrieval-augmented generation for knowledge-intensive NLPtasks. arXivpreprint. https://arxiv.org/abs/2002.08909\\nGuu, K., Lee, K., Tung, Z., Pasupat, P., & Chang, M. (2020). REALM: Retrieval-augmented languagemodel pre-training. In Proceedings of the 37th International Conference on Machine Learning (pp.3929-3938).\\nHan, S., Pool, J., Tran, J., & Dally, W. J. (2015). Learning both weights and connections for efficientneural network. InAdvancesinNeural InformationProcessingSystems(pp. 1135-1143).\\nIzacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domainquestion answering. In Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: MainVolume(pp. 874-880).\\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Chen, D., Dai, W., Chan, H. S.,Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ArXiv.https://doi.org/10.1145/3571730\\nKalra, R., Wu, Z., Gulley, A., Hilliard, A., Guan, X., Koshiyama, A., &Treleaven, P. (2024). HyPA-RAG: AHybridParameter AdaptiveRetrieval-AugmentedGenerationSystemfor AI Legal andPolicyApplications.ArXiv. /abs/2409.09046\\nKarpukhin, V., Oguz, B., Min, S., & Yih, W. (2020). Dense passage retrieval for open-domain questionanswering. arXivpreprint. https://arxiv.org/abs/2004.04906\\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. & Yih, W. T. (2020). Densepassage retrieval for open-domain question answering. In Proceedings of the 2020 Conference onEmpirical MethodsinNatural LanguageProcessing(EMNLP) (pp. 6769-6781).\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Riedel, S. (2020).Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34thInternational ConferenceonNeural InformationProcessingSystem( pp. 9459-9474).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Li, C., Liu, Z., Xiao, S., & Shao, Y. (2023). Making Large Language Models A Better Foundation ForDenseRetrieval. ArXiv. /abs/2312.15503\\nLi, F., Zhu, L., Wang, T., Li, J., Zhang, Z., & Shen, H. T. (2023). Cross-Modal Retrieval: ASystematicReviewof MethodsandFutureDirections. ArXiv. /abs/2308.14263\\nLi, S., Shang, H., Wei, D., Guo, J., Li, Z., He, X., Zhang, M., & Yang, H. (2024). LA-RAG:EnhancingLLM-basedASRAccuracywithRetrieval-AugmentedGeneration. ArXiv. /abs/2409.08597\\nLi, S., Park, S., Lee, I., & Bastani, O. (2023). TRAQ: Trustworthy Retrieval Augmented QuestionAnsweringviaConformal Prediction. ArXiv. /abs/2307.04642\\nLi, Z., Li, C., Zhang, M., Mei, Q., & Bendersky, M. (2024). Retrieval Augmented Generation orLong-Context LLMs?AComprehensiveStudyandHybridApproach. ArXiv. /abs/2407.16833\\nLiu, Z., Wang, H., Niu, Z., Wu, H., Che, W., &Liu, T. (2020). Towards Conversational Recommendationover Multi-TypeDialogs. ArXiv. /abs/2005.03954\\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., & Hajishirzi, H. (2022). When Not to TrustLanguage Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ArXiv./abs/2212.10511\\nMombaerts, L., Ding, T., Banerjee, A., Felice, F., Taws, J., &Borogovac, T. (2024). Meta Knowledge forRetrieval AugmentedLargeLanguageModels. ArXiv. /abs/2408.09017\\nNguyen, X., Pandit, S., Purushwalkam, S., Xu, A., Chen, H., Ming, Y., Ke, Z., Savarese, S., Xong, C., &Joty, S. (2024). SFR-RAG: TowardsContextuallyFaithful LLMs. ArXiv. /abs/2409.09916\\nNiu, C., Wu, Y., Zhu, J., Xu, S., Shum, K., Zhong, R., Song, J., & Zhang, T. (2023). RAGTruth: AHallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. ArXiv./abs/2401.00396\\nQian, H., Zhang, P., Liu, Z., Mao, K., &Dou, Z. (2024). MemoRAG: Moving towards Next-Gen RAGViaMemory-InspiredKnowledgeDiscovery. ArXiv. /abs/2409.05591\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models areunsupervisedmultitasklearners. OpenAI Blog, 1(8), 9.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &Liu, P. J. (2019).ExploringtheLimitsof Transfer LearningwithaUnifiedText-to-Text Transformer. ArXiv. /abs/1910.10683\\nRanade, P., & Joshi, A. (2023). FABULA: Intelligence Report Generation Using Retrieval-AugmentedNarrativeConstruction. ArXiv. https://doi.org/10.1145/3625007.3627505\\nRanjan, R., Gupta, S., & Singh, S. N. (2024). A Comprehensive Survey of Bias in LLMs: CurrentLandscapeandFutureDirections. ArXiv. /abs/2409.16430\\nRavuru, C., Sakhinana, S. S., &Runkana, V. (2024). Agentic Retrieval-Augmented Generation for TimeSeriesAnalysis. ArXiv. /abs/2408.14484'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Robertson, S.G., & Zaragoza,H., (2009). The Probabilistic Relevance Framework: BM25 and Beyond,FoundationsandTrendsinInformationRetrieval, 3(4), pp. 333-389.\\nRoller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Shuster, K., Smith, E. M.,Boureau, Y., &Weston, J. (2020). Recipesfor buildinganopen-domainchatbot. ArXiv. /abs/2004.13637\\nSalton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing.Communicationsof theACM, 18(11), 613-620.\\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller,faster, cheaper andlighter. ArXiv. /abs/1910.01108\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., &Manning, C. D. (2024). RAPTOR: RecursiveAbstractiveProcessingfor Tree-OrganizedRetrieval. ArXiv. /abs/2401.18059\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., & Yih, W.-T. (2023).REPLUG: Retrieval-augmentedblack-boxlanguagemodels. arXivpreprint arXiv:2301.12652.\\nShrestha, R., Zou, Y., Chen, Q., Li, Z., Xie, Y., &Deng, S. (2024). FairRAG: Fair Human GenerationviaFair Retrieval Augmentation. ArXiv. /abs/2403.19964\\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. InAdvancesinNeural InformationProcessingSystems(pp. 3104-3112).\\nThakur, N., Bonifacio, L., Zhang, X., Ogundepo, O., Kamalloo, E., Li, X., Liu, Q., Chen, B.,Rezagholizadeh, M., &Lin, J. (2023). NoMIRACL: KnowingWhenYouDon't Knowfor Robust MultilingualRetrieval-AugmentedGeneration. ArXiv. /abs/2312.11361\\nThakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A HeterogenousBenchmarkfor Zero-shot Evaluationof InformationRetrieval Models. ArXiv, abs/2104.08663.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &Polosukhin, I.(2017). Attentionisall youneed. InAdvancesinNeural InformationProcessingSystems(pp. 5998-6008).\\nWang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., Shi, T., Wang, Z., Li, S., Qian, Q., Yin, R., Lv, C.,Zheng, X., &Huang, X. (2024). Searching for Best Practices in Retrieval-Augmented Generation. ArXiv./abs/2407.01219\\nWang, Z., Araki, J., Jiang, Z., Parvez, M. R., & Neubig, G. (2023). Learning to Filter Context forRetrieval-AugmentedGeneration. ArXiv. /abs/2311.08377\\nXia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., &Yao, H. (2024). RULE: ReliableMultimodal RAGfor FactualityinMedical VisionLanguageModels. ArXiv. /abs/2407.05131\\nXie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2017). Rethinking Spatiotemporal Feature Learning:Speed-AccuracyTrade-offsinVideoClassification. ArXiv. /abs/1712.04851\\nXiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P., Ahmed, J., &Overwijk, A. (2020). ApproximateNearest Neighbor NegativeContrastiveLearningfor DenseText Retrieval. ArXiv. /abs/2007.00808\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &Yih, W. (2022). Retrieval-AugmentedMultimodal LanguageModeling. ArXiv. /abs/2211.12561\\nZhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., & Gonzalez, J. E. (2024). RAFT:AdaptingLanguageModel toDomainSpecificRAG. ArXiv. /abs/2403.10131\\nZhu, Y., Ren, C., Xie, S., Liu, S., Ji, H., Wang, Z., Sun, T., He, L., Li, Z., Zhu, X., & Pan, C. (2024).REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via LargeLanguageModels. ArXiv. /abs/2402.07016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nSUFFICIENT CONTEXT : A N EW LENS ON RETRIEVAL\\nAUGMENTED GENERATION SYSTEMS\\nHailey Joren∗\\nUC San Diego\\nhjoren@ucsd.edu\\nJianyi Zhang†\\nDuke University\\njianyi.zhang@duke.edu\\nChun-Sung Ferng\\nGoogle\\ncsferng@google.com\\nDa-Cheng Juan\\nGoogle\\ndacheng@google.com\\nAnkur Taly\\nGoogle\\nataly@google.com\\nCyrus Rashtchian\\nGoogle\\ncyroid@google.com\\nABSTRACT\\nAugmenting LLMs with context leads to improved performance across many\\napplications. Despite much research on Retrieval Augmented Generation (RAG)\\nsystems, an open question is whether errors arise because LLMs fail to utilize the\\ncontext from retrieval or the context itself is insufficient to answer the query. To\\nshed light on this, we develop a new notion of sufficient context, along with a\\nmethod to classify instances that have enough information to answer the query. We\\nthen use sufficient context to analyze several models and datasets. By stratifying\\nerrors based on context sufficiency, we find that larger models with higher baseline\\nperformance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when\\nthe context is sufficient, but often output incorrect answers instead of abstaining\\nwhen the context is not. On the other hand, smaller models with lower baseline\\nperformance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient\\ncontext. We further categorize cases when the context is useful, and improves\\naccuracy, even though it does not fully answer the query and the model errs without\\nthe context. Building on our findings, we explore ways to reduce hallucinations in\\nRAG systems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of\\ncorrect answers among times where the model responds by 2–10% for Gemini,\\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\\navailable on our github.\\n1 I NTRODUCTION\\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\\nenough information to answer the question when combined with the model’s parametric knowledge.\\nOtherwise, the model should abstain from answering and/or ask for more information. One core\\nchallenge in achieving this ideal outcome is building models that can use the provided context only\\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating\\n∗Work done during an internship at Google.\\n†Work done during an internship at Google.\\n1\\narXiv:2411.06037v3  [cs.CL]  23 Apr 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\\n“relevant information” can range from directly containing the answer to simply being topically related\\nto the question. Even “golden” or oracle documents in datasets vary in how much information they\\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\\nwords, while the goal seems to be to understand how LLMs behave when they do or do not have\\nsufficient information to answer the query, prior work fails to address this head-on.\\nAs our first contribution, we put forth a new notion of sufficient context. We divide instances into two\\ncategories based on whether the context provides enough information to construct an answer to the\\nquery. The sufficient context designation is a function of an input pair consisting of one question and\\nthe associated context. Crucially, it does not require a ground truth answer. Figure 1 shows examples\\nand a breakdown of model responses after splitting the data based on sufficient vs. insufficient context.\\nTo divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here,\\nan autorater is a model that evaluates instances based on a property, e.g., a sufficient context autorater.\\nUsing our sufficient context autorater, we uncover new insights into LLM behavior and into existing\\nbenchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of\\ninstances that have sufficient context to answer the query. In other words, open-book QA cannot\\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,\\nmodels tend to hallucinate more than they abstain, especially for multi-hop questions. This finding\\ncomplements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al.,\\n2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the\\nprovided context is insufficient. Surprisingly, this remains true after we filter out questions that the\\nmodel answers correctly in a closed book (w/o RAG) setting. Together, our analysis deepens our\\nunderstanding of RAG systems by revealing nuances in how models generate responses with retrieval.\\nAs a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\\nWe implement a new selective generation framework that improves accuracy. We use a smaller,\\nintervention model to determine when the model generates or abstains, providing a controllable\\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\\nmultiple models/datasets. We also find that fine-tuning open-source models with sufficient context\\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\\nour autorater analysis are available on our github.\\nTo summarize, our main contributions are\\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\\ninstances scalably and to analyze model responses with or without sufficient context.\\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\\ntakeaway is that SOTA LLMs output correct responses 35–62% of the time with insufficient\\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\\n3. Building on our findings above, we develop an efficient and general method for selective generation,\\nusing both confidence and sufficient context signals. Our method improves the fraction of correct\\nanswers (among total model responses) by up to 2–10% for Gemini, GPT, and Gemma.\\n2 R ELATED WORK\\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\\n(Ir)relevant Context. Prior studies uncover a lack of robustness to imperfect retrieval. However,\\nthese studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise\\n“relevance” definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nProprietary + Conﬁdential\\nQuestion: Who is Lya L. married to?\\nInsufficient \\nContext\\nLya L. married Tom in 2006… \\nThey divorced in 2014… Lya went \\non dates with Paul in 2018…\\nContext C\\nSufficient \\nContext\\nLya L. married Paul in 2020… They \\nlooked happy together at the \\nrecent event.\\nContext A\\nExamples: sufficient context\\nInsufficient \\nContext\\nLya L. is an astronaut, born in \\nOhio…. Lya has two children… \\nLya’s parents are lawyers…\\nContext D\\nSufficient \\nContext\\nLya L. – Wikipedia\\nBorn: October 1, 1980\\nSpouse: Paul (m. 2020)\\nContext B\\nCategorizing Model Responses (Musique Dataset)\\nFigure 1: New insights into RAG systems by looking at whether instances have sufficient context.\\nOn the left, we show examples of sufficient context; on the right, a breakdown of model responses on\\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with\\nRAG, models hallucinate more than abstain, and the insufficiency of the context does not account for\\nthis major issue. Also, standard datasets have many instances with insufficient context (here, 55.4%).\\nWe include results for other datasets (FreshQA, HotPotQA) in Appendix B, showing similar trends.\\nshould not impact the answer at all (and GSM8K is designed to have sufficient context by definition).\\nXie et al. (2024) looks at having counter-memory context, by either replacing the entity name with\\nan erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based\\nentailment to determine relevance, and only uses the relevance scores to influence the training mixture\\nof relevant vs. irrelevant documents. Wu et al. (2024) looks at questions where the LLM gets the\\nanswer correct without retrieval and is non-robust to changes in the retrieval. Multiple methods use\\na model to predict relevance scores (as part of a larger pipeline), without calibration to a formal\\ndefinition (Wang et al., 2024a; Zhou et al., 2024), including for iterative retrieval (Jiang et al., 2024;\\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\\nrelevant documents, but simply uses “does not contain the answer” as a proxy for irrelevant context.\\nReducing Hallucinations. There have also been efforts to improve RAG factuality on open-book QA\\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the “lost\\nin the middle” problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\\nthe premise that the provided query/context should be precisely answerable by the LLM, and hence,\\nonly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\\n3 S UFFICIENT CONTEXT\\nAt a high level, our aim is to classify input instances based on whether the context contains enough\\ninformation to answer the query. We split possible contexts into two cases: (1) Sufficient Context.\\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\\ndefinitive answer. (2) Insufficient Context. Otherwise, a context is insufficient. A context may also\\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more\\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n3.1 D EFINITION OF SUFFICIENT CONTEXT\\nWe first set some notation for a generic open-domain question-answering setting following Trivedi\\net al. (2020). Consider a dataset D with instances of the form q = (Q, C; A), where Q is the query\\nand C is the context that consists of a set of facts. At inference time, we also consider instances\\nq′ = (Q, C) without the ground truth answer, where the goal is to predict an answer A′ from Q, C,\\nand the model’s parametric knowledge. To measure correctness, we compareA′ and A, where there\\nare many options such as exact match, F1 score, or an LLM-based assessment of answer sameness\\n(we use an LLM). Using this notation, we can now define our notion of sufficient context.\\nDefinition (Sufficient Context). An instance q′ = (Q, C) has sufficient context if and only if there\\nexists an answer A′ such that A′ is a plausible answer to the question Q given the information in C.\\nTo understand this, we can build on the Attributable to Identified Sources (AIS) framework (Rashkin\\net al., 2023). Entailment via AIS answers a slightly different question. Namely, given an instance\\nq = ( Q, C; A), the entailment objective is to determine the truth value of the proposition: The\\nanswer to the question Q is A given the information in C. The key difference between entailment\\nand sufficient context is that for sufficient context we do not presuppose that we have the answerA′\\nin advance, only that such an answer exists. Finally, we only consider “plausible” answers, where\\nwe mean that A′ could be an answer to the question Q. For example, if the question asks about a\\nperson’s birthplace, thenA′ should be a location. We note that this allows for the possibility that the\\ncontext contains an incorrect answer to the question. This is a key requirement, because (i) we would\\nlike to be able to use signal from sufficient context at inference time, where we do not have ground\\ntruth answers (see Section 5.1) and (ii) we hope to elucidate findings that are robust to ground truth\\nlabel noise.\\nRemark 1 (Multi-hop queries). In most benchmark dataset (e.g., Musique, HotPotQA), models are\\nexpected to be able to do multi-hop reasoning up to four hops, in which they must combining facts to\\nform the answer. However, they should not infer connections that are not in the context. For example,\\nif “Bob’s mother was born in New York” then this does not suffice to say Bob was born in New York.\\nBut, if the context also says “Bob’s mother is Alice...” and “... all of Alice’s children were born in\\nNew York” then this instance has sufficient context.\\nRemark 2 (Ambiguous queries). If the query is ambiguous, then the context is sufficient if and\\nonly if (i) the context can disambiguate the query and (ii) the context provides an answer to the\\ndisambiguated query. For example, the question could be “What sport does Mia play?” and the\\ncontext could contain both “Mia, from New York, plays basketball. . . ” and “... Mia, from California,\\nplays volleyball.” This is sufficient because if the query is referring to either Mia from New York or\\nBob from California, then the context can answer the question.\\nRemark 3 (Ambiguous contexts). Assume the context contains multiple plausible answers to the\\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\\nqueries that would lead to each answer. For example, if the question is “What country does Ali live\\nin?” and the context is “Ali lives in Paris” then this instance does not have sufficient context because\\nit is not clear if Ali lives in Paris, France or Paris, Texas, USA. If the context further contains “This\\nweekend, Ali took the train from Paris to Marseille.” Then this becomes sufficient because it is almost\\ncertain that Ali lives in France as one cannot take a train from Texas to France.\\n3.2 S UFFICIENT CONTEXT AUTO RATER\\nNext, we consider automating the task of labeling whether instances have sufficient context or not.\\nWe investigate two questions: (1) Can today’s models achieve high accuracy on a challenging,\\nhuman-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5\\nPro can serve as an accurate autorater to label instances in terms of sufficient context. It achieves\\n93% accuracy, outperforms other methods, and operates without needing a ground truth answer.\\nSufficient Context Labeled Dataset. Using the above definition, we construct gold labels for\\neach (query, context) pair. We did not use ground truth answers or model responses. For\\nthe instances, we sample a total of 115 instances (queries, contexts, and answers) from standard\\nbenchmarks (PopQA, FreshQA, Natural Questions, EntityQuestions). We design the dataset to\\nbe very challenging, including single- and multi-hop questions, as well as adding highly related\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 1: Sufficient Context AutoRater.Evaluating model ability to classify sufficient context on a\\ngold-labeled dataset of 115 (query, context; answer) instances. Gemini 1.5 Pro (1-shot)\\nperforms the best, while FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\\nground truth (GT) answers, while others only use (query, context) . Best in column in bold.\\nMetrics: F1 Score Accuracy Precision Recall No GT Answer\\nMethods\\nGemini 1.5 Pro (1-shot) 0.935 0.930 0.935 0.935 ✓\\nGemini 1.5 Pro (0-shot) 0.878 0.870 0.885 0.871 ✓\\nFLAMe (fine-tune PaLM 24B) 0.892 0.878 0.853 0.935 ✓\\nTRUE-NLI (fine-tune T5 11B) 0.818 0.826 0.938 0.726\\nContains GT 0.810 0.809 0.870 0.758\\ninformation in the context even if it is not sufficient (e.g., a named entity from the question often\\nappears in the context). We evaluate methods’ abilities to classify sufficient context (binary labels).\\nMethods: Operating on Query-Context pairs. We use Gemini 1.5 Pro with either instructions\\n(0-shot) or both instructions and a 1-shot example, held out from our dataset. FLAMe 24B is a\\ngeneral autorater model (Vu et al., 2024), but it has a small context window. For FLAMe, we divide\\nthe contexts into 1600 token chunks and ask whether each chunk is sufficient. If any chunk is labeled\\nsufficient, we consider the instance to have sufficient context; otherwise, it’s labeled as insufficient.\\nWe design prompts (in Appendix C) for both models based on the sufficient context definition above.\\nMethods: When a Ground Truth (GT) Answer is Available. For two baselines (TRUE-NLI,\\nContains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT\\nanswers. Contains GT checks if a GT answer appears in the context. Comparing to entailment is\\nparticularly interesting because if a given answer A is entailed by (Q, C), then the context is also\\nsufficient. On the other hand, the reverse is not true, since the answer A is only one possible choice\\nfor A′. As one consequence, if the ground truth answer A is incorrect, then it may not be entailed by\\nthe context. This happens when a named entity is ambiguous (e.g., two people with the same name),\\nand the GT answer is based on one of the people while the context describes the other.\\nResults. Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,\\nwhich implies sufficient context. FLAMe outperforms TRUE-NLI in F1 and accuracy, but lags behind\\nGemini (1-shot), likely because it is a smaller model. The Contains GT method works surprisingly\\nwell, indicating that the presence of a ground truth answer correlates with context sufficiency.\\nDiscussion. In real-world scenarios, we cannot expect candidate answers when evaluating model\\nperformance. Hence, it is desirable to use a method that works using only the query and context.\\nAmong these methods, Gemini 1.5 Pro (1-shot) has high accuracy and balanced precision and recall.\\nTherefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\\nsufficient context is a different (and more complex) task than determining entailment.\\n4 A N EW LENS ON RAG P ERFORMANCE\\nWe set out to understand RAG performance by looking at sufficient context. We first analyze datasets\\n(Section 4.1), then we investigate model performance with/without sufficient context (Section 4.2). We\\nqualitatively discuss cases where insufficient context leads to correct model responses (Section 4.3).\\n4.1 D O BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT ?\\nWe introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we\\ndo not aim to optimize the retrieval methods (which could increase the sufficient context percentage).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreshQA HotpotQA Musique\\n0\\n20\\n40\\n60\\n80\\n100% of Dataset with Sufficient Context\\n77.4\\n46.2 44.6\\n77.4\\n46.2 44.6\\n63.7\\n45.4\\n33.4\\nMeasuring Sufficiency by Context Length\\n10000 T okens\\n6000 T okens\\n2000 T okens\\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\\nThis is not the focus of our work, as we wish to understand how models perform with or without\\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\\nDatasets. We consider FreshQA, Musique-Ans, and HotpotQA as a representative spread of open\\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\\nURLs that should support an answer to the queries, which we use to construct the context (see\\nAppendix A.3 for details on the retrieval). We use the ‘True Premise’ setting (452 instances), skipping\\n‘False Premise’ questions that mislead by design. Musique-Ans (Trivedi et al., 2022) is a multi-hop\\nQA benchmark, created by composing two to four single-hop interconnected questions. Here, ‘Ans’\\nis the standard ‘answerable’ subset. Musique instances have 20 supporting text snippets as sources,\\nwhich we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the\\ncontext (via REPLUG (Shi et al., 2023b) from FlashRAG (Jin et al., 2024)). We randomly sample\\n500 instances from the development sets of Musique-Ans and HotPotQA for evaluation.\\nSufficient Context % of Datasets. Figure 2 shows the fraction of instances that our autorater\\nclassifies as having sufficient context. We explore three context lengths, ranging from a maximum\\nof 2000 to maximum of 10000 tokens. The motivation behind this is to assess if there is a large\\nchange in sufficient context if we were to simply truncate the retrieval (e.g., for models that have\\nsmall context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,\\nwhich makes sense as the context comes from oracle supporting documents. The lower sufficient\\ncontext in Musique is perhaps surprising, given that the retrieval is fixed as part of the dataset. From\\nthe results in Figure 2, we truncate at 6000 tokens for all three datasets in the remainder of the paper.\\n4.2 I NITIAL FINDINGS BASED ON SUFFICIENT CONTEXT\\nIn general, the ideal behavior for a language generation model is to answer questions correctly when\\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\\nanalyze several cases to assess how far we are from this ideal trade-off.\\nExperimental Set-up and LLMEval. We employed a basic chain of thought (CoT) prompting\\napproach, with the prompt structure and further information detailed in Appendix C.4. We then\\nprocessed the outputted answers to identify matches between the response and any of the ground truth\\nanswers. Responses where a clear correct match could not be determined were processed through\\nthe LLMEval pipeline using a zero-shot approach, with the prompt based on Krishna et al. (2024)\\n(see Appendix C.3). Then, for each example, we can rate it as “correct” or “abstain” or “hallucinate”\\ndepending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.\\nModels Abstain Less with RAG. While overall performance improves with RAG, the introduction\\nof additional context paradoxically reduces the model’s ability to abstain from answering when\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCategorizing RAG Responses: Sufficient vs Insufficient Context\\nSufficient ContextInsufficient Context\\nFreshQA Musique HotpotQA\\nFigure 3: Model Performance on Datasets Stratified by Sufficient Context. Given sufficient\\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\\nbut the models are still able to answer a large portion of questions correct without sufficient context.\\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\\nfraction of abstentions drops to 52%. Similarly, GPT 4o’s abstention fraction moves from 34.4%\\nto 31.2% and Gemini 1.5 Pro’s drops from 100% to 18.6%. This phenomenon may arise from the\\nmodel’s increased confidence in the presence of any contextual information, leading to a higher\\npropensity for hallucination rather than abstention.\\nModels Hallucinate with Both Sufficient and Insufficient Context. Considering Figure 3, models\\ngenerally achieve higher accuracy with sufficient context (highergreen bars, top row) than without\\nsufficient context (lower green bars, bottom row). However, looking at each row separately, we\\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\\nmore than they abstain ( red bars are higher than blue bars, usually). The trend holds across all\\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\\nmodels. For instance, Claude abstains more (higher blue bars) with insufficient context, but answers\\nfewer questions correctly (lower green bars) than Gemini and GPT. These differences underscore\\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\\nmuch more hallucinations (higher red bars) than the other models, except for HotPotQA, where we\\nattribute the higher accuracy to the smaller retrieved contexts.\\n4.3 Q UALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\\neven when presented with insufficient context. For example, from Figure 3, all three models are able\\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\\nassumption is that the models already know the answer from pre-training, and they can generate a\\ncorrect response from parametric memory. However, this only explains part of the story.\\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nInstance type Why model may be correct Example\\nYes/No question 50% chance of correct Q: Is there a total eclipse in the United States this year?\\nLimited choice Some chance of correct Q: Which band has more members, Chvrches or\\nGoodbye Mr. Mackenzie?\\nMulti-hop: fragment Use parametric inference\\nQ: Who did the original voice for the character\\nwhose series Mickey’s Safari in Letterland is from?\\nContext says Mickey’s Safari is a video game\\nand Walt Disney voices Mickey Mouse in cartoons.\\nMust infer the game is in the Mickey Mouse series.\\nMulti-hop: partial Use parametric knowledge\\nQ: Claudine’s Return starred the actress who played\\nwhich role on “Married...with Children”?\\nContext lists actresses but not their roles in\\n“Married...with Children”. Must know extra facts.\\nToo many hops Execute complex reasoning\\nQ: How many cyclists have won all three of women’s\\ncycling Grand Tours equivalents in the same year?\\nContext requires cross-referencing lists of events\\nand lists of winners while tracking winners by year .\\nAmbiguous query Guess right interpretation\\nQ: Who is the spouse of a cast member from King\\nof the Mountain?\\nContext has many cast members and query/context do\\nnot specify which spouse to answer about.\\nRater error Mislabel insuff. or correct —\\nClosed-book correct Known from pre-training —\\nTable 2: Qualitative Analysis of Correct Answer & Insufficient Context. Examining model\\nresponses across datasets, we identify common cases where the model generates a correct answer\\neven though our autorater labels the instance as insufficient. We categorize such instances into eight\\ntypes, as well as provide examples. Given that models are also correct on many questions in the\\nclosed-book setting, we believe this mostly explains the 35–62% correct rate with insufficient context.\\nas correct. For example, one type accounts for when the provided context is not sufficient to answer the\\nquery, but it bridges gaps in the model’s knowledge. Another type is when the retrieved information\\nclarifies ambiguities inherent in the question (without answering the question). Finally, we also have\\nthe times where either the autorater or the evaluator model makes an error. We note that our analysis\\nexpands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\\nis correct with RAG (but not without) even though the context does not contain the answer.\\nWe additionally investigated cases where the autorater labels an instance as having sufficient context\\nwhile the LLM evaluator marks the answer as incorrect. One source of these discrepancies occurs\\nwhen the ground truth answer conflicts with the answer provided in the source. This represents a key\\ndifference from methods that measure entailment, where context is evaluated relative to a specific\\nground truth answer (see Table 1). Another source of errors arises when the autorater correctly\\nidentifies that the necessary information is present, but the model fails to properly compose the\\ninformation (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\\ncases, however, determining the source of the error proves challenging.\\n5 T ECHNIQUES TO REDUCE HALLUCINATIONS WITH RAG\\nFrom our previous analysis, we have seen that models may hallucinate rather than abstain and that\\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\\noutput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\\n5.1 S ELECTIVE RAG U SING SUFFICIENT CONTEXT SIGNAL\\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\\nperformance, since all models answer some questions correctly even with insufficient context, as\\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\\naccuracy-coverage trade-off, where “coverage” denotes the portion of inputs on which the model does\\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\\nand then use it to set coverage-accuracy trade-off thresholds.\\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\\nit operates independently from generation, it mitigates unintended downstream effects, whereas\\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\\ninputs (see Section 5.2). Second, it offers a controllable mechanism for tuning abstention, which\\nallows for different operating settings in differing applications, such as strict accuracy compliance in\\nmedical domains or maximal coverage on creative generation tasks.\\nAbstention Signals We utilize two main signals for abstention: the self-rated probabilities as\\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\\nfrom the model multiple times, and then prompting the model multiple times to label each model as\\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\\nwe use P(Correct) instead. We adapt the probability-generating prompt from Li et al. (2024) to obtain\\nthe model’s response and its estimated probability of correctness. For the sufficient context signal,\\nwe use the binary label from an autorater. Our hypothesis is that combining these signals should lead\\nto more effective abstention, particularly in cases where the context is insufficient.\\nMethods. We calculate P(True) by sampling 20 responses for each question and querying the model\\n5 times to evaluate whether the answer is correct or incorrect (without using the ground truth) as\\nin Kadavath et al. (2022). For P(Correct), the prompt requests the most likely and second most\\nlikely answers along with their probabilities. We use string matching to extract the response and\\nself-predicted probability, keeping the one with the highest probability. To determine sufficient\\ncontext, we use FLAMe, a small and efficient model for determining the sufficient context label. We\\ndivide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as\\nsufficient if any of these chunks are sufficient.\\nWe combine the binary sufficient context label with the self-rated answer probability (P(True) for\\nopen-source models or P(Correct) for proprietary models) in a simple logistic regression model to\\npredict hallucinations with 100 iterations of random hyperparameter search. At inference time, we\\nuse the logistic regression model scores to threshold the outputs, abstaining when the score is below\\na chosen threshold as in Joren et al. (2024). We measure the added value for selective accuracy\\nof the sufficient context signal (purple line in Figure 4) by comparing it with the model self-rated\\nconfidence alone (gray line).\\nResults. We find that our approach leads to a better selective accuracy-coverage trade-off compared\\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA\\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\\n70% coverage region. These gains are less pronounced on datasets with lower overall accuracy, such\\nas when using Gemma 27B on Musique. In this scenario, the low overall performance (18.4%) likely\\nmeans that most of the predictive gains are seen by using the self-rated confidence to predict errors\\nfor the majority of samples. As a result, there is no added benefit from the sufficient context signal.\\nDiscussion. As expected, we see a downward trend in which higher coverage leads to lower selective\\naccuracy for both methods. We conclude that the selective generation mechanism with sufficient\\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence\\nalone. As a prerequisite for our method, models should have a non-trivial accuracy on sufficient and\\ninsufficient context instances. Then, intuitively, we can prioritize answering questions the model is\\nlikely to get right before those the model struggles with. While ordering examples is impossible in\\nreal settings, we can estimate a coverage level and use a threshold to choose when to answer.\\n5.2 F INE -TUNING\\nWe also consider fine-tuning models to increase their ability to abstain instead of outputting an\\nincorrect answer. To do so, we train the models with some examples that contain “I don’t know”\\ninstead of their original ground truth answer. The intuition here is that training explicitly on such\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n Gemma 27B - HotpotQA\\nP(True)\\nP(True) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n Gemma 27B - Musique\\nP(True)\\nP(True) + Suff. Context\\nFigure 4: Selective Generation: Coverage vs. Selective Accuracy. For selective generation, we\\nuse a linear combination of sufficient context and self-rated confidence (purple) or confidence alone\\n(gray). The x-axis shows coverage (% of questions answered); the y-axis shows accuracy at each\\ncoverage (# correct / # answered). The combined approach matches or outperforms the baseline\\nconfidence-only method, especially on HotpotQA, where our method improves accuracy for most\\ncoverages. For Gemma 27B on Musique, the methods are identical (coeff. for stuff. context is 0).\\ninputs could encourage the model to abstain instead of hallucinating. We also consider multiple\\nsettings, such as only changing the answers when the example has insufficient context. We present\\nfull details in Appendix B.1. The main takeaways are that fine-tuned models (i) have a higher rate of\\ncorrect answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,\\nit is likely possible to use fine-tuning to steer the model towards better abstention and correctness, but\\nmore work is needed to determine develop a reliable strategy that can balance these objectives.\\n6 C ONCLUSION\\nOur work provided a new lens on LLM responses in RAG systems centered around our notion of\\nsufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\\na model will output a correct answer with access to only insufficient context. Qualitatively, we\\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\\nGemma, and can reduce hallucinations by 2–10% on queries that the model answers.\\nLimitations. Our analysis focuses on QA datasets, but summarization tasks also utilize context,\\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\\n“Summarize the reviews of 5-star hotels in Mallorca” depending on whether the context mentions the\\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\\noften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score\\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nACKNOWLEDGMENTS\\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\\nimprove the presentation.\\nREFERENCES\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nAnthropic. Claude 3.5 sonnet model card addendum, 2024.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\\ngenerate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau\\nYih. Reliable, adaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187,\\n2024.\\nYung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens:\\nDetecting and mitigating contextual hallucinations in large language models using only attention maps. arXiv\\npreprint arXiv:2407.07071, 2024.\\nFlorin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,\\nNicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems. In\\nProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval, pp. 719–729, 2024.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A\\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 6491–6501, 2024.\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805, 2023.\\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\\nLéonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open\\nlanguage models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency\\nevaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pp. 3905–3920, 2022.\\nCheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass,\\nAlexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating\\npositional attention bias improves long context utilization. In Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024 , pp. 14982–14995,\\nBangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:\\n10.18653/v1/2024.findings-acl.890. URL https://aclanthology.org/2024.findings-acl.\\n890.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. In International Conference on Learning\\nRepresentations (ICLR), 2022.\\nAQ Jiang, A Sablayrolles, A Mensch, C Bamford, DS Chaplot, D de las Casas, F Bressand, G Lengyel, G Lample,\\nL Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023.\\nZhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop\\nquestion answering with an iterative approach. arXiv preprint arXiv:2407.13101, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for\\nefficient retrieval-augmented generation research. CoRR, abs/2405.13576, 2024. URL https://arxiv.\\norg/abs/2405.13576.\\nHailey Joren, Chirag Nagpal, Katherine A Heller, and Berk Ustun. Participatory person-\\nalization in classification. In Advances in Neural Information Processing Systems , vol-\\nume 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\\nfile/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf.\\nHailey Joren, Charles Marx, and Berk Ustun. Classification with conceptual safeguards. In The Twelfth Inter-\\nnational Conference on Learning Representations (ICLR) , 2024. URL https://iclr.cc/virtual/\\n2024/poster/17625.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\\nknow. arXiv preprint arXiv:2207.05221, 2022.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay,\\nand Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. arXiv\\npreprint arXiv:2409.12941, 2024.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-\\nintensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. Think twice before assure:\\nConfidence estimation for large language models through reflection on multiple answers. arXiv preprint\\narXiv:2403.09972, 2024.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. Lost in the middle: How language models use long contexts. Transactions of the Associ-\\nation for Computational Linguistics , 12:157–173, 2024. doi: 10.1162/tacl_a_00638. URL https:\\n//aclanthology.org/2024.tacl-1.9.\\nPaul Mineiro. Online joint fine-tuning of multi-agent flows. arXiv preprint arXiv:2406.04516, 2024.\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and\\nHannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. In COLM 2024,\\n2024.\\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.\\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (V olume 1: Long Papers) , pp. 10862–10878, Bangkok,\\nThailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.585.\\nURL https://aclanthology.org/2024.acl-long.585.\\nLucas Monteiro Paes, Carol Xuan Long, Berk Ustun, and Flavio Calmon. On the epistemic limits of personalized\\nprediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural\\nInformation Processing Systems, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. In-context retrieval-augmented language models. Transactions of the Association for Computational\\nLinguistics, 11:1316–1331, 2023.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov,\\nGaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation\\nmodels. Computational Linguistics, 49(4):777–840, 2023.\\nDavid Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and\\nStéphane Clinchant. Bergen: A benchmarking library for retrieval-augmented generation. arXiv preprint\\narXiv:2407.01102, 2024.\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang\\nWang, Shichao Sun, Huanyu Li, et al. Ragchecker: A fine-grained framework for diagnosing retrieval-\\naugmented generation. arXiv preprint arXiv:2408.08067, 2024.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny\\nZhou. Large language models can be easily distracted by irrelevant context. In International Conference on\\nMachine Learning, pp. 31210–31227. PMLR, 2023a.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,\\n2023b.\\nAdi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Constructing benchmarks and interventions\\nfor combating hallucinations in llms. arXiv preprint arXiv:2404.09971, 2024.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Is multihop qa in dire condition?\\nmeasuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , pp. 8846–8863, 2020.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition. Transactions of the Association for Computational Linguistics , 10:\\n539–554, 2022.\\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\\nZhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv\\npreprint arXiv:2310.03214, 2023.\\nTu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational\\nautoraters: Taming large language models for better automatic evaluation. arXiv preprint arXiv:2407.10817,\\n2024.\\nYuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware\\nretrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497 ,\\n2024a.\\nZilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang,\\nAnush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented generation\\nthrough drafting. arXiv preprint arXiv:2407.08223, 2024b.\\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do irrelevant inputs\\nskew the responses of large language models? In COLM 2024, 2024.\\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing\\nthe behavior of large language models in knowledge conflicts. In International Conference on Learning\\nRepresentations (ICLR), 2024.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv\\npreprint arXiv:2401.15884, 2024.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings\\nof the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computa-\\ntional Linguistics, 2018.\\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language\\nmodels know what they don’t know? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),\\nFindings of the Association for Computational Linguistics: ACL 2023 , pp. 8653–8665, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.551. URL\\nhttps://aclanthology.org/2023.findings-acl.551.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust\\nto irrelevant context. In International Conference on Learning Representations (ICLR) , 2024.\\nTan Yu, Anbang Xu, and Rama Akkiraju. In defense of rag in the era of long-context language models. arXiv\\npreprint arXiv:2409.01666, 2024.\\nHanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and\\nTong Zhang. R-tuning: Instructing large language models to say ‘i don’t know’. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (V olume 1: Long Papers), pp. 7106–7132, 2024a.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Liu Yong, and Shen Huang. End-to-end beam retrieval for\\nmulti-hop question answering. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of\\nthe 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (V olume 1: Long Papers) , pp. 1718–1731, Mexico City, Mexico, June\\n2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.96. URL https:\\n//aclanthology.org/2024.naacl-long.96.\\nYujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large\\nlanguage models. In Proceedings of the ACM on Web Conference 2024 , pp. 1453–1463, 2024.\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and\\nreading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774,\\n2021.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nA S UPPORTING EXPERIMENT INFORMATION\\nWe provide sufficient details to reproduce all of our experiments. We list all of the prompts that we\\nuse in the next section.\\nA.1 M ODELS\\nGPT 4o . We use the API-accessible gpt-4o-2024-08-06 model, released on August 6,\\n2024 (Achiam et al., 2023).\\nGemini 1.5 Pro. We use the API-accessible gemini-1.5-pro-0514 model, released on May\\n14, 2024 (Gemini Team et al., 2023).\\nClaude 3.5 Sonnet. We use the only API-accessible claude-3-5-sonnet-20240620 model,\\nreleased on June 20, 2024 (Anthropic, 2024).\\nGemma 2 27B. We use the publicly available instruction tunedgemma-2-27b-it model, released\\non Jun 27, 2024 (Gemma Team et al., 2024).\\nMistral 3 7B. We use the publicly available instruction tuned Mistral-7B-Instruct-v0.3\\nmodel, released on May 22, 2024 (Jiang et al., 2023).\\nFLAMe. We use the published FLAMe-RM-24B model (Vu et al., 2024).\\nTRUE-NLI model. Calculated the maximum probability over the chunks in the context. Use a\\nthreshold of 0.05, where if the maximum probability is higher then this, then we classify as ‘sufficient\\ncontext’. The threshold of 0.05 achieved the highest F1 score on our human labeled dataset. We use\\nthe t5_xxl_true_nli_mixture version of their model (Honovich et al., 2022).\\nA.2 F INE -TUNING SETTINGS\\nIn our fine-tuning setup, we employed the LoRA adaptation technique (Hu et al., 2022) to fine-tune\\nMistral-7B-Instruct-v0.31. We used either a 2,000-example random subset sampled from the training\\nset of the Musique-Ans dataset or from the development set of the HotPotQA data 2. The prompt\\ntemplate for finetuning is provided in Appendix D.\\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were\\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 × 10−5. We note that the\\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\\nA.3 D ATASETS\\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\\nall ‘True Premise’ questions from FreshQA.\\nRetrieval for HotpotQA. We adopt the FlashRAG framework (Jin et al., 2024) to implement our\\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\\nutilizing ‘intfloat/e5-base-v2‘ from Hugging Face’s model hub as a Dense Retriever3. For each query,\\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\\nwithin a prompt template for inference.\\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\\ninput context and ensembling output probabilities across different passes. The REPLUG method is\\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\\n1Available at huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\\n3huggingface.co/intfloat/e5-base-v2\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nRetrieval for FreshQA We use the urls provided in the FreshQA dataset as retrieval for the\\ncontext. We scraped each url and discarded extra HTML content such as headers, footers, and\\nnavigation. We include the title of each webpage in the text, convert any included tables to markdown,\\nand include the table title immediately before the table in the text. When splitting the tables and text\\nfor smaller context windows, we keep tables and sentences intact when possible. For large tables that\\nrequire splitting, we duplicate the table row column headers to include them in each chunk.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 3: Fine-tuned (FT) Mistral 3 7B Instruct. We compare closed book and vanilla RAG with\\nthree FT settings, measuring % Correct ( %C), % Abstain (%A), and % Hallucinate ( %H). Also,\\n“idk” means we change the answer in training samples to be “I don’t know” instead of the given\\nanswer (either for 20% of random examples, or 20% of examples with insufficient context). Best\\n%C for each model/dataset in bold.\\nMusique HotPotQA\\nModel Variant RAG %C %A %H %C %A %H\\nMistral Closed Book 6.6 29.8 63.6 32 7.6 60.4\\n\" Vanilla RAG ✓ 28.8 11.8 59.4 46.6 9.2 44.2\\n\" FT GT answer (Data Mix 1) ✓ 31.4 0 68.6 43.4 0 56.6\\n\" FT idk 20% rand. (Data Mix 2) ✓ 23 1.2 75.8 41.6 0.8 57.6\\n\" FT idk 20% insuff. (Data Mix 3) ✓ 23 2.2 74.8 41.2 2 56.8\\nB A DDITIONAL RESULTS\\nB.1 F INE -TUNING FULL RESULTS\\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide\\nsufficient context labels. However, we would incur significant overhead if we used a 24B model\\nto improve the generation of a much smaller LLM. Instead, we try directly fine-tuning Mistral 3\\n7B to increase accuracy with retrieval. Specifically, we experiment with different data mixtures to\\nencourage the model to output “I don’t know” instead of generating an incorrect response.\\nFine-tuning Data. We repeat the following process separately for each of the Musique-Ans and\\nHotPotQA datasets to create three mixtures of training data with different answers for each dataset.\\nFirst, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change\\nthe answer to “I don’t know” before fine-tuning. For Data Mix 3, we instead randomly choose 400\\nexamples (20%) that our autorater labels as insufficient context and change their answer to “I don’t\\nknow” while keeping the other answers as the ground truth. Our hypothesis is that fine-tuning on\\nData Mix 2 and 3 should steer the model to abstain more and hallucinate less than with Data Mix 1.\\nModels, Methods, Metrics. Using the three data mixtures described above, we fine-tune the Mistral\\n3 7B Instruct model model with LoRA (details in Appendix A.2). At inference time, we use the\\nstandard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla\\nRAG). Consistent with the prior experiments, we use an LLM with ground truth answers to classify\\nresponses as Correct (%C), Abstention (%A), or Hallucination (%H).\\nFine-tuning Results and Discussion. Table 3 shows our experimental results. We verify that the FT\\nvariants have a higher rate of generating correct answers (%C) compared to closed-book and Vanilla\\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\\nshowing the impact of adding “I don’t know” in the training set. In general, FT models using RAG\\noutput incorrect answers (%H) much of the time, and often more than they abstain (%A).\\nB.2 P ERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\\nthe first column shows performance without RAG (closed-book) while the second column shows\\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\\ninsufficient context. The third and fourth columns show the performance of the second column\\nstratified by sufficient vs insufficient context respectively.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n100%\\n11.7%\\n10.0%\\n78.3%\\n1.7%\\n9.1%\\n89.1%\\n46.1%\\n12.7%\\n41.2%\\n40.3%\\n8.6%\\n51.1%\\n11.7%\\n9.3%\\n79.0%\\n4.0%\\n6.9%\\n89.1%\\n38.2%\\n17.6%\\n44.1%\\n53.2%\\n11.8%\\n35.0%\\nWithout RAG\\n23.1%\\n54.1%\\n22.8%\\n    With RAG\\n18.3%54.2%\\n27.5%\\n    With RAG\\nSuff. Context\\n77.4% of Dataset\\n39.2%\\n53.9%\\n6.9%\\n    With RAG\\nInsuff. Context\\n22.6% of Dataset\\nCategorizing model responses (FreshQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\\nthat change based on the question’s timestamp, which may explain the frequent abstentions without\\nRAG (100% for Gemini 1.5 Pro).\\n68.4%\\n8.2%\\n23.4%\\n12.0%\\n30.2%\\n57.8%\\n6.5%\\n26.0%\\n67.5%\\n16.7%\\n33.8%\\n49.4%\\n26.2%25.8%\\n48.0%\\n10.0%\\n24.8%\\n65.2%\\n8.2%\\n19.9%\\n71.9%\\n11.5%\\n29.0%\\n59.5%\\n54.8%\\n18.0% 27.2%\\nWithout RAG\\n7.2%\\n42.8%\\n50.0%\\n    With RAG\\n1.7%\\n34.2%\\n64.1%\\n    With RAG\\nSuff. Context\\n46.2% of Dataset\\n11.9%\\n50.2%\\n37.9%\\n    With RAG\\nInsuff. Context\\n53.8% of Dataset\\nCategorizing model responses (HotpotQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without\\nRAG (e.g., 48.0% for GPT 4o).\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 4: Performance Analysis of RAG Systems Using Human-Annotated Sufficient Context\\nLabels. These tables include results on a curated set of challenging context-dependent questions.\\nTable (a) shows that while larger models generally achieve higher accuracy with sufficient context\\n(present in 54.8% of cases), even top performers exhibit a 14-16% error rate. Table (b) reveals that\\nwith insufficient context (45.2% of cases), models predominantly abstain from answering (50-73%\\nof instances), though significant hallucination rates (15-40%) persist. These patterns of context-\\ndependent performance and hallucination risk are consistent with our analyses of HotpotQA, FreshQA,\\nand Musique datasets, despite variations in absolute performance due to different task complexities.\\n(a) Performance with Sufficient Context (54.8% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 84.1 1.6 14.3\\nGPT 4o 82.5 4.8 12.7\\nClaude 3.5 Sonnet 85.7 11.1 3.2\\nGemini 1.5 Flash 77.8 4.8 17.5\\nGemma 27B 71.4 3.2 25.4\\n(b) Performance with Insufficient Context (45.2% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 9.6 50.0 40.4\\nGPT 4o 23.1 61.5 15.4\\nClaude 3.5 Sonnet 9.6 53.8 36.5\\nGemini 1.5 Flash 7.7 73.1 19.2\\nGemma 27B 9.6 55.8 34.6\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nB.3 C OMPARISON OF QA E VALUATION METRICS\\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\\ncorrectness.\\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\\nour sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\\nthough both metrics reveal similar patterns in model behavior.\\nTable 5: Comparison of evaluation metrics across models and datasets. We show results for\\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\\nFreshQA Musique HotpotQA\\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\\n1. Different formatting affects matching:\\nQ: What date did the creator of Autumn Leaves die?\\nGround Truth: 13 August 1896\\nResponse: August 13, 1896.\\nContains Answer: False\\nLLMEval: Correct\\n2. Semantic equivalents are not captured:\\nQ: What former Los Angeles Lakers majority owner is the\\nfather of Jeanie Marie Buss?\\nGround Truth: Gerald Hatten Buss\\nResponse: Jerry Buss.\\nContains Answer: False\\nLLMEval: Correct\\n3. Partial matches can be marked as correct:\\nQ: What is Amazon Prime Video’s most watched premiere ever?\\nGround Truth: The Rings of Power\\nResponse: The series explores the forging of the Rings of Power,\\nthe rise of Sauron...\\nContains Answer: True\\nLLMEval: Hallucinate\\nThe LLM QA evaluator provides several practical advantages:\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n• Handles variations in model verbosity and formatting\\n• Distinguishes between correct, abstain, and incorrect responses\\n• Enables efficient evaluation across multiple datasets\\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC P ROMPTS\\nC.1 S UFFICIENT CONTEXT AUTORATER PROMPT\\nYou are an expert LLM evaluator that excels at evaluating a QUESTION and REFERENCES.\\nConsider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp <TIMESTAMP>.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION.\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\nEXAMPLE:\\n### QUESTION\\nIn which year did the publisher of Roald Dahl’s Guide to Railway Safety cease to exist?\\n### References\\nRoald Dahl’s Guide to Railway Safety was published in 1991 by the British Railways Board.\\nThe British Railways Board had asked Roald Dahl to write the text of the booklet, and\\nQuentin Blake to illustrate it, to help young people enjoy using the railways safely. The\\nBritish Railways Board (BRB) was a nationalised industry in the United Kingdom that\\noperated from 1963 to 2001. Until 1997 it was responsible for most railway services in Great\\nBritain, trading under the brand name British Railways and, from 1965, British Rail. It\\ndid not operate railways in Northern Ireland, where railways were the responsibility of the\\nGovernment of Northern Ireland.\\n### EXPLANATION\\nThe context mentions that Roald Dahl’s Guide to Railway Safety was published by the\\nBritish Railways Board. It also states that the British Railways Board operated from 1963 to\\n2001, meaning the year it ceased to exist was 2001. Therefore, the context does provide a\\nprecise answer to the question.\\n### JSON\\n{\"Sufficient Context\": 1}\\nRemember the instructions: You are an expert LLM evaluator that excels at evaluating a\\nQUESTION and REFERENCES. Consider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp TIMESTAMP.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\n### QUESTION\\n<question>\\n### REFERENCES\\n<context>\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.2 FLAM E PROMPT\\nINSTRUCTIONS:\\ntitle: Is the context sufficient to infer the answer to the question?\\ndescription: In this task, you will be provided with documents and a question. Use one of the\\nfollowing labels under ’judgment’:\\n1. sufficient: The documents are not sufficient to infer the answer to the question.\\n2. insufficient: The documents are sufficient to infer the answer to the question.\\noutput_fields: judgment\\nCONTEXT:\\ndocuments:<references> question: <question>\\nC.3 LLME VAL PROMPT\\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\\nmatch the GT answers. Hence, we use an LLM to determine: the answers are the same (Correct)\\nor the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with\\nhuman judgments for this evaluating free form responses (Krishna et al., 2024). Responses resulting\\nin empty strings were classified as \"missing,\" while variations of \"I don’t know\" were also treated\\nas missing. We normalized both the ground truth answers and the model’s responses by removing\\npunctuation, converting to lowercase, and eliminating stop words.\\n===Task===\\nI need your help in evaluating an answer provided by an LLM against ground truth answers.\\nYour task is to determine if the LLM’s response matches the ground truth answers. Please\\nanalyze the provided data and make a decision.\\n===Instructions===\\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answers\". 2. Consider\\nthe substance of the answers – look for equivalent information or correct answers. Do not\\nfocus on exact wording unless the exact wording is crucial to the meaning.\\n3. Your final decision should be based on whether the meaning and the vital facts of the\\n\"Ground Truth Answers\" are present in the \"Predicted Answer.\" 4. Categorize the answer as\\none of the following:\\n- \"perfect\": The answer is completely correct and matches the ground truth.\\n- \"acceptable\": The answer is partially correct or contains the main idea of the ground truth.\\n- \"incorrect\": The answer is wrong or contradicts the ground truth.\\n- \"missing\": The answer is \"I don’t know\", \"invalid question\", or similar responses indicating\\nlack of knowledge.\\n===Input Data===\\n- Question: What 1876 battle featured the Other Magpie?\\n- Predicted Answer: The Other Magpie fought in the Battle of the Rosebud.\\n- Ground Truth Answers: Battle of the Rosebud\\n===Output Format===\\nProvide your evaluation in the following format:\\nExplanation: (How you made the decision)\\nDecision: (One of \"perfect\", \"acceptable\", \"incorrect\", or \"missing\")\\nPlease proceed with the evaluation.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.4 D ATASET QUESTION ANSWER PROMPTS\\nThe CoT prompt instructs the model to provide an accurate and concise answer based solely on\\nthe given search results, using an unbiased and journalistic tone. The prompt includes an example\\nquestion, references, and answer to guide the model’s response format. To extract the final answer,\\nwe implemented a pattern matching technique on the model’s response, specifically targeting the text\\nfollowing \"The answer is:\" for CoT prompts.\\nChain of Thought (CoT)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Start with an accurate, engaging, and concise\\nexplanation based only on the provided documents. Must end with \"The answer is:\". Use an\\nunbiased and journalistic tone.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nAnswer Only (AO)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Do not say anything other than the answer itself.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nD F INE -TUNING AND RAG P ROMPTS FOR MISTRAL\\nFinetuning Prompt (FT)\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n<answer>\\nEvaluation Without RAG Prompt\\nAnswer the question based on your own knowledge. Only give me the answer and do not\\noutput any other words. Please follow the following guideline when formulating your answer:\\nif you are uncertain or don’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\nEvaluation With RAG Prompt\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n25')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02784a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting and getting them into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d9aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 51 documents into 230 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: . \n",
      ". \n",
      "Latest updates: hps://dl.acm.org/doi/10.1145/3681780.3697252\n",
      ". \n",
      ". \n",
      "RESEARCH-ARTICLE\n",
      "Automating Bibliometric Analysis with Sentence Transformers and\n",
      "Retrieval-Augmented Generation (RAG): A Pilot...\n",
      "Metadata: {'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='. \\n. \\nLatest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3681780.3697252\\n. \\n. \\nRESEARCH-ARTICLE\\nAutomating Bibliometric Analysis with Sentence Transformers and\\nRetrieval-Augmented Generation (RAG): A Pilot Study in Semantic and\\nContextual Search for Customized Literature Characterization for High-\\nImpact Urban Research\\nHAOWEN XU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nXUEPING LI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJOSE TUPAYACHI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJIANMING (JAMIE) LIAN, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nOLUFEMI A OMITAOMU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\n. \\n. \\nOpen Access Support provided by:\\n. \\nOak Ridge National Laboratory\\n. \\nThe University of Tennessee, Knoxville\\n. \\nPDF Download\\n3681780.3697252.pdf\\n28 January 2026\\nTotal Citations: 2\\nTotal Downloads: 314\\n. \\n. \\nPublished: 29 October 2024\\n. \\n.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content=\". \\nThe University of Tennessee, Knoxville\\n. \\nPDF Download\\n3681780.3697252.pdf\\n28 January 2026\\nTotal Citations: 2\\nTotal Downloads: 314\\n. \\n. \\nPublished: 29 October 2024\\n. \\n. \\nCitation in BibTeX format\\n. \\n. \\nSIGSPATIAL '24: The 32nd ACM\\nInternational Conference on Advances in\\nGeographic Information Systems\\nOctober 29 - November 1, 2024\\nGA, Atlanta, USA\\n. \\n. \\nConference Sponsors:\\nSIGSPATIAL\\nUrbanAI '24: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI (October 2024)\\nh\\ue03cps://doi.org/10.1145/3681780.3697252\\nISBN: 9798400711565\\n.\"),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers\\nand Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature\\nCharacterization for High-Impact Urban Research\\nHaowen Xu\\nxuh4@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nXueping Li\\nXueping.Li@utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJose Tupayachi\\njtupayac@vols.utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJianming (Jamie) Lian\\nlianj@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nOlufemi A Omitaomu\\nomitaomuoa@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nABSTRACT\\nBibliometric analysis is essential for understanding research trends,\\nscope, and impact in urban science, especially in high-impact jour-\\nnals, such Nature Portfolios. However, traditional methods, relying\\non keyword searches and basic NLP techniques, often fail to uncover'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='nals, such Nature Portfolios. However, traditional methods, relying\\non keyword searches and basic NLP techniques, often fail to uncover\\nvaluable insights not explicitly stated in article titles or keywords.\\nThese approaches are unable to perform semantic searches and\\ncontextual understanding, limiting their effectiveness in classifying\\ntopics and characterizing studies. In this paper, we address these\\nlimitations by leveraging Generative AI models, specifically trans-\\nformers and Retrieval-Augmented Generation (RAG), to automate\\nand enhance bibliometric analysis. We developed a technical work-\\nflow that integrates a vector database, Sentence Transformers, a\\nGaussian Mixture Model (GMM), Retrieval Agent, and Large Lan-\\nguage Models (LLMs) to enable contextual search, topic ranking,\\nand characterization of research using customized prompt tem-\\nplates. A pilot study analyzing 223 urban science-related articles\\npublished in Nature Communications over the past decade high-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='and characterization of research using customized prompt tem-\\nplates. A pilot study analyzing 223 urban science-related articles\\npublished in Nature Communications over the past decade high-\\nlights the effectiveness of our approach in generating insightful\\nsummary statistics on the quality, scope, and characteristics of\\npapers in high-impact journals. This study introduces a new para-\\ndigm for enhancing bibliometric analysis and knowledge retrieval\\nin urban research, positioning an AI agent as a powerful tool for\\nadvancing research evaluation and understanding.\\nThis manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-\\n00OR22725 with the US Department of Energy (DOE). The US government retains\\nand the publisher, by accepting the article for publication, acknowledges that the\\nUS government retains a nonexclusive, paid-up, irrevocable, worldwide license to\\npublish or reproduce the published form of this manuscript, or allow others to do'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='US government retains a nonexclusive, paid-up, irrevocable, worldwide license to\\npublish or reproduce the published form of this manuscript, or allow others to do\\nso, for US government purposes. DOE will provide public access to these results\\nof federally sponsored research in accordance with the DOE Public Access Plan\\n(http://energy.gov/downloads/doe-public-access-plan).\\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\\nauthored or co-authored by an employee, contractor or affiliate of the United States\\ngovernment. As such, the Government retains a nonexclusive, royalty-free right to\\npublish or reproduce this article, or to allow others to do so, for Government purposes\\nonly.\\nUrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1156-5/24/10. . . $15.00\\nhttps://doi.org/10.1145/3681780.3697252\\nKEYWORDS'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1156-5/24/10. . . $15.00\\nhttps://doi.org/10.1145/3681780.3697252\\nKEYWORDS\\nBibliometrics Analysis, Large Language Models, Retrieval-Augmented\\nGeneration, Transformers\\nACM Reference Format:\\nHaowen Xu, Xueping Li, Jose Tupayachi, Jianming (Jamie) Lian, and Olufemi\\nA Omitaomu. 2024. Automating Bibliometric Analysis with Sentence Trans-\\nformers and Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature Characteri-\\nzation for High-Impact Urban Research . In 2nd ACM SIGSPATIAL In-\\nternational Workshop on Advances in Urban-AI (UrbanAI’24), October 29-\\nNovember 1 2024, Atlanta, GA, USA. ACM, Seattle, WA, USA, 7 pages.\\nhttps://doi.org/10.1145/3681780.3697252\\n1 INTRODUCTION\\nBibliometric analysis is a widely used method for evaluating and\\nmapping research trends, impact, and scope across various scien-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='https://doi.org/10.1145/3681780.3697252\\n1 INTRODUCTION\\nBibliometric analysis is a widely used method for evaluating and\\nmapping research trends, impact, and scope across various scien-\\ntific domains [ 2]. It provides quantitative insights by analyzing\\npublication records, citations, and other scholarly outputs, helping\\nresearchers and policymakers understand the evolution of specific\\nfields [4]. Over the past few decades, bibliometric analysis has\\nevolved from basic citation counts and keyword frequency met-\\nrics to more sophisticated approaches, incorporating co-authorship\\nnetworks, citation flows, and research topic clusters [7]. These meth-\\nods are particularly important in fields like urban science, where\\nemerging topics such as smart cities require continuous monitor-\\ning to shape the direction of future research and innovation [ 5].\\nBibliometric analysis plays a key role in identifying influential\\nworks, emerging themes, and research gaps, thus guiding strategic'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='ing to shape the direction of future research and innovation [ 5].\\nBibliometric analysis plays a key role in identifying influential\\nworks, emerging themes, and research gaps, thus guiding strategic\\ndecision-making in urban science and smart city development [15].\\nHowever, traditional bibliometric methods face several limita-\\ntions. Most rely heavily on keyword searches and basic text mining\\ntechniques, which depend on exact matches of terminologies and\\npredefined keywords. These techniques often miss critical insights\\nthat are not explicitly captured in the titles or abstracts of research\\narticles, thereby limiting the ability to fully understand and clas-\\nsify research topics [8]. Furthermore, traditional natural language\\nprocessing (NLP) approaches, such as term frequency-inverse doc-\\nument frequency (TF-IDF) or simple word co-occurrence metrics,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nfail to capture the semantic meaning and contextual relationships\\nbetween concepts[9]. Although topic modeling methods like La-\\ntent Dirichlet Allocation (LDA) can offer significant benefits for\\nbibliometric analysis by providing deeper insights into the rela-\\ntionships and structures within research literature [ 1], they are\\nprimarily used for uncovering thematic structures and classifying\\narticle topics and are not designed for enabling semantic search or\\nproviding a contextual understanding of an article that involves\\ndeeper reasoning and interpretation. As a result, traditional biblio-\\nmetric analysis often falls short in generating deeper insights that\\nrequire a thorough review and interpretation of the article’s full\\ncontent. Relying primarily on the analysis of titles, keywords, and\\nstandard metadata, limits the ability to provide a more customized'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='content. Relying primarily on the analysis of titles, keywords, and\\nstandard metadata, limits the ability to provide a more customized\\nand nuanced characterization of research based on the full textual\\ndata.\\nRecent advancements in generative AI models, such as large\\nlanguage models (LLMs), have opened new opportunities for en-\\nhancing research [11]. These models, including transformers and\\nRetrieval-Augmented Generation (RAG) systems, excel at seman-\\ntic understanding and contextual interpretation of complex texts,\\nmaking them highly suitable for extracting valuable insights from\\nresearch articles and technical manuals [10, 14]. In the field of urban\\ninformatics, LLMs have been increasingly applied to analyze large\\nvolumes of text, uncovering patterns and trends that traditional\\nmethods would overlook [6, 13]. In this paper, we propose a novel\\ntechnical workflow for automating and enhancing bibliometric\\nanalysis by integrating Vector Databases, Sentence Transformers,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='methods would overlook [6, 13]. In this paper, we propose a novel\\ntechnical workflow for automating and enhancing bibliometric\\nanalysis by integrating Vector Databases, Sentence Transformers,\\nGaussian Mixture Models (GMM), Retrieval Agents, and an LLM.\\nOur approach enables contextual search, topic ranking, and cus-\\ntomized characterization of research articles, which we demonstrate\\nthrough a pilot study analyzing 201 urban science-related articles\\npublished in Nature Communications over the past decade. This\\nwork addresses the limitations of traditional bibliometric meth-\\nods, introducing a new paradigm for urban research analysis and\\nknowledge retrieval through the development of AI agents with\\ncontextual understanding and reasoning capabilities.\\n2 LITERATURE REVIEW\\nTo overcome the limitations and knowledge gaps in traditional\\nbibliometric analysis, recent studies have employed generative AI\\nmodels, particularly transformer-based language models, to auto-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='To overcome the limitations and knowledge gaps in traditional\\nbibliometric analysis, recent studies have employed generative AI\\nmodels, particularly transformer-based language models, to auto-\\nmate and enhance bibliometric methodologies.\\nFijačko et al. [3] explores the application of generative AI in\\nbibliometric analysis, focusing on 10 years of research abstracts\\nfrom the European Resuscitation Congresses (ERC). Using ChatGPT-\\n4, the study classified 2,491 abstracts into ERC guideline topics, with\\nBasic Life Support and Adult Advanced Life Support being the most\\nfrequent. The research highlights the potential of large language\\nmodels like ChatGPT-4 in categorizing and analyzing scientific\\nliterature and identifying trends. However, challenges included\\npotential misclassification, the limited use of abstract titles rather\\nthan full-text, and heavy reliance on the model’s capabilities. These\\nconstraints highlight the challenges of automating bibliometric'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='than full-text, and heavy reliance on the model’s capabilities. These\\nconstraints highlight the challenges of automating bibliometric\\nanalysis in the absence of comprehensive datasets. However, the\\nstudy effectively showcases the potential of AI to significantly\\nimprove bibliometric methodologies despite these limitations.\\nWeng et al. [12] introduces a methodology for detecting and\\nvisualizing key research topics using GPT-3 embeddings and the\\nHDBSCAN clustering algorithm on 593 abstracts related to urban\\nstudies and machine learning. By clustering abstracts based on\\nsemantic similarity and extracting keywords using the Maximal\\nMarginal Relevance (MMR) algorithm, the study provides an in-\\nteractive tool for exploring abstract clusters and their associated\\ntopics. Challenges included optimizing clustering parameters and\\nrelying solely on abstracts, which may not fully represent the re-\\nsearch. Some clusters contained outliers or minimal data, affecting'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='topics. Challenges included optimizing clustering parameters and\\nrelying solely on abstracts, which may not fully represent the re-\\nsearch. Some clusters contained outliers or minimal data, affecting\\naccuracy. Despite these limitations, the study demonstrates the\\npotential of transformer-based models in facilitating unsupervised\\nbibliometric analysis, though refinement is needed.\\nBoth articles emphasize the benefits of transformer-based and\\nlarge language models for bibliometric analysis, while also address-\\ning critical limitations such as data quality, optimization challenges,\\nand input constraints when working with abstract-based datasets.\\nTo overcome these challenges, there is a need to harness recent\\nadvancements in sentence transformer models and RAG technolo-\\ngies. These innovations can enable the development of an AI agent\\ncapable of advanced contextual understanding of research articles,\\nfacilitating semantic search and providing tailored insights based on'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='capable of advanced contextual understanding of research articles,\\nfacilitating semantic search and providing tailored insights based on\\nuser-specific queries. This, in turn, can generate new bibliometric\\nmetrics, offering deeper and more comprehensive analysis.\\n3 METHODOLOGY\\nThis section starts by outlining the design requirements for our\\nproposed methods, then presents the conceptual workflow and its\\nimplementation, which combines Generative AI techniques with\\nstatistical models.\\n3.1 Design Requirements\\nOverall, we aim to develop an AI-agent styled tool that can interact\\nwith users, who are primarily researchers and college students,\\nthrough human nature conversations, to get their inquire on the\\ncurrent-state of cutting edge research in a specific domain, such as\\nsmart city and urban science. Based on the inquiry, our workflow\\nwill automate a sequence of procedures that leverage the unique\\ncapabilities of sentence transformers and RAG techniques on a'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='smart city and urban science. Based on the inquiry, our workflow\\nwill automate a sequence of procedures that leverage the unique\\ncapabilities of sentence transformers and RAG techniques on a\\nbatch of selected literature filtered and downloaded from academics\\ndatabases, such as Scopous, IEEE Xplore, and Web of Science. Aim-\\ning to shed lights on more advanced, intelligent, and automonous\\nbiblimetric analysis, our workflow aims to enable the following\\nfeatures:\\nConversational Interaction: A chatbot-style interface will\\nbe implemented, allowing users to ask questions through nat-\\nural human conversations, without the need for pre-defined\\nkeywords or technical jargon. This feature will enable users\\nto define search and filter criteria for subsets of bibliographic\\ndata (e.g., research articles, conference proceedings, techni-\\ncal reports, and manuals) that have been pre-selected and\\ndownloaded from popular academic databases. The search'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='data (e.g., research articles, conference proceedings, techni-\\ncal reports, and manuals) that have been pre-selected and\\ndownloaded from popular academic databases. The search\\nprocess will be guided by broad categories, such as domains,\\ndisciplines, and journals, to streamline access to relevant\\nliterature.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nFigure 1: The overall design of the transformer and RAG-powered workflow.\\nSemantic and Contextual Search: Based on the user-defined\\ninquiry, this process matches and retrieves relevant research\\ndocuments or specific sections by analyzing the underly-\\ning meaning and contextual relationships between words,\\nrather than relying solely on keyword matching. The use of\\nsentence transformers and text embeddings, enables users\\nto access information and knowledge based on conceptual\\nrelevance, rather than simple term frequency. This enhances\\nthe precision of literature filtering and facilitates deeper,\\nmore insightful knowledge discovery, which will plays im-\\nportant role as the retrieval agent within the RAG paradigm\\nto benefit further analytics using Generative AI models.\\nCustomized Literature Characterization: Using the output'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='portant role as the retrieval agent within the RAG paradigm\\nto benefit further analytics using Generative AI models.\\nCustomized Literature Characterization: Using the output\\nliterature from the semantic and contextual search as input,\\nGenerative Pre-trained Transformer (GPT) models will be\\nemployed for contextual understanding, reasoning, and in-\\nterpretation. These GPT models will process user inquiries\\nto generate customized characterizations and interpretations\\nof the selected literature, providing deeper insights and cre-\\nating more sophisticated metrics for advanced bibliometric\\nanalyses. This approach aims to enhance the overall un-\\nderstanding of research trends and offer tailored, in-depth\\nevaluations of the literature.\\nAs an example of our end-user capability, a user could ask the\\nchatbot, powered by our method, a question like, “What percentage\\nof research published in Computers, Environment and Urban Sys-\\ntems over the past 5 years in the urban mobility sector uses traffic'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='chatbot, powered by our method, a question like, “What percentage\\nof research published in Computers, Environment and Urban Sys-\\ntems over the past 5 years in the urban mobility sector uses traffic\\nsimulation-based methods versus crowd-sourced data-driven meth-\\nods, and what are their spatial scales?” The semantic and contextual\\nsearch then filters and retrieves relevant articles based on the query,\\nranks them by relevance, and feeds them to the generative AI model.\\nThis enables advanced contextual understanding and reasoning to\\nprovide customized characterizations on individual research’s sim-\\nulation types and spatial scales, which involve information often\\nnot found in keywords or titles. These characterizations can be later\\nused to generate summary statics and insights to facilitate more\\ndetailed trend analysis and thematic mapping.\\n3.2 Workflow Design\\nOur workflow consists of four key procedures, as depicted in Figure'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='used to generate summary statics and insights to facilitate more\\ndetailed trend analysis and thematic mapping.\\n3.2 Workflow Design\\nOur workflow consists of four key procedures, as depicted in Figure\\n1. The workflow is later implemented in a Jupyter lab environment\\nusing Python-based libraries. Each procedure is detailed through\\nthe following following list.\\n3.2.1 Bibliography Selection and Data Preparation. In the first step,\\nusers select literature based on generic search criteria such as disci-\\npline, publication year, and journal name. Data is extracted from'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nacademic databases like Scopus, IEEE Xplore, and SerpApi using\\ntheir respective web services and platforms, or through custom-\\nbuilt web scrapers, such as those powered by SerpAPI. The retrieved\\ndata includes bibliographic summaries in CSV format and individ-\\nual articles in formats like PDF and HTML, which are then stored\\nin a file-based system for further processing.\\n3.2.2 Text Embedding and Data Warehousing. After retrieving the\\nessential documents, a Python script powered by PyPDF2 is used\\nto parse the bibliographic summaries, which include the list of\\ndownloaded articles along with supportive metadata (e.g., authors,\\nyear, source, citations, and h-index), as well as the PDF and HTML\\nversions of the individual articles. This parsing process is designed\\nto upload key textual information into a datastore, building the\\nknowledge base for the proposed AI agent. Unlike traditional infor-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='to upload key textual information into a datastore, building the\\nknowledge base for the proposed AI agent. Unlike traditional infor-\\nmation and content management systems, our workflow utilizes\\na sentence transformer, specifically the all-MiniLM-L6-v2 model\\nfrom Hugging Face, to generate text embeddings—vector represen-\\ntations that encode the semantic and contextual meaning of the text.\\nCompared with traditional NLP methods, sentence transformers,\\nwith its unique self-attention mechanism, have superior advantages\\nin capturing semantic meaning, enabling contextual understanding,\\nhandling synonyms, and long-range dependencies between words\\nin a sentence. These embeddings facilitate more efficient semantic\\nand contextual searches in later stages of the workflow. The text\\nembeddings, along with essential metadata and article content, are\\nuploaded into the datastore. We selected Neo4j, a graph database, as\\nthe datastore for this workflow due to its graph data model, which'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='uploaded into the datastore. We selected Neo4j, a graph database, as\\nthe datastore for this workflow due to its graph data model, which\\nbetter represents the relationships between data entities stored as\\nnodes in the database. In our project, individual articles are repre-\\nsented as nodes within Neo4j, with associated metadata, content,\\nand text embeddings stored as properties of each node.\\n3.2.3 Semantic and Contextual Search. In the third step, the work-\\nflow enables semantic and contextual searches within the literature\\nstored in the knowledge base, leveraging the Neo4j database and\\nsentence transformers. User queries, collected through a chatbot\\ninterface, serve as input for this advanced search. The core function-\\nality compares the text embeddings of the user queries with those\\nof the article contents. We employ an enhanced cosine similarity\\nanalysis, as described in Eq. 1, to calculate a similarity score ranging\\nfrom 0 to 1, where 0 represents complete irrelevance and 1 repre-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='analysis, as described in Eq. 1, to calculate a similarity score ranging\\nfrom 0 to 1, where 0 represents complete irrelevance and 1 repre-\\nsents high relevance. Our implementation extends the standard\\ncosine similarity formula by using Python to chunk the original\\narticle content into sections and paragraphs, enabling more granu-\\nlar comparisons between the query and specific parts of the article.\\nThis process is applied to each article in the database, generating a\\nsimilarity score based on semantic similarity with the user’s query.\\nAt the contextual level, the framework evaluates the query’s con-\\ntext and intelligently selects embeddings from different sections of\\nthe articles to perform a targeted and accurate search.\\nSimilarity Score = a · b\\n∥a∥ ∥b∥ (1)\\nTo draw the decision boundary based on a list of individual\\narticle’s similarity score, we employed GMM to rank and cluster\\narticles by their similarity score, which reflects their relevance. A'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='To draw the decision boundary based on a list of individual\\narticle’s similarity score, we employed GMM to rank and cluster\\narticles by their similarity score, which reflects their relevance. A\\nGMM is a probabilistic model that represents a distribution of data\\nas a mixture of multiple Gaussian (normal) distributions, each char-\\nacterized by its own mean and variance, making it effective for\\nmodeling complex, multimodal datasets. We employed the Akaike\\nInformation Criterion (AIC) and Bayesian Information Criterion\\n(BIC), alongside the elbow method, to determine the optimal num-\\nber of clusters for the Gaussian Mixture Model (GMM) analysis.\\nAfter the clustering analysis, the cluster with the highest average\\nsimilarity scores implies it contains the most relevant articles, which\\nare also further ranked based on its similarity score.\\nArticles in the top-ranked clusters are subsequently fed into\\ngenerative AI models, specifically GPT, to enable more in-depth'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='are also further ranked based on its similarity score.\\nArticles in the top-ranked clusters are subsequently fed into\\ngenerative AI models, specifically GPT, to enable more in-depth\\nanalysis and interpretation. The semantic and contextual search\\nwithin this workflow is a critical component of the RAG paradigm,\\nallowing for further subsetting and refining of input information\\nto ensure more accurate and relevant results. This process also\\nhelps prevent exceeding the token limits of the GPT model context\\nwindow by optimizing the selection of input texts.\\n3.2.4 Customized Article Characterization. The top-ranked clus-\\nters, containing the most relevant articles, are then imported into a\\nGPT model as an external knowledge source to generate customized\\ncharacterizations for each article. These tailored bibliographic char-\\nacteristics serve as metrics, providing more detailed descriptions\\nand classifications of the articles. This approach uncovers valuable'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='acteristics serve as metrics, providing more detailed descriptions\\nand classifications of the articles. This approach uncovers valuable\\ninsights into research trends, focusing on individual articles’ topics,\\ntechnologies, methods, and contributions.\\nWe leverage the contextual reasoning capabilities of large lan-\\nguage models to classify and justify findings based on the semantic\\nmeaning of sections and paragraphs within the articles, extracting\\nuseful information without relying on precise names or keywords.\\nThis process is guided by instructional prompting strategies, where\\nwe design engineered prompt templates and feed them into the\\nGPT model along with the relevant article text segments (specific\\nsections). These segments are further refined and filtered based\\non their content relevance to ensure accurate classification and\\nextraction.\\nAt the technical level, we explored and tested the capabilities\\nof two GPT models, including a local instance of EleutherAI/gpt-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='extraction.\\nAt the technical level, we explored and tested the capabilities\\nof two GPT models, including a local instance of EleutherAI/gpt-\\nneo-1.3B models and the ChatGPT-3.5 Turbo API. Our experiments\\nreveals that small models with on 1.3B parameters suffer from\\nsevere hallucination, and are unable to analyze large size of tokens.\\nThe ChatGPT-3.5 API demonstrates stable performance, particularly\\nin its ability to process large text segments efficiently and produce\\nreasoned characterizations.\\n4 PILOT STUDY\\nThis pilot study aims to demonstrate the feasibility and performance\\nof our proposed methods. For this study, we compiled a dataset\\nof 223 high-impact urban research articles published in Nature\\nCommunications, obtained through the following Scopus query:\\nTITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management”\\nOR “urban planning” ) AND SRCTITLE ( “Nature Communications”\\n) AND PUBYEAR > 2013. We preprocessed the dataset by removing'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='TITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management”\\nOR “urban planning” ) AND SRCTITLE ( “Nature Communications”\\n) AND PUBYEAR > 2013. We preprocessed the dataset by removing\\nall intermediate versions labeled as “Author Correction” or “Pub-\\nlisher Correction. ” The final dataset consists of a CSV file containing\\nbibliometric summaries with all Scopus fields selected, along with\\n223 individual PDF documents of the actual articles.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n(a) Semantic and contextual search based on user’s query.\\n(b) Customized Literature Characterization using GPT’s reasoning capability.\\nFigure 2: Demonstration of the workflow through two use cases.\\n4.1 Use Case Demonstration\\nOur first use case on semantic and contextual search is demon-\\nstrated in Figure 2a, where the user submits an inquiry to identify\\narticles related to urban green space. The chatbot responds by visu-\\nalizing a histogram of similarity scores for all articles and displaying\\nthe GMM clusters of the articles. Additionally, a link is provided to'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\ndownload a CSV file that ranks and clusters the articles based on\\ntheir relevance to the user’s query.\\nOur second use case builds on the output articles from the first\\nuse case and demonstrates the capability to generate customized\\nliterature characterizations, creating new metrics for bibliometric\\nanalysis. Through the chatbot interface, users specify requests and\\ninstructions via prompts to guide the GPT model in generating tai-\\nlored metrics. Examples of these prompts are illustrated in Figure\\n2b. Based on the prompts, the GPT processes the 67 retrieved arti-\\ncles and their critical content, leveraging its contextual reasoning\\nability to derive new literature characteristics, which can then be\\ndeveloped into metrics and summary statistics. Figure 2b also visu-\\nalizes the responses to the user’s queries using pie charts and box\\nplots. Users can submit additional questions and custom requests'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='alizes the responses to the user’s queries using pie charts and box\\nplots. Users can submit additional questions and custom requests\\nthrough the chatbot to extract information and develop unique\\nmetrics tailored to the needs of bibliometric analysis.\\nOur major contribution lies in the development of an autonomous\\nAI agent designed to assist researchers in automating the charac-\\nterization and information extraction of large volumes of literature,\\nincluding datasets exceeding 1,000 articles. This system enables the\\ngeneration of in-depth insights for bibliometric analysis, signifi-\\ncantly enhancing the scalability and depth of literature review and\\nresearch trend identification processes. By automating these tasks,\\nthe AI agent offers a powerful tool for efficiently managing and\\nanalyzing extensive collections of scholarly articles, ultimately fa-\\ncilitating more comprehensive and insightful bibliometric analyses.\\n4.2 Limitation and Future Work'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='analyzing extensive collections of scholarly articles, ultimately fa-\\ncilitating more comprehensive and insightful bibliometric analyses.\\n4.2 Limitation and Future Work\\nDeveloped as a prototype for a more advanced knowledge base and\\nmanagement system, our workflow still faces a few limitations, as\\nthe following:\\nToken Size Limitation: The current implementation using\\nthe ChatGPT API has a maximum token size limitation and\\nincurs service fees based on the number of tokens processed.\\nThis makes it less suitable for analyzing large volumes of\\nliterature.\\nDatabase Query Performance: The current datastore imple-\\nmentation using the Neo4j database may encounter chal-\\nlenges in querying and managing large volumes of embed-\\nding data, as Neo4j is not optimized as a dedicated vector\\ndatabase.\\nLack of Evaluation and Validation: The GPT-generated lit-\\nerature characteristics are not currently evaluated by human\\nexperts, which introduces uncertainty regarding their accu-\\nracy and reliability.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='erature characteristics are not currently evaluated by human\\nexperts, which introduces uncertainty regarding their accu-\\nracy and reliability.\\nAs future work to address these limitations, we propose several\\nexperimental solutions. These include (a) deploying a local version\\nof large language models, such as GPT-Neo, to minimize service\\nfees for large-scale data analysis, (b) fine-tuning the GPT model\\nto reduce unnecessary content and instructions sent to the model,\\nthereby mitigating token size limitations, (c) transitioning our data-\\nstore implementation to dedicated vector databases, such as FAISS\\nor Pinecone, to enhance latency and accuracy, and (d) developing a\\ncomprehensive strategy to evaluate the GPT’s performance in ana-\\nlyzing and characterizing literature. Additionally, more advanced\\nbibliometric analysis methods could be integrated into the current\\nworkflow to extend its analytical capabilities.\\n5 CONCLUSION\\nIn this paper, we have presented a novel workflow that integrates'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='bibliometric analysis methods could be integrated into the current\\nworkflow to extend its analytical capabilities.\\n5 CONCLUSION\\nIn this paper, we have presented a novel workflow that integrates\\ngenerative AI models and advanced analytical techniques through\\nthe RAG paradigm to address the limitations of traditional biblio-\\nmetric analysis methods. By leveraging the contextual reasoning\\ncapabilities of large language models and enhanced semantic search\\ntechniques, our system offers a more nuanced and insightful analy-\\nsis of research literature. This approach, demonstrated through the\\nanalysis of urban science-related articles, enables customized char-\\nacterizations and generates new metrics for bibliometric analysis,\\nproviding deeper insights into research trends, methodologies, and\\ncontributions.\\nOur pilot study demonstrates the feasibility of this workflow,\\nshowcasing its ability to facilitate advanced semantic and contex-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='contributions.\\nOur pilot study demonstrates the feasibility of this workflow,\\nshowcasing its ability to facilitate advanced semantic and contex-\\ntual searches, cluster relevant articles, and produce tailored bib-\\nliographic insights through generative AI. However, the current\\nimplementation faces challenges, including token size limitations,\\ndatabase query performance issues, and the lack of expert evalua-\\ntion for the AI-generated results.\\nTo address these limitations, future work will explore the de-\\nployment of local language models, fine-tuning of GPT models to\\noptimize token usage, and transitioning to vector databases like\\nFAISS or Pinecone to improve performance. Additionally, we aim\\nto establish a comprehensive validation framework involving hu-\\nman experts to ensure the accuracy and reliability of the gener-\\nated bibliometric insights. As advancements in AI and bibliometric\\nmethodologies continue, our workflow has the potential to serve as'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='man experts to ensure the accuracy and reliability of the gener-\\nated bibliometric insights. As advancements in AI and bibliometric\\nmethodologies continue, our workflow has the potential to serve as\\na powerful and autonomous tool for researchers and policymakers\\nseeking to analyze and interpret vast bodies of scientific literature\\nmore effectively.\\n6 ACKNOWLEDGMENTS\\nThis work was supported by the U.S. Department of Energy (U.S\\nDOE), Advanced Research Projects Agency–Energy (ARPA-E) un-\\nder the project #DE-AR0001780. We thank our collaborators from\\nthe University of Tennessee Knoxville.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nREFERENCES\\n[1] Xieling Chen and Haoran Xie. 2020. A structural topic modeling-based biblio-\\nmetric study of sentiment analysis literature. Cognitive Computation 12 (2020),\\n1097–1129.\\n[2] Naveen Donthu, Satish Kumar, Debmalya Mukherjee, Nitesh Pandey, and\\nWeng Marc Lim. 2021. How to conduct a bibliometric analysis: An overview and\\nguidelines. Journal of business research 133 (2021), 285–296.\\n[3] Nino Fijačko, Ruth Masterson Creber, Benjamin S Abella, Primož Kocbek, Špela\\nMetličar, Robert Greif, and Gregor Štiglic. 2024. Using generative artificial intelli-\\ngence in bibliometric analysis: 10 years of research trends from the European\\nResuscitation Congresses. Resuscitation Plus 18 (2024), 100584.\\n[4] Ye-na Gan, Duo-duo Li, Nicola Robinson, and Jian-ping Liu. 2022. Practical guid-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Resuscitation Congresses. Resuscitation Plus 18 (2024), 100584.\\n[4] Ye-na Gan, Duo-duo Li, Nicola Robinson, and Jian-ping Liu. 2022. Practical guid-\\nance on bibliometric analysis and mapping knowledge domains methodology–A\\nsummary. European Journal of Integrative Medicine 56 (2022), 102203.\\n[5] Yi-Ming Guo, Zhen-Ling Huang, Ji Guo, Hua Li, Xing-Rong Guo, and Mpeoane Ju-\\ndith Nkeli. 2019. Bibliometric analysis on smart cities research. Sustainability 11,\\n13 (2019), 3606.\\n[6] Jianyuan Liang, Anqi Zhao, Shuyang Hou, Fengying Jin, and Huayi Wu. 2024. A\\nGPT-enhanced framework on knowledge extraction and reuse for geographic\\nanalysis models in Google Earth Engine. International Journal of Digital Earth\\n17, 1 (2024), 2398063.\\n[7] Luis Javier Cabeza Ramírez, Sandra M Sánchez-Cañizares, and Fernando J Fuentes-\\nGarcía. 2019. Past themes and tracking research trends in entrepreneurship: A\\nco-word, cites and usage count analysis. Sustainability 11, 11 (2019), 3121.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='García. 2019. Past themes and tracking research trends in entrepreneurship: A\\nco-word, cites and usage count analysis. Sustainability 11, 11 (2019), 3121.\\n[8] Rodrigo Romero-Silva and Sander De Leeuw. 2021. Learning from the past\\nto shape the future: A comprehensive text mining analysis of OR/MS reviews.\\nOmega 100 (2021), 102388.\\n[9] Iqra Safder and Saeed-Ul Hassan. 2019. Bibliometric-enhanced information\\nretrieval: a novel deep feature engineering approach for algorithm searching\\nfrom full-text publications. Scientometrics 119 (2019), 257–277.\\n[10] Jose Tupayachi, Haowen Xu, Olufemi A Omitaomu, Mustafa Can Camur, Aliza\\nSharmin, and Xueping Li. 2024. Towards Next-Generation Urban Decision Sup-\\nport Systems through AI-Powered Construction of Scientific Ontology Using\\nLarge Language Models—A Case in Optimizing Intermodal Freight Transporta-\\ntion. Smart Cities 7, 5 (2024), 2392–2421.\\n[11] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Large Language Models—A Case in Optimizing Intermodal Freight Transporta-\\ntion. Smart Cities 7, 5 (2024), 2392–2421.\\n[11] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large\\nlanguage model based autonomous agents. Frontiers of Computer Science 18, 6\\n(2024), 186345.\\n[12] Min-Hsien Weng, Shaoqun Wu, and Mark Dyer. 2022. Identification and visual-\\nization of key topics in scientific publications with transformer-based language\\nmodels and document clustering methods. Applied Sciences 12, 21 (2022), 11220.\\n[13] Haowen Xu, Femi Omitaomu, Soheil Sabri, Sisi Zlatanova, Xiao Li, and Yongze\\nSong. 2024. Leveraging Generative AI for Urban Digital Twins: A Scoping Review\\non the Autonomous Generation of Urban Data, Scenarios, Designs, and 3D City\\nModels for Smart City Advancement. arXiv preprint arXiv:2405.19464 (2024).\\nhttps://doi.org/10.48550/arXiv.2405.19464 arXiv:2405.19464 [cs.AI] Computer'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Models for Smart City Advancement. arXiv preprint arXiv:2405.19464 (2024).\\nhttps://doi.org/10.48550/arXiv.2405.19464 arXiv:2405.19464 [cs.AI] Computer\\nScience > Artificial Intelligence.\\n[14] Haowen Xu, Jinghui Yuan, Anye Zhou, Guanhao Xu, Wan Li, Xinyue Ye, et al. 2024.\\nGenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities\\nand Challenges for Integrating Large Language Models (LLMs) and Retrieval-\\nAugmented Generation (RAG) with Intelligent Transportation Systems. arXiv\\npreprint arXiv:2409.00494 (2024).\\n[15] Li Zhao, Zhi-ying Tang, and Xin Zou. 2019. Mapping the knowledge domain of\\nsmart-city research: A bibliometric and scientometric analysis. Sustainability 11,\\n23 (2019), 6648.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, CurrentLandscapeandFutureDirections\\nShailjaGupta(CarnegieMellonUniversity, USA)RajeshRanjan(CarnegieMellonUniversity, USA)SuryaNarayanSingh(BITSindri, India)\\nAbstract'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing itsevolution fromfoundational concepts to the current state of theart. RAGcombinesretrieval mechanismswith generative language modelstoenhancetheaccuracyof outputs, addressingkeylimitationsof LLMs.Thestudyexploresthebasicarchitectureof RAG, focusingonhowretrieval andgenerationareintegratedto handle knowledge-intensive tasks. Adetailed reviewof the significant technological advancements inRAG is provided, including key innovations in retrieval-augmented language models and applicationsacross various domains such as question-answering, summarization, and knowledge-based tasks.Recent research breakthroughs are discussed, highlighting novel methods for improving retrievalefficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethicalconcerns in deployment. Future research directions are proposed, with a focus on improving therobustness of RAGmodels, expanding'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='examines ongoing challenges such as scalability, bias, and ethicalconcerns in deployment. Future research directions are proposed, with a focus on improving therobustness of RAGmodels, expanding the scope of application of RAGmodels, andaddressingsocietalimplications. This survey aims to serve as a foundational resource for researchers and practitioners inunderstandingthepotential of RAGanditstrajectoryinthefieldof natural languageprocessing.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure1: TrendsinRAGcapturedfromrecent researchpapers\\nKeywords: Retrieval-Augmented Generation (RAG), InformationRetrieval, Natural LanguageProcessing(NLP), Artificial Intelligence(AI), MachineLearning(ML), LargeLanguageModel (LLM).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Introduction\\n1.1Introductionof Natural LanguageGeneration(NLG)\\nNatural Language Processing (NLP) has become a pivotal domain within artificial intelligence (AI), withapplications ranging from simple text classification to more complex tasks such as summarization,machinetranslation, andquestionanswering. Aparticularlysignificant branchof NLPisNatural LanguageGeneration (NLG), which focuses on the production of human-like language from structured orunstructured data. NLG's goal is to enable machines to generate coherent, relevant, and context-awaretext, improvinginteractionsbetweenhumansandmachines(Gatt et. al. 2018). AsAI evolves, thedemandfor more contextually awareandfactuallygroundedgeneratedcontent hasincreased, bringingabout newchallengesandinnovationsinNLG.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Traditional NLG models, especially sequence-to-sequence architectures (Sutskever et al. 2014), haveexhibited significant advancements ingeneratingfluent andcoherent text. However, thesemodelstendtorely heavily on training data, often struggling when tasked with generating factually accurate orcontextually rich content for queries that require knowledge beyond their trainingset. Asaresult, modelslike GPT (Radford et al. 2019) or BERT-based (Devlin et al. 2019) text generators are prone tohallucinations, where they produceplausiblebut incorrect or non-existent information(Ji et al. 2022). Thislimitation has prompted the exploration of hybrid models that combine retrieval mechanisms withgenerative capabilities to ensure both fluency and factual correctness in outputs. There has been asignificant rise in several research papers in this field and several new methods across the RAGcomponents have been proposed. Apart from new algorithms and methods, RAGhas also seen steepadoption across'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='rise in several research papers in this field and several new methods across the RAGcomponents have been proposed. Apart from new algorithms and methods, RAGhas also seen steepadoption across various applications. However, there is a gapinasufficient surveyof thisspacetrackingtheevolutionandrecent changesinthisspace. Thecurrent surveyintendstofill thisgap.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.2Overviewof Retrieval-AugmentedGeneration(RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address thelimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,which retrieves relevant documents or information from an external knowledge source, and (ii) ageneration module, which processesthisinformationtogeneratehuman-liketext (Lewiset al. 2020). Thiscombination allows RAG models to not only generate fluent text but also ground their outputs inreal-world, up-to-datedata.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The retrieval module in RAG typically leverages dense vector representations to identify relevantdocuments from large datasets, such as Wikipedia or proprietary databases. Once retrieved, thesedocuments are passed to the generative module, often built using transformer-based architectures, togenerate responses grounded in the retrieved knowledge. This methodology helps mitigate thehallucination problem and ensures that the generated text is more factual and contextually appropriate(Thakur et al. 2021). Over the period, RAGmodels have seen applicationsinvariousdomains, includingopen-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), andpersonalizedrecommendations.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure2: Abasicflowof theRAGsystemalongwithitscomponent\\n1.3Evolutionof HybridModelsinNLP\\nBefore the introduction of RAG, NLPmodelsprimarilyreliedoneither retrieval or generationapproaches,each with its own set of advantages and limitations. Retrieval-based systems, such as traditionalinformation retrieval engines (Salton et al., 1975), efficiently provided relevant documents or snippets inresponse to a query but could not synthesize new information or present the results in a coherentnarrative. On the other hand, purely generative models, which became popular with the rise oftransformer architectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factualaccuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The development of hybrid systems combining retrieval and generation began to gain momentum asresearchers recognizedthecomplementarystrengthsof bothapproaches. Earlyeffortsinhybridmodelingcan be traced back to works like DrQA(Chen et al. 2017), which employed retrieval techniques to fetchrelevant documents for question-answering tasks. However, the generative component in such systemswas minimal, often limited to selecting text directlyfromtheretrieveddocuments. Similarly, inmodelslikeInformationRetrieval (Dai et al. 2019), retrieval wastreatedasdistinct, independent components.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The real innovation came with the realization that retrieval and generation could be tightly integrated.Models like REALM (Guu et al., 2020) represented a key milestone, as they trained the retrieval andgenerative components jointly, enabling better alignment between the retrieved information and thegenerated output. RAG (Lewis et al. 2020) further extended this paradigm by using dense passageretrieval (Karpukhin et al., 2020) to fetch relevant documents and transformers like BART (Lewis et al.,2020) for a generation. This architecture provided a more seamless integration of retrieval andgeneration, allowingthemodel toanswer open-endedquestionswithbothfluencyandfactual grounding.\\n1.4Importanceof FactuallyGroundedLanguageGeneration'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.4Importanceof FactuallyGroundedLanguageGeneration\\nOne of the main motivations for developing RAG is the increasing demand for factually accurate,contextually relevant, and up-to-date generated content. Inmanyapplications, suchascustomer service,medical diagnostics, or legal advisory systems, the need for reliable and grounded responses isparamount. Generative models that produce hallucinated or inaccurate information can lead to seriousconsequences, suchasspreadingmisinformationor providingincorrect advice(Ji et al. 2022).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='RAG models directly address these concerns by grounding their generative process in external,up-to-date knowledge sources. This groundingimprovesthefactual accuracyof theoutput andenhancesthe relevance of responses by incorporating real-world data that is directly tiedtothequery. Additionally,RAGmodels are less likely to propagate biases present in static training data, astheycanretrievemorediverseandbalancedinformationfromexternal sources\\n1.5Applicationsof RAGModels'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.5Applicationsof RAGModels\\nRAG models have been applied across a wide array of domains where factual accuracy and contextualunderstanding are critical. One of themost prominent applicationsisinopen-domainquestionanswering,where the model must generate answers based on a wide range of topics. RAGhas proven effective inimproving answer accuracy byretrievingrelevant informationandthengeneratingresponsesgroundedinthat data (Izacard et. al. 2021). Models like Dense PassageRetrieval (DPR) (Karpukhinet al., 2020) andFusion-in-Decoder (Izacardet. al. 2021) havebeenusedtogreat effect inthiscontext, showingsignificantimprovementsover traditional generativeor retrieval-onlymodels.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"In conversational AI, RAGmodels have enhanced the capabilities of dialogue systems by ensuring thatresponses are both coherent and grounded in factual information (Roller et al., 2020). For example,chatbots used in customer service can benefit fromRAG's ability to retrievespecificdetailsfromproductdatabasesor documentation, leadingtomoreaccurateanduseful responsesfor end-users.\\nOther applications include medical diagnosis systems, where RAGcan retrieve and integrate the latestresearch findings or patient-specific data togenerateaccuratediagnosticsuggestions, andlegal advisorysystems, where the model can retrieve relevant case law or statutes to provide legally sound advice.Furthermore, RAGhasfoundapplicationsinpersonalizedrecommendationsystems, whereit canretrieveuser preferencesor past interactionsandgeneratepersonalizedsuggestions.\\n1.6ChallengesandLimitationsof RAG\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Despite the promise of RAGmodels, several challenges need attention. The retrieval mechanism, whilepowerful, can still struggle with retrieving the most relevant documents, particularly when dealing withambiguous queries or niche knowledge domains. The reliance on dense vector representations, suchasthose used in DPR, can sometimes lead to irrelevant or off-topic documents being retrieved. Efforts torefine retrieval techniques, including the incorporation of more sophisticated query expansion andcontextual disambiguation, are needed to improve performance in these areas. The integration betweenretrieval and generation, while seamless in theory, can sometimes fail in practice. For instance, thegenerative module may not always effectively incorporate the retrieved information into its responses,leading to inconsistencies or incoherence between the retrieved facts and the generated text. Researchinto better alignment mechanisms, such as improved attention models or hierarchical fusion'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='to inconsistencies or incoherence between the retrieved facts and the generated text. Researchinto better alignment mechanisms, such as improved attention models or hierarchical fusion techniques,may help alleviate these issues (Izacard et. al. 2021). Additionally, the computational overhead of RAGmodels is a concern, as they require both a retrieval and a generation step for each query. This dualprocess can be resource-intensive, particularly for large-scale applications (Borgeaud et al. 2021).Techniques such as model pruning (Han et al. 2015) or knowledge distillation (Sanh et al., 2019) mayoffer ways to reduce the computational burden without sacrificing performance. Finally, there are ethicalconcerns associated with the deployment of RAGmodels, particularly in termsof biasandtransparency.Biases in AI and LLM have been a well-researched and evolving field with researchers identifyingdifferent types of biases not limited to Gender, socio-economic class, or even educational'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='in AI and LLM have been a well-researched and evolving field with researchers identifyingdifferent types of biases not limited to Gender, socio-economic class, or even educational background(Gupta et. al. 2024; Ranjan et. al., 2024). While RAG has the potential to reduce biases by retrievingmore balanced information, there is still the risk of amplifying biases present in the retrieved sources'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='(Binns, 2018). Furthermore, ensuring transparency in how retrieval results are selected and used ingenerationiscrucial for maintainingtrust inthesesystems.\\n1.7Scopeof theSurvey\\nThis paper aims to provide a comprehensive survey of RAG models, covering their evolution, keyarchitectural components, recent research in this area, current challenges and limitations of RAG, andfutureresearchdirection.\\n2: CoreComponentsandArchitectural Overviewof RAGSystems\\n2.1Overviewof RAGModels'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2: CoreComponentsandArchitectural Overviewof RAGSystems\\n2.1Overviewof RAGModels\\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments naturallanguage generation (NLG) with external retrieval mechanisms to enhance themodel'sknowledgebase.Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vastcorpora, rely entirely on their internal representations of knowledge, making themsusceptible to issueslike hallucinations—where the models generate plausible but incorrect information. Thesemodelscannotefficiently update their knowledge bases without retraining, making them less practical for dynamic,knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al.2020). Toovercometheselimitations, thepaper (Lewiset al. 2020) proposedtheRAGarchitecture, whichretrievesreal-time, relevant external documentstogroundthegeneratedtext infactual information.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='TheRAGmodel incorporatestwokeycomponents:\\n1. Retriever: This retrieves the most relevant documents froma corpus using techniques such asdensepassageretrieval (DPR) (Karpukhinet. al. 2020) or traditional BM25algorithms.2. Generator: It synthesizes the retrieved documents into coherent, contextually relevantresponses.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='RAG’s strength lies in its ability to leverage external knowledge dynamically, allowing it to outperformgenerative models like GPT-3andknowledge-groundedsystemslikeBERT, whichrelyonstaticdatasets.In open-domain question answering, RAG has been demonstrated to be highly effective, consistentlyretrievingrelevant informationandimprovingthefactual accuracyof thegeneratedresponses(Guu, K., etal. 2020). In addition to knowledge retrieval, RAGmodels excel at updating knowledgebases. Sincethemodel fetches external documents for each query, it requires no retraining to incorporate the latestinformation. This flexibility makes RAG models particularly suitable for domains where information isconstantly evolving, such as medical research, financial news, and legal proceedings. Furthermore,studies have shown that RAGmodels achieve superior results in a varietyof knowledge-intensivetasks,includingdocument summarizationand, knowledge-groundeddialogues\\n2.2Retriever MechanismsinRAGSystems'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2.2Retriever MechanismsinRAGSystems\\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.Effective retrieval ensures that the model's output is grounded in accurate information. Several retrievalmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticatedtechniqueslikeDensePassageRetrieval (DPR).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.1BM25'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"BM25 is a well-established informationretrieval algorithmthat usesthetermfrequency-inversedocumentfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25remains a strong baseline for many modern retrieval systems, including those used in RAG models.BM25 calculates therelevancescoreof adocument basedonhowfrequentlyaquerytermappearsinthedocument while adjusting for the document's length and the frequency of the term across the corpus(Robertson et. al. 2009). WhileBM25iseffectivefor keywordmatching, it haslimitationsinunderstandingsemantic meaning. For example, BM25 cannot capture the relationships between words and tends toperform poorly on more complex, natural language queries that require an understanding of context.Despite this limitation, BM25 is still widely used because of itssimplicityandefficiency. BM25iseffectivefor tasks involvingsimpler, keyword-basedqueries, althoughmoremodernretrieval modelslikeDPRtendtooutperformit\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='BM25 is still widely used because of itssimplicityandefficiency. BM25iseffectivefor tasks involvingsimpler, keyword-basedqueries, althoughmoremodernretrieval modelslikeDPRtendtooutperformit insemanticallycomplextasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.2DensePassageRetrieval (DPR)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Dense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modernapproach to information retrieval. It uses a dense vector space in which both the query and thedocuments are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, wherethe query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xionget. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query anddocuments, making it highly effective for open-domain question-answering tasks. The strength of DPRlies in its ability to retrieve relevant information based on semantic meaning rather than keywordmatching. By training the retriever on a large corpus of question-answer pairs, DPRcan finddocumentsthat are contextually related to the query, even when the query and the document do not share exactterms. Recent research has further improved DPRbyintegratingit withpre-trainedlanguagemodelsandanexampleisLLMadaptedfor'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='related to the query, even when the query and the document do not share exactterms. Recent research has further improved DPRbyintegratingit withpre-trainedlanguagemodelsandanexampleisLLMadaptedfor thedenseRetrievAl approach(Li et. al. 2023)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.3REALM(Retrieval-AugmentedLanguageModel)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Another significant advancement in retrieval mechanisms for RAGmodels is REALM(Guu et al. (2020).REALM integrates retrieval into the language model's pre-training process, ensuring that the retriever isoptimized alongside the generator for downstreamtasks. ThekeyinnovationinREALMisthat it learnstoretrieve documents that improve the model’s performance on specific tasks, suchasquestionansweringor document summarization. During training, REALM updates both the retriever and the generator,ensuring that the retrieval process is optimized for the generation task. REALM’s retriever is trained toidentify documents that are not only relevant to the query but also helpful for generating accurate andcoherent responses. As a result, REALM significantly improves the quality of generated responses,particularly in tasks that require external knowledge. Recent studies have demonstrated that REALMoutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='in tasks that require external knowledge. Recent studies have demonstrated that REALMoutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval istightlycoupledwithgeneration.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The core of RAG lies in the quality of retrieved passages, but many current methods rely onsimilarity-based retrieval (Mallen et al. 2022). Self-RAG (Asai et al. 2023b), and REPLUG (Shi et al.,2023) have advanced by leveraging LLMs to enhance retrieval capabilities, achieving more adaptiveretrieval. After initial retrieval, cross-encoder models are used to re-rank the retrieved results by jointlyencoding the query and each retrieved document to compute relevance scores. These models providemore context-aware retrieval at the cost of higher computational overhead. Pointwise and PairwiseRanking, often based on Learning-to-Rank (LTR) algorithms, are used to assign relevance scores to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='retrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAGsystems utilizeself-attentionwithintheLLMtomanagecontext andrelevanceacrossdifferent partsof theinput and retrieved text. Cross-attentionmechanismsareusedwhenintegratingretrievedinformationintothe generative model, ensuring that the most relevant pieces of information are emphasized duringgeneration.\\n2.3Generator MechanismsinRAGSystems\\nIn Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role inproducing the final output by integrating retrieved information with the input query. After the retrievalcomponent pulls relevant knowledge from external sources, the generator synthesizes this informationinto coherent, contextually appropriate responses. The Large Language Model (LLM) serves as thebackbone of the generator, which ensures the generated text is fluent, accurate, and aligned with theoriginal query.\\n2.3.1T5(Text-to-Text Transfer Transformer)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2.3.1T5(Text-to-Text Transfer Transformer)\\nT5 (Text-to-Text Transfer Transformer) (Raffel et al. 2020) is one of the most commonlyusedmodelsforgeneration tasks in RAGsystems. T5isversatileinitsapproach, framingeveryNLPtaskasatext-to-texttask. This uniform framework allows T5 to be fine-tuned for a wide range of tasks, includingquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,T5-based RAG models have been shown to outperform traditional generative models like GPT-3 andBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.Moreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAGsystemsthat needtotackleadiverserangeof knowledge-intensivetasks.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.3.2BARTBART(Bidirectional andAuto-RegressiveTransformer), introducedbyLewiset al. (2020), isanotherprominent generativemodel usedinRAGsystems. BARTisparticularlywell-suitedfor tasksinvolvingtextgenerationfromnoisyinputs, suchassummarizationandopen-domainquestionanswering. Asadenoisingautoencoder, BARTcanreconstruct corruptedtext sequences, makingit robust for tasksthatrequirethegenerationof coherent, factual outputsfromincompleteor noisydata. Whenpairedwitharetriever inaRAGsystem, BARThasbeenshowntoimprovethefactual accuracyof generatedtext bygroundingit inexternal knowledge. Studieshavedemonstratedthat BART-basedRAGmodelsachievestate-of-the-art resultsinvariousknowledge-intensivetasks, includingdialoguegenerationandnewssummarization.\\n3. Retrieval-AugmentedGenerationModelsAcrossDifferent Modalities'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='3.1 Text-Based RAG Models: Text-based RAG models represent the most mature and widelyresearched category. These models leverage textual data for both retrieval and generation tasks,enabling applications such as question-answering, summarization, and conversational agents.Transformer architectures, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), arefoundational in text-based RAG models. These models utilize self-attention mechanisms to capturecontextual relationships within text, which enhances both retrieval accuracy and generation fluency.Dense retrieval models, such as those using dense embeddings fromBERT, offer superior performancecompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leveragedense representations to retrieve relevant documents more effectively. Recent advancements focus onintegrating retrieval and generation into a single training pipeline. REALM (Guu et al., 2020) is an'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='example of such anend-to-endmodel that jointlyoptimizesretrieval andgenerationprocesses, improvingoverall taskperformance.\\n3.2 Audio-Based RAGModels: Audio-based RAGmodels extend the principles of retrieval-augmentedgeneration to the audio modality, enablingapplicationssuchasspeechrecognition, audiosummarization,and conversational agents in voice interfaces. Audiodataisoftenrepresentedusingembeddingsderivedfrom pre-trained models like Wav2Vec 2.0 (Baevski et al., 2020). These embeddings serve as input toretrieval andgenerationcomponents, enablingthemodel tohandleaudiodataeffectively.\\n3.3 Video-Based RAG Models: Video-based RAG models incorporate both visual and textualinformation to enhance performance in tasks such as video understanding, captioning, and retrieval.Video data is represented using embeddings from models like I3D (Xie et. al. 2017) or TimeSformer(Bertasius et al. 2021). These embeddings capture temporal and spatial features essential for effectiveretrieval andgeneration.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='3.4 Multimodal RAG Models: Multimodal RAG models integrate data from multiple modalities—text,audio, video, and images—to provide a more holistic approach to retrieval andgenerationtasks. Modelslike Flamingo (Alayrac et al., 2022) integrate multiple modalities into a unified framework, enablingsimultaneous processing of text, images, and videos. Techniques for cross-modal retrieval involveretrievingrelevant informationacrossdifferent modalities(Li. et. al. 2023).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Multimodal capabilities enhance the versatility and efficiency of RAG across various applications.”Retrieval as generation” (Wang et. al. 2024) extends the Retrieval-Augmented Generation (RAG)framework tomultimodal applicationsbyincorporatingtext-to-imageandimage-to-text retrieval. Utilizingalarge dataset of pairedimagesandtext descriptions, thesystemacceleratesimagegenerationwhenuserqueries align with stored text descriptions (\"retrieval as generation\"). The image-to-text functionalityallowsuserstoengageindiscussionsbasedoninput images.\\nFigure3: Timelineof theevolutionof theRAGsystemanditscomponents'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='4. Recent Advancement inthefield:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='There has been significant advancement in this field and this section intendstocapturekeyfindingsof afewimportant recent papers. Anovel agenticRetrieval-AugmentedGeneration(RAG) framework(Ravuruet. al. 2024) employs a hierarchical, multi-agent architecturewherespecializedsub-agents, usingsmallerpre-trained language models (SLMs), are fine-tuned for specific time series tasks. The master agentdelegates tasks to these sub-agents, who retrieve relevant prompts fromasharedknowledgerepository.In this modular, multi-agent approach, the authors achieve state-of-the-art performance demonstratingimproved flexibility and effectiveness over task-specificmethodsintimeseriesanalysis. RULE(Xiaet. al.2024), a multimodal Retrieval-Augmented Generation (RAG) framework designed to improve thefactuality of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG byintroducing a calibrated selection strategy to control factuality risk, and, by developing a preferenceoptimization'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG byintroducing a calibrated selection strategy to control factuality risk, and, by developing a preferenceoptimization strategy to balance the model’s intrinsic knowledge with retrieved contexts, proving itseffectiveness in enhancing factual accuracy in Med-LVLM systems. METRAG (Gan et. al. 2024), amulti-layered, thoughts-enhanced retrieval-augmented generation framework, integratesLLMsupervisionto generate utility-oriented thoughts and combines document similarity with utility for improvedperformance. It also incorporates a task-adaptive summarizer to produce compact thoughts. Using themulti-layered thoughts from these stages, an LLM generates knowledge-augmented content,demonstrating superior performance on knowledge-intensive tasks compared to traditional approaches.Distractor document is'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure4: EvolvingTrendsinRAGcapturedfromresearchpapers'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"one of the key traits of Retrieval AugmentedFine-Tuning(RAFT) (Zhanget. al. 2024) wherethemodel istrained to disregard irrelevant, distractor documents and instead cite directly fromrelevant sources. Thisprocess, combined with a chain-of-thought reasoning style, enhances themodel'sreasoningcapabilities.RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, includingPubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO(Wang et. al. 2023) , a method designed toenhancethequalityof context providedtogenerativemodelsin tasks like open-domain question answering and fact verification, addresses issues of over- orunder-relianceonretrievedpassages, whichcanleadtoproblemssuchashallucinationsinthegeneratedoutputs. The method improves context quality by identifying useful context through lexical andinformation-theoretic approaches and training context filtering models to refine retrieved contexts duringtest time.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='method improves context quality by identifying useful context through lexical andinformation-theoretic approaches and training context filtering models to refine retrieved contexts duringtest time. ReflectionTokenisakeyattributeof Self-reflectiveRetrieval Augmented-Generation(Self-RAG)(Asai et. al. 2023), anovel frameworkdesignedtoimprovethefactual accuracyof largelanguagemodels(LLMs) by combining retrieval with self-reflection. Unlike traditional methodsthat retrieveandincorporatea fixed number of passages, Self-RAGadaptively retrievesrelevant passagesandusesreflectiontokensto evaluate and refine its responses, allowing the model to adjust its behavior according to task-specificneeds and has shown superior performance in open-domain question-answering, reasoning, factverification, and long-formgenerationtasks. Intelligenceandeffectivenessof RAGaredependent alot onthe quality of retrieval and more meta-data understanding of the repository would enhance theeffectiveness of the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='and long-formgenerationtasks. Intelligenceandeffectivenessof RAGaredependent alot onthe quality of retrieval and more meta-data understanding of the repository would enhance theeffectiveness of the RAGsystem. Anovel data-centric Retrieval-Augmented Generation (RAG) workflowadvances beyond the traditional retrieve-then-read mode and employs aprepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextuallyrelevant, time-critical, or domain-specific information. Key innovations include generating metadata,synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MKSummary)for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG(Chang et. al. 2024), a zero-shot framework that integrates community structures within KnowledgeGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances theaccuracy and contextual relevance of fact-checking by utilizing multi-hop connections'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='within KnowledgeGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances theaccuracy and contextual relevance of fact-checking by utilizing multi-hop connections within KGs,outperforming traditional methods without requiring additional domain-specific training. The RAPTORmodel (Sarthi et. al. 2024) introduces a hierarchical approach to retrieval-augmented language models,addressing limitations in traditional methods that retrieve only short, contiguous text chunks. RAPTORforms a summary tree to retrieve information at varying abstraction levels by recursively embedding,clustering, and summarizing text. Experiments demonstrate RAPTOR’s superior performance, especiallyin question-answering tasks requiring complex reasoning. When paired with GPT-4, RAPTORimprovesaccuracyontheQuALITYbenchmarkby20%.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"This advancement in RAGfurther provestheutilityof theRAGsystemhowever recent LLMlaunchesthatsupport long-termcontext havesignificantlyshownimprovedperformance. Arecent study(Li et. al. 2024)compared the efficiency of Retrieval Augmented Generation (RAG) and long-context (LC) LargeLanguage Models (LLMs), such as Gemini-1.5 and GPT-4. While LC models outperform RAG whenadequately resourced, RAG's cost-efficiency remains advantageous. To balance performance and cost,the paper introduces Self-Route. This method dynamically directs queries to either RAGor LCbasedonmodel self-reflection, optimizing both computation cost and performance. This study offers valuableinsights into the optimal application of RAGand LCin handling long-context tasks. Nguyen et. al., 2024introduce SFR-RAG, a small but highly efficient Retrieval AugmentedGeneration(RAG) model, whichisdesigned to enhance the integration of external contextual information into Large Language Models(LLMs) while minimizing\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='a small but highly efficient Retrieval AugmentedGeneration(RAG) model, whichisdesigned to enhance the integration of external contextual information into Large Language Models(LLMs) while minimizing hallucinations. LA-RAG (Li et. al., 2024), a novel Retrieval-AugmentedGeneration (RAG) paradigm designed to enhance Automatic Speech Recognition (ASR) in largelanguage models (LLMs). One of the key benefits of LA-RAG is its ability to leverage fine-grainedtoken-level speech data stores alongside a speech-to-speech retrieval mechanism, improving ASR'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='accuracy by incorporating LLMin-context learning (ICL). The studyfocusesondatasetsof Mandarinandvarious Chinese dialects, demonstrating significant accuracy improvements, particularly in managingaccent variations, which have historically been a challenge for existing speech encoders. The findingshighlight LA-RAG’s potential to advance ASR technology, offering a more robust solution for diverseacoustic conditions. Large Language Models (LLMs) face challenges in AI legal and policy contextsdueto outdated knowledge and hallucinations. HyPA-RAG(Kalra et. al., 2024), aHybridParameter-AdaptiveRetrieval-Augmented Generation system, improves accuracy by using adaptive parameter tuning andhybrid retrieval strategies. Tested on NYC Local Law144 (LL144), HyPA-RAGdemonstrates enhancedcorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG(Qianet. al.,2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome thelimitations of'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='and contextual precision, addressing the complexities of legal texts. MemoRAG(Qianet. al.,2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome thelimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG’sdual-system architecture utilizes a lightweight long-range LLM to generate draft answers and guideretrieval tools, while a more powerful LLM refines the final output. This framework, optimized for bettercluing and memory capacity, significantly outperforms conventional RAG models across both complexand straightforward tasks. NLLB-E5 (Acharya et. al., 2024) introduces a scalable multilingual retrievalmodel aimed at addressing the challenges faced in supporting multiple languages, particularlylow-resource languages like Indiclanguages. ByleveragingtheNLLBencoder andadistillationapproachfromtheE5multilingual retriever, NLLB-E5enableszero-shot retrieval acrosslanguageswithout theneedfor multilingual training'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='like Indiclanguages. ByleveragingtheNLLBencoder andadistillationapproachfromtheE5multilingual retriever, NLLB-E5enableszero-shot retrieval acrosslanguageswithout theneedfor multilingual training data. Evaluations on benchmarks such as Hindi-BEIR showcase its robustperformance, highlighting task-specific challenges and advancing multilingual information access forglobal inclusivity.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='5. Current Challenges andLimitationsinRetrieval-AugmentedGeneration(RAG):\\nThissectionintendstohighlight thecurrent challengesandlimitationsof RAGconsideringthecurrentlandscapeof thesystemandthiswouldshapethefutureresearchdirectionsinthefield.\\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrievalcomponentsrelyonexternal databases, handlingvast anddynamicallygrowingdatasetsrequiresefficientretrieval algorithms. High computational costs and memory requirements also make it difficult to deployRAGmodelsinreal-timeor resource-constrainedenvironments(Shi et al. 2023), (Asai et al. 2023b).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Retrieval Quality and Relevance: Ensuring the quality andrelevanceof retrieveddocumentsremainsasignificant concern. Retrieval models can sometimes return irrelevant or outdated information, whichnegatively affects the accuracy of the generated output. Improving retrieval precision, especially forlong-formcontent generation, remainsanactiveareaof research(Mallenet al. 2022), (Shi et al. 2023).\\nBias and Fairness: Similar to other machine learning models, RAG systems can exhibit bias due tobiases present in the retrieved datasets. Retrieval-based models may amplifyharmful biasesinretrievedknowledge, leading to biased outputs in a generation. Developing bias mitigation techniquesfor retrievalandgenerationintandemisanongoingchallenge.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Coherence: RAG models often struggle with integrating the retrieved knowledge into coherent,contextually relevant text. The alignment between retrieved passages and the generationmodel'soutputis not always seamless, leading to inconsistencies or factual hallucinations in the final response(Ji et al.2022).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Interpretability and Transparency: Like many AI systems, RAG models are often treated as blackboxes, with limited transparency in how retrieval influences generation. Improving the interpretability ofthesemodelsiscrucial tofosteringtrust, especiallyincritical applications(Roller et al. 2020).\\n6. FutureResearchDirectionsfor Retrieval-AugmentedGeneration(RAG)\\nRetrieval-augmented generation (RAG) represents a significant advancement in natural languageprocessing and related fields by combining retrieval and generative mechanisms. This section exploreskeyareasfor futureresearch, highlightingthepotential for innovationandimprovement inRAGsystems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.1 Enhancing Multimodal Integration: The integration of text, image, audio, and video data in RAGmodels remains an evolving challenge. Future research should focus on improving multimodal fusiontechniques to enable seamless interaction between different data types. This includes developingadvanced methods for aligning and synthesizing information across modalities. Recent works (Chen et.al. 2022), (Yasunaga et. al. 2022), (Zhu et. al. 2024) have explored multimodal learning, but furtherinnovations are needed to enhance the coherence andcontextualityof multimodal outputs.Researchintocross-modal retrieval aims to improve the ability of RAGsystems to retrieve relevant information acrossdifferent modalities. For example, combining text-based queries with image or video content retrievalcould enhance applications such as visual question answering and multimedia search. This is anotherfuturedirectiontoexplorefor RAGrelatedresearch.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.2 Scaling and Efficiency: As RAG models are deployed in increasingly large-scale applications,scalability becomes a critical concern. Research should focus on developing methods toefficientlyscaleretrieval and generation processes without compromising performance. Techniques such as distributedcomputing and efficient indexing methods are essential for handling large datasets. Improving theefficiency of RAG models involves optimizing both retrieval and generation components to reducecomputational resourcesandlatency.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrievalprocesses to cater to individual user preferences and contexts. This involves developing techniques toadapt retrieval strategies based on user history, behaviour, and preferences. Enhancing the contextualadaptation of RAGmodels by deeper understandingof thecontext andsentimentsof query(Guptaet. al.2024) and the repository of ducments is crucial for improving the relevance of generated responses.Research should explore methods for dynamic adjustment of retrieval and generation processes basedontheevolvingcontext of interactions. Thisincludesincorporatinguser feedbackandcontextual cuesintotheRAGpipeline.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.4 Ethical and PrivacyConsiderations: Addressingbiases(Shresthaet. al. 2024), (Guptaet. al. 2024)in general and specifics to RAG models is a critical area for future research. As RAG systems aredeployed in diverse applications, ensuring fairness and mitigating biases in retrieved and generatedcontent is essential. Future RAG research should focus on privacy-preserving techniques to protectsensitive information during retrieval and generation. This includes developing methods for secure datahandling and privacy-aware retrieval strategies. Interpretability of model is also a critical area to focusuponasapart of ongoingresearchinimprovingRAG.\\n6.5 Cross-Lingual and Low-Resource Languages: Expanding RAG technology to support multiplelanguages ( Chirkova et. al. 2024), especially low-resource languages, is a promising direction. Future'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='research should aimtoimprovecross-lingual retrieval andgenerationcapabilitiestoprovideaccurateandrelevant results across different languages. Enhancing RAG models to effectively support low-resourcelanguages involves developing methods to retrieve and generate content with limited training data.Research should focus on techniques for transfer learning and data augmentation to improveperformanceinunderrepresentedlanguages.\\n6.6 Advanced Retrieval Mechanisms: Future RAG research should explore dynamic retrievalmechanisms that adapt to changing query patterns and content requirements. This includes developingmodels that can dynamically update their retrieval strategiesbasedonnewinformationandevolvinguserneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, suchasdenseand sparse retrieval, could enhance the effectiveness of RAGsystems. Research shouldexplorehowtointegratedifferent retrieval methodstoachieveoptimal performancefor diversetasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.7 Integration with Emerging Technologies: Integrating RAGmodels with brain-computer interfaces(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.Research should explore how RAG systems can leverage BCI data to enhance user experience andgenerate context-aware responses.The integration of RAG with AR and VR technologies presentsopportunities for creating immersive and interactive experiences. FutureresearchshouldinvestigatehowRAG models can be used to enhance AR and VR applications by providing contextually relevantinformationandinteractions.\\n7. Conclusion'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Retrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive researchdedicated to improving retrieval effectiveness and enhancing coherent generation to minimizehallucinations. Fromitsearlyiterationstorecent advancements, RAGhasbeeninstrumental inintegratingexternal knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. Inparticular, recent domain-specific work has showcased RAG's potential in specialized areas such aslegal, medical, and low-resource language applications, highlighting its adaptability andscope. However,despite these advances, this paper identifies clear gaps that remain unresolved. Challengessuchastheintegration of ambiguous or unstructured information, effective handling of domain-specific contexts, andthe high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments. Thefuture research directions outlined in this paper—ranging from improving retrieval mechanisms toenhancing context management and ensuring scalability—will serveasacritical guidefor thenext phaseof innovation in this space. By addressing these gaps, the next generation of RAG models has thepotential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing theboundariesof what ispossibleinretrieval-augmentedAI applications.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='References:\\nAcharya, A., Murthy, R., Kumar, V., &Sen, J. (2024). NLLB-E5: AScalable Multilingual Retrieval Model.ArXiv. /abs/2409.05401\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M.,Menick, J., Borgeaud, S., . . . Simonyan, K. (2022). Flamingo: AVisual Language Model for Few-ShotLearning. ArXiv. /abs/2204.14198\\nAsai, A., Wu, Z., Wang, Y., Sil, A., &Hajishirzi, H. (2023). Self-RAG: Learning to retrieve, generate, andcritiquethroughself-reflection. arXivpreprint arXiv:2310.11511.\\nBaevski, A., Zhou, H., Mohamed, A., &Auli, M. (2020). Wav2vec 2.0: AFramework for Self-SupervisedLearningof SpeechRepresentations. ArXiv. /abs/2006.11477\\nBertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for VideoUnderstanding?ArXiv. /abs/2102.05095'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for VideoUnderstanding?ArXiv. /abs/2102.05095\\nBinns, R. (2018). Fairness in machine learning: Lessons from political philosophy. Proceedings of the2018ConferenceonFairness, Accountability, andTransparency(pp. 149-159).\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. V., Lespiau, J.,Damoc, B., Clark, A., Casas, D. D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L.,Jones, C., Cassirer, A., . . . Sifre, L. (2021). Improving language models by retrieving from trillions oftokens. ArXiv. /abs/2112.04426\\nBrown, T., et al. (2020). \"LanguageModelsareFew-Shot Learners.\" arXivpreprint arXiv:2005.14165.\\nChang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in KnowledgeGraphsfor AdvancedRetrieval-AugmentedGenerationinFact-Checking. ArXiv. /abs/2408.08535'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Chang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in KnowledgeGraphsfor AdvancedRetrieval-AugmentedGenerationinFact-Checking. ArXiv. /abs/2408.08535\\nChen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to answer open-domainquestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers) (pp. 1870-1879).\\nChen, W., Hu, H., Chen, X., Verga, P., &Cohen, W. W. (2022). MuRAG: Multimodal Retrieval-AugmentedGenerator for OpenQuestionAnsweringover ImagesandText. ArXiv. /abs/2210.02928\\nChirkova, N., Rau, D., Déjean, H., Formal, T., Clinchant, S., &Nikoulina, V. (2024). Retrieval-augmentedgenerationinmultilingual settings. ArXiv. /abs/2407.01463\\nDai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For FirstStageRetrieval. ArXiv. /abs/1910.10687'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Dai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For FirstStageRetrieval. ArXiv. /abs/1910.10687\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectionaltransformers for language understanding. In Proceedings of the 2019 Conference of theNorthAmericanChapter of the Association for Computational Linguistics: Human Language Technologies (pp.4171-4186).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep BidirectionalTransformersfor LanguageUnderstanding. ArXiv. /abs/1810.04805\\nGan, C., Yang, D., Hu, B., Zhang, H., Li, S., Liu, Z., Shen, Y., Ju, L., Zhang, Z., Gu, J., Liang, L., &Zhou,J. (2024). Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi LayeredThoughts. ArXiv. /abs/2405.19893\\nGatt, A., &Krahmer, &E. (2018). Surveyof thestateof theart innatural languagegeneration: Coretasks,applications, andevaluation. Journal of Artificial IntelligenceResearch, 61, 65-170.\\nGupta, S., &Ranjan, R. (2024). Evaluation of LLMs Biases TowardsEliteUniversities: APersona-BasedExploration. ArXiv. /abs/2407.12801\\nGupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: FromRule-basedtomodernLLMbasedsystem. ArXiv. /abs/2409.09989'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Gupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: FromRule-basedtomodernLLMbasedsystem. ArXiv. /abs/2409.09989\\nGuu, J., Lee, K., & Pasupat, P. (2020). Retrieval-augmented generation for knowledge-intensive NLPtasks. arXivpreprint. https://arxiv.org/abs/2002.08909\\nGuu, K., Lee, K., Tung, Z., Pasupat, P., & Chang, M. (2020). REALM: Retrieval-augmented languagemodel pre-training. In Proceedings of the 37th International Conference on Machine Learning (pp.3929-3938).\\nHan, S., Pool, J., Tran, J., & Dally, W. J. (2015). Learning both weights and connections for efficientneural network. InAdvancesinNeural InformationProcessingSystems(pp. 1135-1143).\\nIzacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domainquestion answering. In Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: MainVolume(pp. 874-880).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Chen, D., Dai, W., Chan, H. S.,Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ArXiv.https://doi.org/10.1145/3571730\\nKalra, R., Wu, Z., Gulley, A., Hilliard, A., Guan, X., Koshiyama, A., &Treleaven, P. (2024). HyPA-RAG: AHybridParameter AdaptiveRetrieval-AugmentedGenerationSystemfor AI Legal andPolicyApplications.ArXiv. /abs/2409.09046\\nKarpukhin, V., Oguz, B., Min, S., & Yih, W. (2020). Dense passage retrieval for open-domain questionanswering. arXivpreprint. https://arxiv.org/abs/2004.04906\\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. & Yih, W. T. (2020). Densepassage retrieval for open-domain question answering. In Proceedings of the 2020 Conference onEmpirical MethodsinNatural LanguageProcessing(EMNLP) (pp. 6769-6781).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Riedel, S. (2020).Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34thInternational ConferenceonNeural InformationProcessingSystem( pp. 9459-9474).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Li, C., Liu, Z., Xiao, S., & Shao, Y. (2023). Making Large Language Models A Better Foundation ForDenseRetrieval. ArXiv. /abs/2312.15503\\nLi, F., Zhu, L., Wang, T., Li, J., Zhang, Z., & Shen, H. T. (2023). Cross-Modal Retrieval: ASystematicReviewof MethodsandFutureDirections. ArXiv. /abs/2308.14263\\nLi, S., Shang, H., Wei, D., Guo, J., Li, Z., He, X., Zhang, M., & Yang, H. (2024). LA-RAG:EnhancingLLM-basedASRAccuracywithRetrieval-AugmentedGeneration. ArXiv. /abs/2409.08597\\nLi, S., Park, S., Lee, I., & Bastani, O. (2023). TRAQ: Trustworthy Retrieval Augmented QuestionAnsweringviaConformal Prediction. ArXiv. /abs/2307.04642\\nLi, Z., Li, C., Zhang, M., Mei, Q., & Bendersky, M. (2024). Retrieval Augmented Generation orLong-Context LLMs?AComprehensiveStudyandHybridApproach. ArXiv. /abs/2407.16833\\nLiu, Z., Wang, H., Niu, Z., Wu, H., Che, W., &Liu, T. (2020). Towards Conversational Recommendationover Multi-TypeDialogs. ArXiv. /abs/2005.03954'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Liu, Z., Wang, H., Niu, Z., Wu, H., Che, W., &Liu, T. (2020). Towards Conversational Recommendationover Multi-TypeDialogs. ArXiv. /abs/2005.03954\\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., & Hajishirzi, H. (2022). When Not to TrustLanguage Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ArXiv./abs/2212.10511\\nMombaerts, L., Ding, T., Banerjee, A., Felice, F., Taws, J., &Borogovac, T. (2024). Meta Knowledge forRetrieval AugmentedLargeLanguageModels. ArXiv. /abs/2408.09017\\nNguyen, X., Pandit, S., Purushwalkam, S., Xu, A., Chen, H., Ming, Y., Ke, Z., Savarese, S., Xong, C., &Joty, S. (2024). SFR-RAG: TowardsContextuallyFaithful LLMs. ArXiv. /abs/2409.09916\\nNiu, C., Wu, Y., Zhu, J., Xu, S., Shum, K., Zhong, R., Song, J., & Zhang, T. (2023). RAGTruth: AHallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. ArXiv./abs/2401.00396'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Qian, H., Zhang, P., Liu, Z., Mao, K., &Dou, Z. (2024). MemoRAG: Moving towards Next-Gen RAGViaMemory-InspiredKnowledgeDiscovery. ArXiv. /abs/2409.05591\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models areunsupervisedmultitasklearners. OpenAI Blog, 1(8), 9.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &Liu, P. J. (2019).ExploringtheLimitsof Transfer LearningwithaUnifiedText-to-Text Transformer. ArXiv. /abs/1910.10683\\nRanade, P., & Joshi, A. (2023). FABULA: Intelligence Report Generation Using Retrieval-AugmentedNarrativeConstruction. ArXiv. https://doi.org/10.1145/3625007.3627505\\nRanjan, R., Gupta, S., & Singh, S. N. (2024). A Comprehensive Survey of Bias in LLMs: CurrentLandscapeandFutureDirections. ArXiv. /abs/2409.16430\\nRavuru, C., Sakhinana, S. S., &Runkana, V. (2024). Agentic Retrieval-Augmented Generation for TimeSeriesAnalysis. ArXiv. /abs/2408.14484'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Robertson, S.G., & Zaragoza,H., (2009). The Probabilistic Relevance Framework: BM25 and Beyond,FoundationsandTrendsinInformationRetrieval, 3(4), pp. 333-389.\\nRoller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Shuster, K., Smith, E. M.,Boureau, Y., &Weston, J. (2020). Recipesfor buildinganopen-domainchatbot. ArXiv. /abs/2004.13637\\nSalton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing.Communicationsof theACM, 18(11), 613-620.\\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller,faster, cheaper andlighter. ArXiv. /abs/1910.01108\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., &Manning, C. D. (2024). RAPTOR: RecursiveAbstractiveProcessingfor Tree-OrganizedRetrieval. ArXiv. /abs/2401.18059'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., &Manning, C. D. (2024). RAPTOR: RecursiveAbstractiveProcessingfor Tree-OrganizedRetrieval. ArXiv. /abs/2401.18059\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., & Yih, W.-T. (2023).REPLUG: Retrieval-augmentedblack-boxlanguagemodels. arXivpreprint arXiv:2301.12652.\\nShrestha, R., Zou, Y., Chen, Q., Li, Z., Xie, Y., &Deng, S. (2024). FairRAG: Fair Human GenerationviaFair Retrieval Augmentation. ArXiv. /abs/2403.19964\\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. InAdvancesinNeural InformationProcessingSystems(pp. 3104-3112).\\nThakur, N., Bonifacio, L., Zhang, X., Ogundepo, O., Kamalloo, E., Li, X., Liu, Q., Chen, B.,Rezagholizadeh, M., &Lin, J. (2023). NoMIRACL: KnowingWhenYouDon't Knowfor Robust MultilingualRetrieval-AugmentedGeneration. ArXiv. /abs/2312.11361\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Thakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A HeterogenousBenchmarkfor Zero-shot Evaluationof InformationRetrieval Models. ArXiv, abs/2104.08663.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &Polosukhin, I.(2017). Attentionisall youneed. InAdvancesinNeural InformationProcessingSystems(pp. 5998-6008).\\nWang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., Shi, T., Wang, Z., Li, S., Qian, Q., Yin, R., Lv, C.,Zheng, X., &Huang, X. (2024). Searching for Best Practices in Retrieval-Augmented Generation. ArXiv./abs/2407.01219\\nWang, Z., Araki, J., Jiang, Z., Parvez, M. R., & Neubig, G. (2023). Learning to Filter Context forRetrieval-AugmentedGeneration. ArXiv. /abs/2311.08377\\nXia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., &Yao, H. (2024). RULE: ReliableMultimodal RAGfor FactualityinMedical VisionLanguageModels. ArXiv. /abs/2407.05131\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Xia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., &Yao, H. (2024). RULE: ReliableMultimodal RAGfor FactualityinMedical VisionLanguageModels. ArXiv. /abs/2407.05131\\nXie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2017). Rethinking Spatiotemporal Feature Learning:Speed-AccuracyTrade-offsinVideoClassification. ArXiv. /abs/1712.04851\\nXiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P., Ahmed, J., &Overwijk, A. (2020). ApproximateNearest Neighbor NegativeContrastiveLearningfor DenseText Retrieval. ArXiv. /abs/2007.00808'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &Yih, W. (2022). Retrieval-AugmentedMultimodal LanguageModeling. ArXiv. /abs/2211.12561\\nZhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., & Gonzalez, J. E. (2024). RAFT:AdaptingLanguageModel toDomainSpecificRAG. ArXiv. /abs/2403.10131\\nZhu, Y., Ren, C., Xie, S., Liu, S., Ji, H., Wang, Z., Sun, T., He, L., Li, Z., Zhu, X., & Pan, C. (2024).REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via LargeLanguageModels. ArXiv. /abs/2402.07016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nSUFFICIENT CONTEXT : A N EW LENS ON RETRIEVAL\\nAUGMENTED GENERATION SYSTEMS\\nHailey Joren∗\\nUC San Diego\\nhjoren@ucsd.edu\\nJianyi Zhang†\\nDuke University\\njianyi.zhang@duke.edu\\nChun-Sung Ferng\\nGoogle\\ncsferng@google.com\\nDa-Cheng Juan\\nGoogle\\ndacheng@google.com\\nAnkur Taly\\nGoogle\\nataly@google.com\\nCyrus Rashtchian\\nGoogle\\ncyroid@google.com\\nABSTRACT\\nAugmenting LLMs with context leads to improved performance across many\\napplications. Despite much research on Retrieval Augmented Generation (RAG)\\nsystems, an open question is whether errors arise because LLMs fail to utilize the\\ncontext from retrieval or the context itself is insufficient to answer the query. To\\nshed light on this, we develop a new notion of sufficient context, along with a\\nmethod to classify instances that have enough information to answer the query. We\\nthen use sufficient context to analyze several models and datasets. By stratifying'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='method to classify instances that have enough information to answer the query. We\\nthen use sufficient context to analyze several models and datasets. By stratifying\\nerrors based on context sufficiency, we find that larger models with higher baseline\\nperformance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when\\nthe context is sufficient, but often output incorrect answers instead of abstaining\\nwhen the context is not. On the other hand, smaller models with lower baseline\\nperformance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient\\ncontext. We further categorize cases when the context is useful, and improves\\naccuracy, even though it does not fully answer the query and the model errs without\\nthe context. Building on our findings, we explore ways to reduce hallucinations in\\nRAG systems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='RAG systems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of\\ncorrect answers among times where the model responds by 2–10% for Gemini,\\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\\navailable on our github.\\n1 I NTRODUCTION\\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='et al., 2021). Then, the model synthesizes the query along with this added context to generate the\\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\\nenough information to answer the question when combined with the model’s parametric knowledge.\\nOtherwise, the model should abstain from answering and/or ask for more information. One core\\nchallenge in achieving this ideal outcome is building models that can use the provided context only\\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='when it helps answer the question correctly. Several works have investigated this issue by evaluating\\n∗Work done during an internship at Google.\\n†Work done during an internship at Google.\\n1\\narXiv:2411.06037v3  [cs.CL]  23 Apr 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\\n“relevant information” can range from directly containing the answer to simply being topically related\\nto the question. Even “golden” or oracle documents in datasets vary in how much information they\\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\\nwords, while the goal seems to be to understand how LLMs behave when they do or do not have\\nsufficient information to answer the query, prior work fails to address this head-on.\\nAs our first contribution, we put forth a new notion of sufficient context. We divide instances into two\\ncategories based on whether the context provides enough information to construct an answer to the\\nquery. The sufficient context designation is a function of an input pair consisting of one question and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='query. The sufficient context designation is a function of an input pair consisting of one question and\\nthe associated context. Crucially, it does not require a ground truth answer. Figure 1 shows examples\\nand a breakdown of model responses after splitting the data based on sufficient vs. insufficient context.\\nTo divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here,\\nan autorater is a model that evaluates instances based on a property, e.g., a sufficient context autorater.\\nUsing our sufficient context autorater, we uncover new insights into LLM behavior and into existing\\nbenchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of\\ninstances that have sufficient context to answer the query. In other words, open-book QA cannot\\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='instances that have sufficient context to answer the query. In other words, open-book QA cannot\\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,\\nmodels tend to hallucinate more than they abstain, especially for multi-hop questions. This finding\\ncomplements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al.,\\n2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the\\nprovided context is insufficient. Surprisingly, this remains true after we filter out questions that the\\nmodel answers correctly in a closed book (w/o RAG) setting. Together, our analysis deepens our\\nunderstanding of RAG systems by revealing nuances in how models generate responses with retrieval.\\nAs a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\\nWe implement a new selective generation framework that improves accuracy. We use a smaller,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='As a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\\nWe implement a new selective generation framework that improves accuracy. We use a smaller,\\nintervention model to determine when the model generates or abstains, providing a controllable\\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\\nmultiple models/datasets. We also find that fine-tuning open-source models with sufficient context\\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\\nour autorater analysis are available on our github.\\nTo summarize, our main contributions are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='to a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\\nour autorater analysis are available on our github.\\nTo summarize, our main contributions are\\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\\ninstances scalably and to analyze model responses with or without sufficient context.\\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\\ntakeaway is that SOTA LLMs output correct responses 35–62% of the time with insufficient\\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\\n3. Building on our findings above, we develop an efficient and general method for selective generation,\\nusing both confidence and sufficient context signals. Our method improves the fraction of correct'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='using both confidence and sufficient context signals. Our method improves the fraction of correct\\nanswers (among total model responses) by up to 2–10% for Gemini, GPT, and Gemma.\\n2 R ELATED WORK\\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\\n(Ir)relevant Context. Prior studies uncover a lack of robustness to imperfect retrieval. However,\\nthese studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise\\n“relevance” definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nProprietary + Conﬁdential\\nQuestion: Who is Lya L. married to?\\nInsufficient \\nContext\\nLya L. married Tom in 2006… \\nThey divorced in 2014… Lya went \\non dates with Paul in 2018…\\nContext C\\nSufficient \\nContext\\nLya L. married Paul in 2020… They \\nlooked happy together at the \\nrecent event.\\nContext A\\nExamples: sufficient context\\nInsufficient \\nContext\\nLya L. is an astronaut, born in \\nOhio…. Lya has two children… \\nLya’s parents are lawyers…\\nContext D\\nSufficient \\nContext\\nLya L. – Wikipedia\\nBorn: October 1, 1980\\nSpouse: Paul (m. 2020)\\nContext B\\nCategorizing Model Responses (Musique Dataset)\\nFigure 1: New insights into RAG systems by looking at whether instances have sufficient context.\\nOn the left, we show examples of sufficient context; on the right, a breakdown of model responses on\\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='On the left, we show examples of sufficient context; on the right, a breakdown of model responses on\\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with\\nRAG, models hallucinate more than abstain, and the insufficiency of the context does not account for\\nthis major issue. Also, standard datasets have many instances with insufficient context (here, 55.4%).\\nWe include results for other datasets (FreshQA, HotPotQA) in Appendix B, showing similar trends.\\nshould not impact the answer at all (and GSM8K is designed to have sufficient context by definition).\\nXie et al. (2024) looks at having counter-memory context, by either replacing the entity name with\\nan erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='an erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based\\nentailment to determine relevance, and only uses the relevance scores to influence the training mixture\\nof relevant vs. irrelevant documents. Wu et al. (2024) looks at questions where the LLM gets the\\nanswer correct without retrieval and is non-robust to changes in the retrieval. Multiple methods use\\na model to predict relevance scores (as part of a larger pipeline), without calibration to a formal\\ndefinition (Wang et al., 2024a; Zhou et al., 2024), including for iterative retrieval (Jiang et al., 2024;\\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\\nrelevant documents, but simply uses “does not contain the answer” as a proxy for irrelevant context.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Yan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\\nrelevant documents, but simply uses “does not contain the answer” as a proxy for irrelevant context.\\nReducing Hallucinations. There have also been efforts to improve RAG factuality on open-book QA\\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the “lost\\nin the middle” problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\\nthe premise that the provided query/context should be precisely answerable by the LLM, and hence,\\nonly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='only analyze their findings in the sufficient context scenario. Independent of RAG, many papers\\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\\n3 S UFFICIENT CONTEXT\\nAt a high level, our aim is to classify input instances based on whether the context contains enough\\ninformation to answer the query. We split possible contexts into two cases: (1) Sufficient Context.\\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\\ndefinitive answer. (2) Insufficient Context. Otherwise, a context is insufficient. A context may also\\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the information in the context is incomplete, inconclusive, or contradictory. In this section, we more\\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n3.1 D EFINITION OF SUFFICIENT CONTEXT\\nWe first set some notation for a generic open-domain question-answering setting following Trivedi\\net al. (2020). Consider a dataset D with instances of the form q = (Q, C; A), where Q is the query\\nand C is the context that consists of a set of facts. At inference time, we also consider instances\\nq′ = (Q, C) without the ground truth answer, where the goal is to predict an answer A′ from Q, C,\\nand the model’s parametric knowledge. To measure correctness, we compareA′ and A, where there\\nare many options such as exact match, F1 score, or an LLM-based assessment of answer sameness\\n(we use an LLM). Using this notation, we can now define our notion of sufficient context.\\nDefinition (Sufficient Context). An instance q′ = (Q, C) has sufficient context if and only if there\\nexists an answer A′ such that A′ is a plausible answer to the question Q given the information in C.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='exists an answer A′ such that A′ is a plausible answer to the question Q given the information in C.\\nTo understand this, we can build on the Attributable to Identified Sources (AIS) framework (Rashkin\\net al., 2023). Entailment via AIS answers a slightly different question. Namely, given an instance\\nq = ( Q, C; A), the entailment objective is to determine the truth value of the proposition: The\\nanswer to the question Q is A given the information in C. The key difference between entailment\\nand sufficient context is that for sufficient context we do not presuppose that we have the answerA′\\nin advance, only that such an answer exists. Finally, we only consider “plausible” answers, where\\nwe mean that A′ could be an answer to the question Q. For example, if the question asks about a\\nperson’s birthplace, thenA′ should be a location. We note that this allows for the possibility that the\\ncontext contains an incorrect answer to the question. This is a key requirement, because (i) we would'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='context contains an incorrect answer to the question. This is a key requirement, because (i) we would\\nlike to be able to use signal from sufficient context at inference time, where we do not have ground\\ntruth answers (see Section 5.1) and (ii) we hope to elucidate findings that are robust to ground truth\\nlabel noise.\\nRemark 1 (Multi-hop queries). In most benchmark dataset (e.g., Musique, HotPotQA), models are\\nexpected to be able to do multi-hop reasoning up to four hops, in which they must combining facts to\\nform the answer. However, they should not infer connections that are not in the context. For example,\\nif “Bob’s mother was born in New York” then this does not suffice to say Bob was born in New York.\\nBut, if the context also says “Bob’s mother is Alice...” and “... all of Alice’s children were born in\\nNew York” then this instance has sufficient context.\\nRemark 2 (Ambiguous queries). If the query is ambiguous, then the context is sufficient if and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='New York” then this instance has sufficient context.\\nRemark 2 (Ambiguous queries). If the query is ambiguous, then the context is sufficient if and\\nonly if (i) the context can disambiguate the query and (ii) the context provides an answer to the\\ndisambiguated query. For example, the question could be “What sport does Mia play?” and the\\ncontext could contain both “Mia, from New York, plays basketball. . . ” and “... Mia, from California,\\nplays volleyball.” This is sufficient because if the query is referring to either Mia from New York or\\nBob from California, then the context can answer the question.\\nRemark 3 (Ambiguous contexts). Assume the context contains multiple plausible answers to the\\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\\nqueries that would lead to each answer. For example, if the question is “What country does Ali live'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='queries that would lead to each answer. For example, if the question is “What country does Ali live\\nin?” and the context is “Ali lives in Paris” then this instance does not have sufficient context because\\nit is not clear if Ali lives in Paris, France or Paris, Texas, USA. If the context further contains “This\\nweekend, Ali took the train from Paris to Marseille.” Then this becomes sufficient because it is almost\\ncertain that Ali lives in France as one cannot take a train from Texas to France.\\n3.2 S UFFICIENT CONTEXT AUTO RATER\\nNext, we consider automating the task of labeling whether instances have sufficient context or not.\\nWe investigate two questions: (1) Can today’s models achieve high accuracy on a challenging,\\nhuman-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='human-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5\\nPro can serve as an accurate autorater to label instances in terms of sufficient context. It achieves\\n93% accuracy, outperforms other methods, and operates without needing a ground truth answer.\\nSufficient Context Labeled Dataset. Using the above definition, we construct gold labels for\\neach (query, context) pair. We did not use ground truth answers or model responses. For\\nthe instances, we sample a total of 115 instances (queries, contexts, and answers) from standard\\nbenchmarks (PopQA, FreshQA, Natural Questions, EntityQuestions). We design the dataset to\\nbe very challenging, including single- and multi-hop questions, as well as adding highly related\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 1: Sufficient Context AutoRater.Evaluating model ability to classify sufficient context on a\\ngold-labeled dataset of 115 (query, context; answer) instances. Gemini 1.5 Pro (1-shot)\\nperforms the best, while FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\\nground truth (GT) answers, while others only use (query, context) . Best in column in bold.\\nMetrics: F1 Score Accuracy Precision Recall No GT Answer\\nMethods\\nGemini 1.5 Pro (1-shot) 0.935 0.930 0.935 0.935 ✓\\nGemini 1.5 Pro (0-shot) 0.878 0.870 0.885 0.871 ✓\\nFLAMe (fine-tune PaLM 24B) 0.892 0.878 0.853 0.935 ✓\\nTRUE-NLI (fine-tune T5 11B) 0.818 0.826 0.938 0.726\\nContains GT 0.810 0.809 0.870 0.758\\ninformation in the context even if it is not sufficient (e.g., a named entity from the question often\\nappears in the context). We evaluate methods’ abilities to classify sufficient context (binary labels).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='appears in the context). We evaluate methods’ abilities to classify sufficient context (binary labels).\\nMethods: Operating on Query-Context pairs. We use Gemini 1.5 Pro with either instructions\\n(0-shot) or both instructions and a 1-shot example, held out from our dataset. FLAMe 24B is a\\ngeneral autorater model (Vu et al., 2024), but it has a small context window. For FLAMe, we divide\\nthe contexts into 1600 token chunks and ask whether each chunk is sufficient. If any chunk is labeled\\nsufficient, we consider the instance to have sufficient context; otherwise, it’s labeled as insufficient.\\nWe design prompts (in Appendix C) for both models based on the sufficient context definition above.\\nMethods: When a Ground Truth (GT) Answer is Available. For two baselines (TRUE-NLI,\\nContains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Contains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT\\nanswers. Contains GT checks if a GT answer appears in the context. Comparing to entailment is\\nparticularly interesting because if a given answer A is entailed by (Q, C), then the context is also\\nsufficient. On the other hand, the reverse is not true, since the answer A is only one possible choice\\nfor A′. As one consequence, if the ground truth answer A is incorrect, then it may not be entailed by\\nthe context. This happens when a named entity is ambiguous (e.g., two people with the same name),\\nand the GT answer is based on one of the people while the context describes the other.\\nResults. Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Results. Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,\\nwhich implies sufficient context. FLAMe outperforms TRUE-NLI in F1 and accuracy, but lags behind\\nGemini (1-shot), likely because it is a smaller model. The Contains GT method works surprisingly\\nwell, indicating that the presence of a ground truth answer correlates with context sufficiency.\\nDiscussion. In real-world scenarios, we cannot expect candidate answers when evaluating model\\nperformance. Hence, it is desirable to use a method that works using only the query and context.\\nAmong these methods, Gemini 1.5 Pro (1-shot) has high accuracy and balanced precision and recall.\\nTherefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Therefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\\nsufficient context is a different (and more complex) task than determining entailment.\\n4 A N EW LENS ON RAG P ERFORMANCE\\nWe set out to understand RAG performance by looking at sufficient context. We first analyze datasets\\n(Section 4.1), then we investigate model performance with/without sufficient context (Section 4.2). We\\nqualitatively discuss cases where insufficient context leads to correct model responses (Section 4.3).\\n4.1 D O BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT ?\\nWe introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='We introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we\\ndo not aim to optimize the retrieval methods (which could increase the sufficient context percentage).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreshQA HotpotQA Musique\\n0\\n20\\n40\\n60\\n80\\n100% of Dataset with Sufficient Context\\n77.4\\n46.2 44.6\\n77.4\\n46.2 44.6\\n63.7\\n45.4\\n33.4\\nMeasuring Sufficiency by Context Length\\n10000 T okens\\n6000 T okens\\n2000 T okens\\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\\nThis is not the focus of our work, as we wish to understand how models perform with or without\\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\\nDatasets. We consider FreshQA, Musique-Ans, and HotpotQA as a representative spread of open'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='sufficient context. Having a mix of both is inevitable in generic RAG systems.\\nDatasets. We consider FreshQA, Musique-Ans, and HotpotQA as a representative spread of open\\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\\nURLs that should support an answer to the queries, which we use to construct the context (see\\nAppendix A.3 for details on the retrieval). We use the ‘True Premise’ setting (452 instances), skipping\\n‘False Premise’ questions that mislead by design. Musique-Ans (Trivedi et al., 2022) is a multi-hop\\nQA benchmark, created by composing two to four single-hop interconnected questions. Here, ‘Ans’\\nis the standard ‘answerable’ subset. Musique instances have 20 supporting text snippets as sources,\\nwhich we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='which we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the\\ncontext (via REPLUG (Shi et al., 2023b) from FlashRAG (Jin et al., 2024)). We randomly sample\\n500 instances from the development sets of Musique-Ans and HotPotQA for evaluation.\\nSufficient Context % of Datasets. Figure 2 shows the fraction of instances that our autorater\\nclassifies as having sufficient context. We explore three context lengths, ranging from a maximum\\nof 2000 to maximum of 10000 tokens. The motivation behind this is to assess if there is a large\\nchange in sufficient context if we were to simply truncate the retrieval (e.g., for models that have\\nsmall context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='small context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,\\nwhich makes sense as the context comes from oracle supporting documents. The lower sufficient\\ncontext in Musique is perhaps surprising, given that the retrieval is fixed as part of the dataset. From\\nthe results in Figure 2, we truncate at 6000 tokens for all three datasets in the remainder of the paper.\\n4.2 I NITIAL FINDINGS BASED ON SUFFICIENT CONTEXT\\nIn general, the ideal behavior for a language generation model is to answer questions correctly when\\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\\nanalyze several cases to assess how far we are from this ideal trade-off.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='that the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\\nanalyze several cases to assess how far we are from this ideal trade-off.\\nExperimental Set-up and LLMEval. We employed a basic chain of thought (CoT) prompting\\napproach, with the prompt structure and further information detailed in Appendix C.4. We then\\nprocessed the outputted answers to identify matches between the response and any of the ground truth\\nanswers. Responses where a clear correct match could not be determined were processed through\\nthe LLMEval pipeline using a zero-shot approach, with the prompt based on Krishna et al. (2024)\\n(see Appendix C.3). Then, for each example, we can rate it as “correct” or “abstain” or “hallucinate”\\ndepending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='depending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.\\nModels Abstain Less with RAG. While overall performance improves with RAG, the introduction\\nof additional context paradoxically reduces the model’s ability to abstain from answering when\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCategorizing RAG Responses: Sufficient vs Insufficient Context\\nSufficient ContextInsufficient Context\\nFreshQA Musique HotpotQA\\nFigure 3: Model Performance on Datasets Stratified by Sufficient Context. Given sufficient\\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\\nbut the models are still able to answer a large portion of questions correct without sufficient context.\\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='One prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\\nfraction of abstentions drops to 52%. Similarly, GPT 4o’s abstention fraction moves from 34.4%\\nto 31.2% and Gemini 1.5 Pro’s drops from 100% to 18.6%. This phenomenon may arise from the\\nmodel’s increased confidence in the presence of any contextual information, leading to a higher\\npropensity for hallucination rather than abstention.\\nModels Hallucinate with Both Sufficient and Insufficient Context. Considering Figure 3, models\\ngenerally achieve higher accuracy with sufficient context (highergreen bars, top row) than without\\nsufficient context (lower green bars, bottom row). However, looking at each row separately, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='generally achieve higher accuracy with sufficient context (highergreen bars, top row) than without\\nsufficient context (lower green bars, bottom row). However, looking at each row separately, we\\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\\nmore than they abstain ( red bars are higher than blue bars, usually). The trend holds across all\\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\\nmodels. For instance, Claude abstains more (higher blue bars) with insufficient context, but answers\\nfewer questions correctly (lower green bars) than Gemini and GPT. These differences underscore\\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\\nmuch more hallucinations (higher red bars) than the other models, except for HotPotQA, where we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\\nmuch more hallucinations (higher red bars) than the other models, except for HotPotQA, where we\\nattribute the higher accuracy to the smaller retrieved contexts.\\n4.3 Q UALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\\neven when presented with insufficient context. For example, from Figure 3, all three models are able\\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\\nassumption is that the models already know the answer from pre-training, and they can generate a\\ncorrect response from parametric memory. However, this only explains part of the story.\\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='correct response from parametric memory. However, this only explains part of the story.\\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nInstance type Why model may be correct Example\\nYes/No question 50% chance of correct Q: Is there a total eclipse in the United States this year?\\nLimited choice Some chance of correct Q: Which band has more members, Chvrches or\\nGoodbye Mr. Mackenzie?\\nMulti-hop: fragment Use parametric inference\\nQ: Who did the original voice for the character\\nwhose series Mickey’s Safari in Letterland is from?\\nContext says Mickey’s Safari is a video game\\nand Walt Disney voices Mickey Mouse in cartoons.\\nMust infer the game is in the Mickey Mouse series.\\nMulti-hop: partial Use parametric knowledge\\nQ: Claudine’s Return starred the actress who played\\nwhich role on “Married...with Children”?\\nContext lists actresses but not their roles in\\n“Married...with Children”. Must know extra facts.\\nToo many hops Execute complex reasoning\\nQ: How many cyclists have won all three of women’s\\ncycling Grand Tours equivalents in the same year?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='“Married...with Children”. Must know extra facts.\\nToo many hops Execute complex reasoning\\nQ: How many cyclists have won all three of women’s\\ncycling Grand Tours equivalents in the same year?\\nContext requires cross-referencing lists of events\\nand lists of winners while tracking winners by year .\\nAmbiguous query Guess right interpretation\\nQ: Who is the spouse of a cast member from King\\nof the Mountain?\\nContext has many cast members and query/context do\\nnot specify which spouse to answer about.\\nRater error Mislabel insuff. or correct —\\nClosed-book correct Known from pre-training —\\nTable 2: Qualitative Analysis of Correct Answer & Insufficient Context. Examining model\\nresponses across datasets, we identify common cases where the model generates a correct answer\\neven though our autorater labels the instance as insufficient. We categorize such instances into eight\\ntypes, as well as provide examples. Given that models are also correct on many questions in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='even though our autorater labels the instance as insufficient. We categorize such instances into eight\\ntypes, as well as provide examples. Given that models are also correct on many questions in the\\nclosed-book setting, we believe this mostly explains the 35–62% correct rate with insufficient context.\\nas correct. For example, one type accounts for when the provided context is not sufficient to answer the\\nquery, but it bridges gaps in the model’s knowledge. Another type is when the retrieved information\\nclarifies ambiguities inherent in the question (without answering the question). Finally, we also have\\nthe times where either the autorater or the evaluator model makes an error. We note that our analysis\\nexpands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\\nis correct with RAG (but not without) even though the context does not contain the answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='expands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\\nis correct with RAG (but not without) even though the context does not contain the answer.\\nWe additionally investigated cases where the autorater labels an instance as having sufficient context\\nwhile the LLM evaluator marks the answer as incorrect. One source of these discrepancies occurs\\nwhen the ground truth answer conflicts with the answer provided in the source. This represents a key\\ndifference from methods that measure entailment, where context is evaluated relative to a specific\\nground truth answer (see Table 1). Another source of errors arises when the autorater correctly\\nidentifies that the necessary information is present, but the model fails to properly compose the\\ninformation (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\\ncases, however, determining the source of the error proves challenging.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='information (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\\ncases, however, determining the source of the error proves challenging.\\n5 T ECHNIQUES TO REDUCE HALLUCINATIONS WITH RAG\\nFrom our previous analysis, we have seen that models may hallucinate rather than abstain and that\\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\\noutput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\\n5.1 S ELECTIVE RAG U SING SUFFICIENT CONTEXT SIGNAL\\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\\nperformance, since all models answer some questions correctly even with insufficient context, as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='to abstain given insufficient context. However, this heavy-handed approach can lower overall\\nperformance, since all models answer some questions correctly even with insufficient context, as\\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\\naccuracy-coverage trade-off, where “coverage” denotes the portion of inputs on which the model does\\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\\nand then use it to set coverage-accuracy trade-off thresholds.\\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\\nit operates independently from generation, it mitigates unintended downstream effects, whereas\\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\\ninputs (see Section 5.2). Second, it offers a controllable mechanism for tuning abstention, which\\nallows for different operating settings in differing applications, such as strict accuracy compliance in\\nmedical domains or maximal coverage on creative generation tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='allows for different operating settings in differing applications, such as strict accuracy compliance in\\nmedical domains or maximal coverage on creative generation tasks.\\nAbstention Signals We utilize two main signals for abstention: the self-rated probabilities as\\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\\nfrom the model multiple times, and then prompting the model multiple times to label each model as\\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\\nwe use P(Correct) instead. We adapt the probability-generating prompt from Li et al. (2024) to obtain\\nthe model’s response and its estimated probability of correctness. For the sufficient context signal,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the model’s response and its estimated probability of correctness. For the sufficient context signal,\\nwe use the binary label from an autorater. Our hypothesis is that combining these signals should lead\\nto more effective abstention, particularly in cases where the context is insufficient.\\nMethods. We calculate P(True) by sampling 20 responses for each question and querying the model\\n5 times to evaluate whether the answer is correct or incorrect (without using the ground truth) as\\nin Kadavath et al. (2022). For P(Correct), the prompt requests the most likely and second most\\nlikely answers along with their probabilities. We use string matching to extract the response and\\nself-predicted probability, keeping the one with the highest probability. To determine sufficient\\ncontext, we use FLAMe, a small and efficient model for determining the sufficient context label. We\\ndivide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='divide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as\\nsufficient if any of these chunks are sufficient.\\nWe combine the binary sufficient context label with the self-rated answer probability (P(True) for\\nopen-source models or P(Correct) for proprietary models) in a simple logistic regression model to\\npredict hallucinations with 100 iterations of random hyperparameter search. At inference time, we\\nuse the logistic regression model scores to threshold the outputs, abstaining when the score is below\\na chosen threshold as in Joren et al. (2024). We measure the added value for selective accuracy\\nof the sufficient context signal (purple line in Figure 4) by comparing it with the model self-rated\\nconfidence alone (gray line).\\nResults. We find that our approach leads to a better selective accuracy-coverage trade-off compared\\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Results. We find that our approach leads to a better selective accuracy-coverage trade-off compared\\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA\\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\\n70% coverage region. These gains are less pronounced on datasets with lower overall accuracy, such\\nas when using Gemma 27B on Musique. In this scenario, the low overall performance (18.4%) likely\\nmeans that most of the predictive gains are seen by using the self-rated confidence to predict errors\\nfor the majority of samples. As a result, there is no added benefit from the sufficient context signal.\\nDiscussion. As expected, we see a downward trend in which higher coverage leads to lower selective\\naccuracy for both methods. We conclude that the selective generation mechanism with sufficient\\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='accuracy for both methods. We conclude that the selective generation mechanism with sufficient\\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence\\nalone. As a prerequisite for our method, models should have a non-trivial accuracy on sufficient and\\ninsufficient context instances. Then, intuitively, we can prioritize answering questions the model is\\nlikely to get right before those the model struggles with. While ordering examples is impossible in\\nreal settings, we can estimate a coverage level and use a threshold to choose when to answer.\\n5.2 F INE -TUNING\\nWe also consider fine-tuning models to increase their ability to abstain instead of outputting an\\nincorrect answer. To do so, we train the models with some examples that contain “I don’t know”\\ninstead of their original ground truth answer. The intuition here is that training explicitly on such\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n Gemma 27B - HotpotQA\\nP(True)\\nP(True) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n Gemma 27B - Musique\\nP(True)\\nP(True) + Suff. Context\\nFigure 4: Selective Generation: Coverage vs. Selective Accuracy. For selective generation, we\\nuse a linear combination of sufficient context and self-rated confidence (purple) or confidence alone'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Figure 4: Selective Generation: Coverage vs. Selective Accuracy. For selective generation, we\\nuse a linear combination of sufficient context and self-rated confidence (purple) or confidence alone\\n(gray). The x-axis shows coverage (% of questions answered); the y-axis shows accuracy at each\\ncoverage (# correct / # answered). The combined approach matches or outperforms the baseline\\nconfidence-only method, especially on HotpotQA, where our method improves accuracy for most\\ncoverages. For Gemma 27B on Musique, the methods are identical (coeff. for stuff. context is 0).\\ninputs could encourage the model to abstain instead of hallucinating. We also consider multiple\\nsettings, such as only changing the answers when the example has insufficient context. We present\\nfull details in Appendix B.1. The main takeaways are that fine-tuned models (i) have a higher rate of\\ncorrect answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='correct answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,\\nit is likely possible to use fine-tuning to steer the model towards better abstention and correctness, but\\nmore work is needed to determine develop a reliable strategy that can balance these objectives.\\n6 C ONCLUSION\\nOur work provided a new lens on LLM responses in RAG systems centered around our notion of\\nsufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\\na model will output a correct answer with access to only insufficient context. Qualitatively, we\\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='categorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\\nGemma, and can reduce hallucinations by 2–10% on queries that the model answers.\\nLimitations. Our analysis focuses on QA datasets, but summarization tasks also utilize context,\\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\\n“Summarize the reviews of 5-star hotels in Mallorca” depending on whether the context mentions the\\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\\noften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='could have used our autorater to iteratively judge whether to retrieve more or answer the question.\\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score\\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nACKNOWLEDGMENTS\\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\\nimprove the presentation.\\nREFERENCES\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nAnthropic. Claude 3.5 sonnet model card addendum, 2024.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\\ngenerate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau\\nYih. Reliable, adaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187,\\n2024.\\nYung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens:\\nDetecting and mitigating contextual hallucinations in large language models using only attention maps. arXiv\\npreprint arXiv:2407.07071, 2024.\\nFlorin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,\\nNicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems. In\\nProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval, pp. 719–729, 2024.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Retrieval, pp. 719–729, 2024.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A\\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 6491–6501, 2024.\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805, 2023.\\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\\nLéonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open\\nlanguage models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency\\nevaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pp. 3905–3920, 2022.\\nCheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass,\\nAlexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating\\npositional attention bias improves long context utilization. In Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024 , pp. 14982–14995,\\nBangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:\\n10.18653/v1/2024.findings-acl.890. URL https://aclanthology.org/2024.findings-acl.\\n890.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. In International Conference on Learning\\nRepresentations (ICLR), 2022.\\nAQ Jiang, A Sablayrolles, A Mensch, C Bamford, DS Chaplot, D de las Casas, F Bressand, G Lengyel, G Lample,\\nL Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023.\\nZhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop\\nquestion answering with an iterative approach. arXiv preprint arXiv:2407.13101, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for\\nefficient retrieval-augmented generation research. CoRR, abs/2405.13576, 2024. URL https://arxiv.\\norg/abs/2405.13576.\\nHailey Joren, Chirag Nagpal, Katherine A Heller, and Berk Ustun. Participatory person-\\nalization in classification. In Advances in Neural Information Processing Systems , vol-\\nume 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\\nfile/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf.\\nHailey Joren, Charles Marx, and Berk Ustun. Classification with conceptual safeguards. In The Twelfth Inter-\\nnational Conference on Learning Representations (ICLR) , 2024. URL https://iclr.cc/virtual/\\n2024/poster/17625.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\\nknow. arXiv preprint arXiv:2207.05221, 2022.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay,\\nand Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. arXiv\\npreprint arXiv:2409.12941, 2024.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-\\nintensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. Think twice before assure:\\nConfidence estimation for large language models through reflection on multiple answers. arXiv preprint\\narXiv:2403.09972, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Confidence estimation for large language models through reflection on multiple answers. arXiv preprint\\narXiv:2403.09972, 2024.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. Lost in the middle: How language models use long contexts. Transactions of the Associ-\\nation for Computational Linguistics , 12:157–173, 2024. doi: 10.1162/tacl_a_00638. URL https:\\n//aclanthology.org/2024.tacl-1.9.\\nPaul Mineiro. Online joint fine-tuning of multi-agent flows. arXiv preprint arXiv:2406.04516, 2024.\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and\\nHannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. In COLM 2024,\\n2024.\\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.\\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (V olume 1: Long Papers) , pp. 10862–10878, Bangkok,\\nThailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.585.\\nURL https://aclanthology.org/2024.acl-long.585.\\nLucas Monteiro Paes, Carol Xuan Long, Berk Ustun, and Flavio Calmon. On the epistemic limits of personalized\\nprediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural\\nInformation Processing Systems, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Information Processing Systems, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. In-context retrieval-augmented language models. Transactions of the Association for Computational\\nLinguistics, 11:1316–1331, 2023.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov,\\nGaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation\\nmodels. Computational Linguistics, 49(4):777–840, 2023.\\nDavid Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and\\nStéphane Clinchant. Bergen: A benchmarking library for retrieval-augmented generation. arXiv preprint\\narXiv:2407.01102, 2024.\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='arXiv:2407.01102, 2024.\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang\\nWang, Shichao Sun, Huanyu Li, et al. Ragchecker: A fine-grained framework for diagnosing retrieval-\\naugmented generation. arXiv preprint arXiv:2408.08067, 2024.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny\\nZhou. Large language models can be easily distracted by irrelevant context. In International Conference on\\nMachine Learning, pp. 31210–31227. PMLR, 2023a.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,\\n2023b.\\nAdi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Constructing benchmarks and interventions\\nfor combating hallucinations in llms. arXiv preprint arXiv:2404.09971, 2024.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Is multihop qa in dire condition?\\nmeasuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , pp. 8846–8863, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='measuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , pp. 8846–8863, 2020.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition. Transactions of the Association for Computational Linguistics , 10:\\n539–554, 2022.\\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\\nZhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv\\npreprint arXiv:2310.03214, 2023.\\nTu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational\\nautoraters: Taming large language models for better automatic evaluation. arXiv preprint arXiv:2407.10817,\\n2024.\\nYuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='2024.\\nYuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware\\nretrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497 ,\\n2024a.\\nZilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang,\\nAnush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented generation\\nthrough drafting. arXiv preprint arXiv:2407.08223, 2024b.\\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do irrelevant inputs\\nskew the responses of large language models? In COLM 2024, 2024.\\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing\\nthe behavior of large language models in knowledge conflicts. In International Conference on Learning\\nRepresentations (ICLR), 2024.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Representations (ICLR), 2024.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv\\npreprint arXiv:2401.15884, 2024.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings\\nof the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computa-\\ntional Linguistics, 2018.\\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language\\nmodels know what they don’t know? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),\\nFindings of the Association for Computational Linguistics: ACL 2023 , pp. 8653–8665, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.551. URL\\nhttps://aclanthology.org/2023.findings-acl.551.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.551. URL\\nhttps://aclanthology.org/2023.findings-acl.551.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust\\nto irrelevant context. In International Conference on Learning Representations (ICLR) , 2024.\\nTan Yu, Anbang Xu, and Rama Akkiraju. In defense of rag in the era of long-context language models. arXiv\\npreprint arXiv:2409.01666, 2024.\\nHanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and\\nTong Zhang. R-tuning: Instructing large language models to say ‘i don’t know’. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (V olume 1: Long Papers), pp. 7106–7132, 2024a.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Liu Yong, and Shen Huang. End-to-end beam retrieval for\\nmulti-hop question answering. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of\\nthe 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (V olume 1: Long Papers) , pp. 1718–1731, Mexico City, Mexico, June\\n2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.96. URL https:\\n//aclanthology.org/2024.naacl-long.96.\\nYujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large\\nlanguage models. In Proceedings of the ACM on Web Conference 2024 , pp. 1453–1463, 2024.\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and\\nreading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774,\\n2021.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nA S UPPORTING EXPERIMENT INFORMATION\\nWe provide sufficient details to reproduce all of our experiments. We list all of the prompts that we\\nuse in the next section.\\nA.1 M ODELS\\nGPT 4o . We use the API-accessible gpt-4o-2024-08-06 model, released on August 6,\\n2024 (Achiam et al., 2023).\\nGemini 1.5 Pro. We use the API-accessible gemini-1.5-pro-0514 model, released on May\\n14, 2024 (Gemini Team et al., 2023).\\nClaude 3.5 Sonnet. We use the only API-accessible claude-3-5-sonnet-20240620 model,\\nreleased on June 20, 2024 (Anthropic, 2024).\\nGemma 2 27B. We use the publicly available instruction tunedgemma-2-27b-it model, released\\non Jun 27, 2024 (Gemma Team et al., 2024).\\nMistral 3 7B. We use the publicly available instruction tuned Mistral-7B-Instruct-v0.3\\nmodel, released on May 22, 2024 (Jiang et al., 2023).\\nFLAMe. We use the published FLAMe-RM-24B model (Vu et al., 2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='model, released on May 22, 2024 (Jiang et al., 2023).\\nFLAMe. We use the published FLAMe-RM-24B model (Vu et al., 2024).\\nTRUE-NLI model. Calculated the maximum probability over the chunks in the context. Use a\\nthreshold of 0.05, where if the maximum probability is higher then this, then we classify as ‘sufficient\\ncontext’. The threshold of 0.05 achieved the highest F1 score on our human labeled dataset. We use\\nthe t5_xxl_true_nli_mixture version of their model (Honovich et al., 2022).\\nA.2 F INE -TUNING SETTINGS\\nIn our fine-tuning setup, we employed the LoRA adaptation technique (Hu et al., 2022) to fine-tune\\nMistral-7B-Instruct-v0.31. We used either a 2,000-example random subset sampled from the training\\nset of the Musique-Ans dataset or from the development set of the HotPotQA data 2. The prompt\\ntemplate for finetuning is provided in Appendix D.\\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='template for finetuning is provided in Appendix D.\\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were\\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 × 10−5. We note that the\\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\\nA.3 D ATASETS\\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\\nall ‘True Premise’ questions from FreshQA.\\nRetrieval for HotpotQA. We adopt the FlashRAG framework (Jin et al., 2024) to implement our\\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\\nutilizing ‘intfloat/e5-base-v2‘ from Hugging Face’s model hub as a Dense Retriever3. For each query,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='utilizing ‘intfloat/e5-base-v2‘ from Hugging Face’s model hub as a Dense Retriever3. For each query,\\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\\nwithin a prompt template for inference.\\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\\ninput context and ensembling output probabilities across different passes. The REPLUG method is\\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\\n1Available at huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\\n3huggingface.co/intfloat/e5-base-v2\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nRetrieval for FreshQA We use the urls provided in the FreshQA dataset as retrieval for the\\ncontext. We scraped each url and discarded extra HTML content such as headers, footers, and\\nnavigation. We include the title of each webpage in the text, convert any included tables to markdown,\\nand include the table title immediately before the table in the text. When splitting the tables and text\\nfor smaller context windows, we keep tables and sentences intact when possible. For large tables that\\nrequire splitting, we duplicate the table row column headers to include them in each chunk.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 3: Fine-tuned (FT) Mistral 3 7B Instruct. We compare closed book and vanilla RAG with\\nthree FT settings, measuring % Correct ( %C), % Abstain (%A), and % Hallucinate ( %H). Also,\\n“idk” means we change the answer in training samples to be “I don’t know” instead of the given\\nanswer (either for 20% of random examples, or 20% of examples with insufficient context). Best\\n%C for each model/dataset in bold.\\nMusique HotPotQA\\nModel Variant RAG %C %A %H %C %A %H\\nMistral Closed Book 6.6 29.8 63.6 32 7.6 60.4\\n\" Vanilla RAG ✓ 28.8 11.8 59.4 46.6 9.2 44.2\\n\" FT GT answer (Data Mix 1) ✓ 31.4 0 68.6 43.4 0 56.6\\n\" FT idk 20% rand. (Data Mix 2) ✓ 23 1.2 75.8 41.6 0.8 57.6\\n\" FT idk 20% insuff. (Data Mix 3) ✓ 23 2.2 74.8 41.2 2 56.8\\nB A DDITIONAL RESULTS\\nB.1 F INE -TUNING FULL RESULTS\\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='B A DDITIONAL RESULTS\\nB.1 F INE -TUNING FULL RESULTS\\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide\\nsufficient context labels. However, we would incur significant overhead if we used a 24B model\\nto improve the generation of a much smaller LLM. Instead, we try directly fine-tuning Mistral 3\\n7B to increase accuracy with retrieval. Specifically, we experiment with different data mixtures to\\nencourage the model to output “I don’t know” instead of generating an incorrect response.\\nFine-tuning Data. We repeat the following process separately for each of the Musique-Ans and\\nHotPotQA datasets to create three mixtures of training data with different answers for each dataset.\\nFirst, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='First, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change\\nthe answer to “I don’t know” before fine-tuning. For Data Mix 3, we instead randomly choose 400\\nexamples (20%) that our autorater labels as insufficient context and change their answer to “I don’t\\nknow” while keeping the other answers as the ground truth. Our hypothesis is that fine-tuning on\\nData Mix 2 and 3 should steer the model to abstain more and hallucinate less than with Data Mix 1.\\nModels, Methods, Metrics. Using the three data mixtures described above, we fine-tune the Mistral\\n3 7B Instruct model model with LoRA (details in Appendix A.2). At inference time, we use the\\nstandard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='standard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla\\nRAG). Consistent with the prior experiments, we use an LLM with ground truth answers to classify\\nresponses as Correct (%C), Abstention (%A), or Hallucination (%H).\\nFine-tuning Results and Discussion. Table 3 shows our experimental results. We verify that the FT\\nvariants have a higher rate of generating correct answers (%C) compared to closed-book and Vanilla\\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\\nshowing the impact of adding “I don’t know” in the training set. In general, FT models using RAG\\noutput incorrect answers (%H) much of the time, and often more than they abstain (%A).\\nB.2 P ERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='output incorrect answers (%H) much of the time, and often more than they abstain (%A).\\nB.2 P ERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\\nthe first column shows performance without RAG (closed-book) while the second column shows\\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\\ninsufficient context. The third and fourth columns show the performance of the second column\\nstratified by sufficient vs insufficient context respectively.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n100%\\n11.7%\\n10.0%\\n78.3%\\n1.7%\\n9.1%\\n89.1%\\n46.1%\\n12.7%\\n41.2%\\n40.3%\\n8.6%\\n51.1%\\n11.7%\\n9.3%\\n79.0%\\n4.0%\\n6.9%\\n89.1%\\n38.2%\\n17.6%\\n44.1%\\n53.2%\\n11.8%\\n35.0%\\nWithout RAG\\n23.1%\\n54.1%\\n22.8%\\n    With RAG\\n18.3%54.2%\\n27.5%\\n    With RAG\\nSuff. Context\\n77.4% of Dataset\\n39.2%\\n53.9%\\n6.9%\\n    With RAG\\nInsuff. Context\\n22.6% of Dataset\\nCategorizing model responses (FreshQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\\nthat change based on the question’s timestamp, which may explain the frequent abstentions without\\nRAG (100% for Gemini 1.5 Pro).\\n68.4%\\n8.2%\\n23.4%\\n12.0%\\n30.2%\\n57.8%\\n6.5%\\n26.0%\\n67.5%\\n16.7%\\n33.8%\\n49.4%\\n26.2%25.8%\\n48.0%\\n10.0%\\n24.8%\\n65.2%\\n8.2%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='RAG (100% for Gemini 1.5 Pro).\\n68.4%\\n8.2%\\n23.4%\\n12.0%\\n30.2%\\n57.8%\\n6.5%\\n26.0%\\n67.5%\\n16.7%\\n33.8%\\n49.4%\\n26.2%25.8%\\n48.0%\\n10.0%\\n24.8%\\n65.2%\\n8.2%\\n19.9%\\n71.9%\\n11.5%\\n29.0%\\n59.5%\\n54.8%\\n18.0% 27.2%\\nWithout RAG\\n7.2%\\n42.8%\\n50.0%\\n    With RAG\\n1.7%\\n34.2%\\n64.1%\\n    With RAG\\nSuff. Context\\n46.2% of Dataset\\n11.9%\\n50.2%\\n37.9%\\n    With RAG\\nInsuff. Context\\n53.8% of Dataset\\nCategorizing model responses (HotpotQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without\\nRAG (e.g., 48.0% for GPT 4o).\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 4: Performance Analysis of RAG Systems Using Human-Annotated Sufficient Context\\nLabels. These tables include results on a curated set of challenging context-dependent questions.\\nTable (a) shows that while larger models generally achieve higher accuracy with sufficient context\\n(present in 54.8% of cases), even top performers exhibit a 14-16% error rate. Table (b) reveals that\\nwith insufficient context (45.2% of cases), models predominantly abstain from answering (50-73%\\nof instances), though significant hallucination rates (15-40%) persist. These patterns of context-\\ndependent performance and hallucination risk are consistent with our analyses of HotpotQA, FreshQA,\\nand Musique datasets, despite variations in absolute performance due to different task complexities.\\n(a) Performance with Sufficient Context (54.8% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 84.1 1.6 14.3\\nGPT 4o 82.5 4.8 12.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='(a) Performance with Sufficient Context (54.8% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 84.1 1.6 14.3\\nGPT 4o 82.5 4.8 12.7\\nClaude 3.5 Sonnet 85.7 11.1 3.2\\nGemini 1.5 Flash 77.8 4.8 17.5\\nGemma 27B 71.4 3.2 25.4\\n(b) Performance with Insufficient Context (45.2% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 9.6 50.0 40.4\\nGPT 4o 23.1 61.5 15.4\\nClaude 3.5 Sonnet 9.6 53.8 36.5\\nGemini 1.5 Flash 7.7 73.1 19.2\\nGemma 27B 9.6 55.8 34.6\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nB.3 C OMPARISON OF QA E VALUATION METRICS\\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\\ncorrectness.\\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\\nour sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\\nthough both metrics reveal similar patterns in model behavior.\\nTable 5: Comparison of evaluation metrics across models and datasets. We show results for\\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='checking whether the response contains one of the ground truth answer strings (\"Contains\"), where\\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\\nFreshQA Musique HotpotQA\\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\\n1. Different formatting affects matching:\\nQ: What date did the creator of Autumn Leaves die?\\nGround Truth: 13 August 1896'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='1. Different formatting affects matching:\\nQ: What date did the creator of Autumn Leaves die?\\nGround Truth: 13 August 1896\\nResponse: August 13, 1896.\\nContains Answer: False\\nLLMEval: Correct\\n2. Semantic equivalents are not captured:\\nQ: What former Los Angeles Lakers majority owner is the\\nfather of Jeanie Marie Buss?\\nGround Truth: Gerald Hatten Buss\\nResponse: Jerry Buss.\\nContains Answer: False\\nLLMEval: Correct\\n3. Partial matches can be marked as correct:\\nQ: What is Amazon Prime Video’s most watched premiere ever?\\nGround Truth: The Rings of Power\\nResponse: The series explores the forging of the Rings of Power,\\nthe rise of Sauron...\\nContains Answer: True\\nLLMEval: Hallucinate\\nThe LLM QA evaluator provides several practical advantages:\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n• Handles variations in model verbosity and formatting\\n• Distinguishes between correct, abstain, and incorrect responses\\n• Enables efficient evaluation across multiple datasets\\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC P ROMPTS\\nC.1 S UFFICIENT CONTEXT AUTORATER PROMPT\\nYou are an expert LLM evaluator that excels at evaluating a QUESTION and REFERENCES.\\nConsider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp <TIMESTAMP>.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION.\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\nEXAMPLE:\\n### QUESTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Output the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\nEXAMPLE:\\n### QUESTION\\nIn which year did the publisher of Roald Dahl’s Guide to Railway Safety cease to exist?\\n### References\\nRoald Dahl’s Guide to Railway Safety was published in 1991 by the British Railways Board.\\nThe British Railways Board had asked Roald Dahl to write the text of the booklet, and\\nQuentin Blake to illustrate it, to help young people enjoy using the railways safely. The\\nBritish Railways Board (BRB) was a nationalised industry in the United Kingdom that\\noperated from 1963 to 2001. Until 1997 it was responsible for most railway services in Great\\nBritain, trading under the brand name British Railways and, from 1965, British Rail. It\\ndid not operate railways in Northern Ireland, where railways were the responsibility of the\\nGovernment of Northern Ireland.\\n### EXPLANATION\\nThe context mentions that Roald Dahl’s Guide to Railway Safety was published by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Government of Northern Ireland.\\n### EXPLANATION\\nThe context mentions that Roald Dahl’s Guide to Railway Safety was published by the\\nBritish Railways Board. It also states that the British Railways Board operated from 1963 to\\n2001, meaning the year it ceased to exist was 2001. Therefore, the context does provide a\\nprecise answer to the question.\\n### JSON\\n{\"Sufficient Context\": 1}\\nRemember the instructions: You are an expert LLM evaluator that excels at evaluating a\\nQUESTION and REFERENCES. Consider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp TIMESTAMP.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='First, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\n### QUESTION\\n<question>\\n### REFERENCES\\n<context>\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.2 FLAM E PROMPT\\nINSTRUCTIONS:\\ntitle: Is the context sufficient to infer the answer to the question?\\ndescription: In this task, you will be provided with documents and a question. Use one of the\\nfollowing labels under ’judgment’:\\n1. sufficient: The documents are not sufficient to infer the answer to the question.\\n2. insufficient: The documents are sufficient to infer the answer to the question.\\noutput_fields: judgment\\nCONTEXT:\\ndocuments:<references> question: <question>\\nC.3 LLME VAL PROMPT\\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\\nmatch the GT answers. Hence, we use an LLM to determine: the answers are the same (Correct)\\nor the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='or the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with\\nhuman judgments for this evaluating free form responses (Krishna et al., 2024). Responses resulting\\nin empty strings were classified as \"missing,\" while variations of \"I don’t know\" were also treated\\nas missing. We normalized both the ground truth answers and the model’s responses by removing\\npunctuation, converting to lowercase, and eliminating stop words.\\n===Task===\\nI need your help in evaluating an answer provided by an LLM against ground truth answers.\\nYour task is to determine if the LLM’s response matches the ground truth answers. Please\\nanalyze the provided data and make a decision.\\n===Instructions===\\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answers\". 2. Consider\\nthe substance of the answers – look for equivalent information or correct answers. Do not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='===Instructions===\\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answers\". 2. Consider\\nthe substance of the answers – look for equivalent information or correct answers. Do not\\nfocus on exact wording unless the exact wording is crucial to the meaning.\\n3. Your final decision should be based on whether the meaning and the vital facts of the\\n\"Ground Truth Answers\" are present in the \"Predicted Answer.\" 4. Categorize the answer as\\none of the following:\\n- \"perfect\": The answer is completely correct and matches the ground truth.\\n- \"acceptable\": The answer is partially correct or contains the main idea of the ground truth.\\n- \"incorrect\": The answer is wrong or contradicts the ground truth.\\n- \"missing\": The answer is \"I don’t know\", \"invalid question\", or similar responses indicating\\nlack of knowledge.\\n===Input Data===\\n- Question: What 1876 battle featured the Other Magpie?\\n- Predicted Answer: The Other Magpie fought in the Battle of the Rosebud.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='lack of knowledge.\\n===Input Data===\\n- Question: What 1876 battle featured the Other Magpie?\\n- Predicted Answer: The Other Magpie fought in the Battle of the Rosebud.\\n- Ground Truth Answers: Battle of the Rosebud\\n===Output Format===\\nProvide your evaluation in the following format:\\nExplanation: (How you made the decision)\\nDecision: (One of \"perfect\", \"acceptable\", \"incorrect\", or \"missing\")\\nPlease proceed with the evaluation.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.4 D ATASET QUESTION ANSWER PROMPTS\\nThe CoT prompt instructs the model to provide an accurate and concise answer based solely on\\nthe given search results, using an unbiased and journalistic tone. The prompt includes an example\\nquestion, references, and answer to guide the model’s response format. To extract the final answer,\\nwe implemented a pattern matching technique on the model’s response, specifically targeting the text\\nfollowing \"The answer is:\" for CoT prompts.\\nChain of Thought (CoT)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Start with an accurate, engaging, and concise\\nexplanation based only on the provided documents. Must end with \"The answer is:\". Use an\\nunbiased and journalistic tone.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='unbiased and journalistic tone.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nAnswer Only (AO)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Do not say anything other than the answer itself.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nD F INE -TUNING AND RAG P ROMPTS FOR MISTRAL\\nFinetuning Prompt (FT)\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n<answer>\\nEvaluation Without RAG Prompt\\nAnswer the question based on your own knowledge. Only give me the answer and do not\\noutput any other words. Please follow the following guideline when formulating your answer:\\nif you are uncertain or don’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\nEvaluation With RAG Prompt\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Evaluation With RAG Prompt\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n25')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16406ea0",
   "metadata": {},
   "source": [
    "***Embeding and VectorStoreDB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a487c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06a6be6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1352.63it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x16410a660>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873751dc",
   "metadata": {},
   "source": [
    "### VectoreStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f702a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1690c06e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c921fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='. \\n. \\nLatest updates: h\\ue03cps://dl.acm.org/doi/10.1145/3681780.3697252\\n. \\n. \\nRESEARCH-ARTICLE\\nAutomating Bibliometric Analysis with Sentence Transformers and\\nRetrieval-Augmented Generation (RAG): A Pilot Study in Semantic and\\nContextual Search for Customized Literature Characterization for High-\\nImpact Urban Research\\nHAOWEN XU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nXUEPING LI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJOSE TUPAYACHI, The University of Tennessee, Knoxville, Knoxville, TN, United States\\n. \\nJIANMING (JAMIE) LIAN, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\nOLUFEMI A OMITAOMU, Oak Ridge National Laboratory, Oak Ridge, TN, United States\\n. \\n. \\n. \\nOpen Access Support provided by:\\n. \\nOak Ridge National Laboratory\\n. \\nThe University of Tennessee, Knoxville\\n. \\nPDF Download\\n3681780.3697252.pdf\\n28 January 2026\\nTotal Citations: 2\\nTotal Downloads: 314\\n. \\n. \\nPublished: 29 October 2024\\n. \\n.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content=\". \\nThe University of Tennessee, Knoxville\\n. \\nPDF Download\\n3681780.3697252.pdf\\n28 January 2026\\nTotal Citations: 2\\nTotal Downloads: 314\\n. \\n. \\nPublished: 29 October 2024\\n. \\n. \\nCitation in BibTeX format\\n. \\n. \\nSIGSPATIAL '24: The 32nd ACM\\nInternational Conference on Advances in\\nGeographic Information Systems\\nOctober 29 - November 1, 2024\\nGA, Atlanta, USA\\n. \\n. \\nConference Sponsors:\\nSIGSPATIAL\\nUrbanAI '24: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI (October 2024)\\nh\\ue03cps://doi.org/10.1145/3681780.3697252\\nISBN: 9798400711565\\n.\"),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers\\nand Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature\\nCharacterization for High-Impact Urban Research\\nHaowen Xu\\nxuh4@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nXueping Li\\nXueping.Li@utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJose Tupayachi\\njtupayac@vols.utk.edu\\nUniversity of Tennessee, Knoxville\\nKnoxville, Tennessee, USA\\nJianming (Jamie) Lian\\nlianj@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nOlufemi A Omitaomu\\nomitaomuoa@ornl.gov\\nOak Ridge National Laboratory\\nOak Ridge, Tennessee, USA\\nABSTRACT\\nBibliometric analysis is essential for understanding research trends,\\nscope, and impact in urban science, especially in high-impact jour-\\nnals, such Nature Portfolios. However, traditional methods, relying\\non keyword searches and basic NLP techniques, often fail to uncover'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='nals, such Nature Portfolios. However, traditional methods, relying\\non keyword searches and basic NLP techniques, often fail to uncover\\nvaluable insights not explicitly stated in article titles or keywords.\\nThese approaches are unable to perform semantic searches and\\ncontextual understanding, limiting their effectiveness in classifying\\ntopics and characterizing studies. In this paper, we address these\\nlimitations by leveraging Generative AI models, specifically trans-\\nformers and Retrieval-Augmented Generation (RAG), to automate\\nand enhance bibliometric analysis. We developed a technical work-\\nflow that integrates a vector database, Sentence Transformers, a\\nGaussian Mixture Model (GMM), Retrieval Agent, and Large Lan-\\nguage Models (LLMs) to enable contextual search, topic ranking,\\nand characterization of research using customized prompt tem-\\nplates. A pilot study analyzing 223 urban science-related articles\\npublished in Nature Communications over the past decade high-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='and characterization of research using customized prompt tem-\\nplates. A pilot study analyzing 223 urban science-related articles\\npublished in Nature Communications over the past decade high-\\nlights the effectiveness of our approach in generating insightful\\nsummary statistics on the quality, scope, and characteristics of\\npapers in high-impact journals. This study introduces a new para-\\ndigm for enhancing bibliometric analysis and knowledge retrieval\\nin urban research, positioning an AI agent as a powerful tool for\\nadvancing research evaluation and understanding.\\nThis manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-\\n00OR22725 with the US Department of Energy (DOE). The US government retains\\nand the publisher, by accepting the article for publication, acknowledges that the\\nUS government retains a nonexclusive, paid-up, irrevocable, worldwide license to\\npublish or reproduce the published form of this manuscript, or allow others to do'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='US government retains a nonexclusive, paid-up, irrevocable, worldwide license to\\npublish or reproduce the published form of this manuscript, or allow others to do\\nso, for US government purposes. DOE will provide public access to these results\\nof federally sponsored research in accordance with the DOE Public Access Plan\\n(http://energy.gov/downloads/doe-public-access-plan).\\nPublication rights licensed to ACM. ACM acknowledges that this contribution was\\nauthored or co-authored by an employee, contractor or affiliate of the United States\\ngovernment. As such, the Government retains a nonexclusive, royalty-free right to\\npublish or reproduce this article, or to allow others to do so, for Government purposes\\nonly.\\nUrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1156-5/24/10. . . $15.00\\nhttps://doi.org/10.1145/3681780.3697252\\nKEYWORDS'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\\nACM ISBN 979-8-4007-1156-5/24/10. . . $15.00\\nhttps://doi.org/10.1145/3681780.3697252\\nKEYWORDS\\nBibliometrics Analysis, Large Language Models, Retrieval-Augmented\\nGeneration, Transformers\\nACM Reference Format:\\nHaowen Xu, Xueping Li, Jose Tupayachi, Jianming (Jamie) Lian, and Olufemi\\nA Omitaomu. 2024. Automating Bibliometric Analysis with Sentence Trans-\\nformers and Retrieval-Augmented Generation (RAG): A Pilot Study in\\nSemantic and Contextual Search for Customized Literature Characteri-\\nzation for High-Impact Urban Research . In 2nd ACM SIGSPATIAL In-\\nternational Workshop on Advances in Urban-AI (UrbanAI’24), October 29-\\nNovember 1 2024, Atlanta, GA, USA. ACM, Seattle, WA, USA, 7 pages.\\nhttps://doi.org/10.1145/3681780.3697252\\n1 INTRODUCTION\\nBibliometric analysis is a widely used method for evaluating and\\nmapping research trends, impact, and scope across various scien-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='https://doi.org/10.1145/3681780.3697252\\n1 INTRODUCTION\\nBibliometric analysis is a widely used method for evaluating and\\nmapping research trends, impact, and scope across various scien-\\ntific domains [ 2]. It provides quantitative insights by analyzing\\npublication records, citations, and other scholarly outputs, helping\\nresearchers and policymakers understand the evolution of specific\\nfields [4]. Over the past few decades, bibliometric analysis has\\nevolved from basic citation counts and keyword frequency met-\\nrics to more sophisticated approaches, incorporating co-authorship\\nnetworks, citation flows, and research topic clusters [7]. These meth-\\nods are particularly important in fields like urban science, where\\nemerging topics such as smart cities require continuous monitor-\\ning to shape the direction of future research and innovation [ 5].\\nBibliometric analysis plays a key role in identifying influential\\nworks, emerging themes, and research gaps, thus guiding strategic'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='ing to shape the direction of future research and innovation [ 5].\\nBibliometric analysis plays a key role in identifying influential\\nworks, emerging themes, and research gaps, thus guiding strategic\\ndecision-making in urban science and smart city development [15].\\nHowever, traditional bibliometric methods face several limita-\\ntions. Most rely heavily on keyword searches and basic text mining\\ntechniques, which depend on exact matches of terminologies and\\npredefined keywords. These techniques often miss critical insights\\nthat are not explicitly captured in the titles or abstracts of research\\narticles, thereby limiting the ability to fully understand and clas-\\nsify research topics [8]. Furthermore, traditional natural language\\nprocessing (NLP) approaches, such as term frequency-inverse doc-\\nument frequency (TF-IDF) or simple word co-occurrence metrics,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nfail to capture the semantic meaning and contextual relationships\\nbetween concepts[9]. Although topic modeling methods like La-\\ntent Dirichlet Allocation (LDA) can offer significant benefits for\\nbibliometric analysis by providing deeper insights into the rela-\\ntionships and structures within research literature [ 1], they are\\nprimarily used for uncovering thematic structures and classifying\\narticle topics and are not designed for enabling semantic search or\\nproviding a contextual understanding of an article that involves\\ndeeper reasoning and interpretation. As a result, traditional biblio-\\nmetric analysis often falls short in generating deeper insights that\\nrequire a thorough review and interpretation of the article’s full\\ncontent. Relying primarily on the analysis of titles, keywords, and\\nstandard metadata, limits the ability to provide a more customized'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='content. Relying primarily on the analysis of titles, keywords, and\\nstandard metadata, limits the ability to provide a more customized\\nand nuanced characterization of research based on the full textual\\ndata.\\nRecent advancements in generative AI models, such as large\\nlanguage models (LLMs), have opened new opportunities for en-\\nhancing research [11]. These models, including transformers and\\nRetrieval-Augmented Generation (RAG) systems, excel at seman-\\ntic understanding and contextual interpretation of complex texts,\\nmaking them highly suitable for extracting valuable insights from\\nresearch articles and technical manuals [10, 14]. In the field of urban\\ninformatics, LLMs have been increasingly applied to analyze large\\nvolumes of text, uncovering patterns and trends that traditional\\nmethods would overlook [6, 13]. In this paper, we propose a novel\\ntechnical workflow for automating and enhancing bibliometric\\nanalysis by integrating Vector Databases, Sentence Transformers,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='methods would overlook [6, 13]. In this paper, we propose a novel\\ntechnical workflow for automating and enhancing bibliometric\\nanalysis by integrating Vector Databases, Sentence Transformers,\\nGaussian Mixture Models (GMM), Retrieval Agents, and an LLM.\\nOur approach enables contextual search, topic ranking, and cus-\\ntomized characterization of research articles, which we demonstrate\\nthrough a pilot study analyzing 201 urban science-related articles\\npublished in Nature Communications over the past decade. This\\nwork addresses the limitations of traditional bibliometric meth-\\nods, introducing a new paradigm for urban research analysis and\\nknowledge retrieval through the development of AI agents with\\ncontextual understanding and reasoning capabilities.\\n2 LITERATURE REVIEW\\nTo overcome the limitations and knowledge gaps in traditional\\nbibliometric analysis, recent studies have employed generative AI\\nmodels, particularly transformer-based language models, to auto-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='To overcome the limitations and knowledge gaps in traditional\\nbibliometric analysis, recent studies have employed generative AI\\nmodels, particularly transformer-based language models, to auto-\\nmate and enhance bibliometric methodologies.\\nFijačko et al. [3] explores the application of generative AI in\\nbibliometric analysis, focusing on 10 years of research abstracts\\nfrom the European Resuscitation Congresses (ERC). Using ChatGPT-\\n4, the study classified 2,491 abstracts into ERC guideline topics, with\\nBasic Life Support and Adult Advanced Life Support being the most\\nfrequent. The research highlights the potential of large language\\nmodels like ChatGPT-4 in categorizing and analyzing scientific\\nliterature and identifying trends. However, challenges included\\npotential misclassification, the limited use of abstract titles rather\\nthan full-text, and heavy reliance on the model’s capabilities. These\\nconstraints highlight the challenges of automating bibliometric'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='than full-text, and heavy reliance on the model’s capabilities. These\\nconstraints highlight the challenges of automating bibliometric\\nanalysis in the absence of comprehensive datasets. However, the\\nstudy effectively showcases the potential of AI to significantly\\nimprove bibliometric methodologies despite these limitations.\\nWeng et al. [12] introduces a methodology for detecting and\\nvisualizing key research topics using GPT-3 embeddings and the\\nHDBSCAN clustering algorithm on 593 abstracts related to urban\\nstudies and machine learning. By clustering abstracts based on\\nsemantic similarity and extracting keywords using the Maximal\\nMarginal Relevance (MMR) algorithm, the study provides an in-\\nteractive tool for exploring abstract clusters and their associated\\ntopics. Challenges included optimizing clustering parameters and\\nrelying solely on abstracts, which may not fully represent the re-\\nsearch. Some clusters contained outliers or minimal data, affecting'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='topics. Challenges included optimizing clustering parameters and\\nrelying solely on abstracts, which may not fully represent the re-\\nsearch. Some clusters contained outliers or minimal data, affecting\\naccuracy. Despite these limitations, the study demonstrates the\\npotential of transformer-based models in facilitating unsupervised\\nbibliometric analysis, though refinement is needed.\\nBoth articles emphasize the benefits of transformer-based and\\nlarge language models for bibliometric analysis, while also address-\\ning critical limitations such as data quality, optimization challenges,\\nand input constraints when working with abstract-based datasets.\\nTo overcome these challenges, there is a need to harness recent\\nadvancements in sentence transformer models and RAG technolo-\\ngies. These innovations can enable the development of an AI agent\\ncapable of advanced contextual understanding of research articles,\\nfacilitating semantic search and providing tailored insights based on'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='capable of advanced contextual understanding of research articles,\\nfacilitating semantic search and providing tailored insights based on\\nuser-specific queries. This, in turn, can generate new bibliometric\\nmetrics, offering deeper and more comprehensive analysis.\\n3 METHODOLOGY\\nThis section starts by outlining the design requirements for our\\nproposed methods, then presents the conceptual workflow and its\\nimplementation, which combines Generative AI techniques with\\nstatistical models.\\n3.1 Design Requirements\\nOverall, we aim to develop an AI-agent styled tool that can interact\\nwith users, who are primarily researchers and college students,\\nthrough human nature conversations, to get their inquire on the\\ncurrent-state of cutting edge research in a specific domain, such as\\nsmart city and urban science. Based on the inquiry, our workflow\\nwill automate a sequence of procedures that leverage the unique\\ncapabilities of sentence transformers and RAG techniques on a'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='smart city and urban science. Based on the inquiry, our workflow\\nwill automate a sequence of procedures that leverage the unique\\ncapabilities of sentence transformers and RAG techniques on a\\nbatch of selected literature filtered and downloaded from academics\\ndatabases, such as Scopous, IEEE Xplore, and Web of Science. Aim-\\ning to shed lights on more advanced, intelligent, and automonous\\nbiblimetric analysis, our workflow aims to enable the following\\nfeatures:\\nConversational Interaction: A chatbot-style interface will\\nbe implemented, allowing users to ask questions through nat-\\nural human conversations, without the need for pre-defined\\nkeywords or technical jargon. This feature will enable users\\nto define search and filter criteria for subsets of bibliographic\\ndata (e.g., research articles, conference proceedings, techni-\\ncal reports, and manuals) that have been pre-selected and\\ndownloaded from popular academic databases. The search'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='data (e.g., research articles, conference proceedings, techni-\\ncal reports, and manuals) that have been pre-selected and\\ndownloaded from popular academic databases. The search\\nprocess will be guided by broad categories, such as domains,\\ndisciplines, and journals, to streamline access to relevant\\nliterature.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nFigure 1: The overall design of the transformer and RAG-powered workflow.\\nSemantic and Contextual Search: Based on the user-defined\\ninquiry, this process matches and retrieves relevant research\\ndocuments or specific sections by analyzing the underly-\\ning meaning and contextual relationships between words,\\nrather than relying solely on keyword matching. The use of\\nsentence transformers and text embeddings, enables users\\nto access information and knowledge based on conceptual\\nrelevance, rather than simple term frequency. This enhances\\nthe precision of literature filtering and facilitates deeper,\\nmore insightful knowledge discovery, which will plays im-\\nportant role as the retrieval agent within the RAG paradigm\\nto benefit further analytics using Generative AI models.\\nCustomized Literature Characterization: Using the output'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='portant role as the retrieval agent within the RAG paradigm\\nto benefit further analytics using Generative AI models.\\nCustomized Literature Characterization: Using the output\\nliterature from the semantic and contextual search as input,\\nGenerative Pre-trained Transformer (GPT) models will be\\nemployed for contextual understanding, reasoning, and in-\\nterpretation. These GPT models will process user inquiries\\nto generate customized characterizations and interpretations\\nof the selected literature, providing deeper insights and cre-\\nating more sophisticated metrics for advanced bibliometric\\nanalyses. This approach aims to enhance the overall un-\\nderstanding of research trends and offer tailored, in-depth\\nevaluations of the literature.\\nAs an example of our end-user capability, a user could ask the\\nchatbot, powered by our method, a question like, “What percentage\\nof research published in Computers, Environment and Urban Sys-\\ntems over the past 5 years in the urban mobility sector uses traffic'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='chatbot, powered by our method, a question like, “What percentage\\nof research published in Computers, Environment and Urban Sys-\\ntems over the past 5 years in the urban mobility sector uses traffic\\nsimulation-based methods versus crowd-sourced data-driven meth-\\nods, and what are their spatial scales?” The semantic and contextual\\nsearch then filters and retrieves relevant articles based on the query,\\nranks them by relevance, and feeds them to the generative AI model.\\nThis enables advanced contextual understanding and reasoning to\\nprovide customized characterizations on individual research’s sim-\\nulation types and spatial scales, which involve information often\\nnot found in keywords or titles. These characterizations can be later\\nused to generate summary statics and insights to facilitate more\\ndetailed trend analysis and thematic mapping.\\n3.2 Workflow Design\\nOur workflow consists of four key procedures, as depicted in Figure'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='used to generate summary statics and insights to facilitate more\\ndetailed trend analysis and thematic mapping.\\n3.2 Workflow Design\\nOur workflow consists of four key procedures, as depicted in Figure\\n1. The workflow is later implemented in a Jupyter lab environment\\nusing Python-based libraries. Each procedure is detailed through\\nthe following following list.\\n3.2.1 Bibliography Selection and Data Preparation. In the first step,\\nusers select literature based on generic search criteria such as disci-\\npline, publication year, and journal name. Data is extracted from'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\nacademic databases like Scopus, IEEE Xplore, and SerpApi using\\ntheir respective web services and platforms, or through custom-\\nbuilt web scrapers, such as those powered by SerpAPI. The retrieved\\ndata includes bibliographic summaries in CSV format and individ-\\nual articles in formats like PDF and HTML, which are then stored\\nin a file-based system for further processing.\\n3.2.2 Text Embedding and Data Warehousing. After retrieving the\\nessential documents, a Python script powered by PyPDF2 is used\\nto parse the bibliographic summaries, which include the list of\\ndownloaded articles along with supportive metadata (e.g., authors,\\nyear, source, citations, and h-index), as well as the PDF and HTML\\nversions of the individual articles. This parsing process is designed\\nto upload key textual information into a datastore, building the\\nknowledge base for the proposed AI agent. Unlike traditional infor-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='to upload key textual information into a datastore, building the\\nknowledge base for the proposed AI agent. Unlike traditional infor-\\nmation and content management systems, our workflow utilizes\\na sentence transformer, specifically the all-MiniLM-L6-v2 model\\nfrom Hugging Face, to generate text embeddings—vector represen-\\ntations that encode the semantic and contextual meaning of the text.\\nCompared with traditional NLP methods, sentence transformers,\\nwith its unique self-attention mechanism, have superior advantages\\nin capturing semantic meaning, enabling contextual understanding,\\nhandling synonyms, and long-range dependencies between words\\nin a sentence. These embeddings facilitate more efficient semantic\\nand contextual searches in later stages of the workflow. The text\\nembeddings, along with essential metadata and article content, are\\nuploaded into the datastore. We selected Neo4j, a graph database, as\\nthe datastore for this workflow due to its graph data model, which'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='uploaded into the datastore. We selected Neo4j, a graph database, as\\nthe datastore for this workflow due to its graph data model, which\\nbetter represents the relationships between data entities stored as\\nnodes in the database. In our project, individual articles are repre-\\nsented as nodes within Neo4j, with associated metadata, content,\\nand text embeddings stored as properties of each node.\\n3.2.3 Semantic and Contextual Search. In the third step, the work-\\nflow enables semantic and contextual searches within the literature\\nstored in the knowledge base, leveraging the Neo4j database and\\nsentence transformers. User queries, collected through a chatbot\\ninterface, serve as input for this advanced search. The core function-\\nality compares the text embeddings of the user queries with those\\nof the article contents. We employ an enhanced cosine similarity\\nanalysis, as described in Eq. 1, to calculate a similarity score ranging\\nfrom 0 to 1, where 0 represents complete irrelevance and 1 repre-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='analysis, as described in Eq. 1, to calculate a similarity score ranging\\nfrom 0 to 1, where 0 represents complete irrelevance and 1 repre-\\nsents high relevance. Our implementation extends the standard\\ncosine similarity formula by using Python to chunk the original\\narticle content into sections and paragraphs, enabling more granu-\\nlar comparisons between the query and specific parts of the article.\\nThis process is applied to each article in the database, generating a\\nsimilarity score based on semantic similarity with the user’s query.\\nAt the contextual level, the framework evaluates the query’s con-\\ntext and intelligently selects embeddings from different sections of\\nthe articles to perform a targeted and accurate search.\\nSimilarity Score = a · b\\n∥a∥ ∥b∥ (1)\\nTo draw the decision boundary based on a list of individual\\narticle’s similarity score, we employed GMM to rank and cluster\\narticles by their similarity score, which reflects their relevance. A'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='To draw the decision boundary based on a list of individual\\narticle’s similarity score, we employed GMM to rank and cluster\\narticles by their similarity score, which reflects their relevance. A\\nGMM is a probabilistic model that represents a distribution of data\\nas a mixture of multiple Gaussian (normal) distributions, each char-\\nacterized by its own mean and variance, making it effective for\\nmodeling complex, multimodal datasets. We employed the Akaike\\nInformation Criterion (AIC) and Bayesian Information Criterion\\n(BIC), alongside the elbow method, to determine the optimal num-\\nber of clusters for the Gaussian Mixture Model (GMM) analysis.\\nAfter the clustering analysis, the cluster with the highest average\\nsimilarity scores implies it contains the most relevant articles, which\\nare also further ranked based on its similarity score.\\nArticles in the top-ranked clusters are subsequently fed into\\ngenerative AI models, specifically GPT, to enable more in-depth'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='are also further ranked based on its similarity score.\\nArticles in the top-ranked clusters are subsequently fed into\\ngenerative AI models, specifically GPT, to enable more in-depth\\nanalysis and interpretation. The semantic and contextual search\\nwithin this workflow is a critical component of the RAG paradigm,\\nallowing for further subsetting and refining of input information\\nto ensure more accurate and relevant results. This process also\\nhelps prevent exceeding the token limits of the GPT model context\\nwindow by optimizing the selection of input texts.\\n3.2.4 Customized Article Characterization. The top-ranked clus-\\nters, containing the most relevant articles, are then imported into a\\nGPT model as an external knowledge source to generate customized\\ncharacterizations for each article. These tailored bibliographic char-\\nacteristics serve as metrics, providing more detailed descriptions\\nand classifications of the articles. This approach uncovers valuable'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='acteristics serve as metrics, providing more detailed descriptions\\nand classifications of the articles. This approach uncovers valuable\\ninsights into research trends, focusing on individual articles’ topics,\\ntechnologies, methods, and contributions.\\nWe leverage the contextual reasoning capabilities of large lan-\\nguage models to classify and justify findings based on the semantic\\nmeaning of sections and paragraphs within the articles, extracting\\nuseful information without relying on precise names or keywords.\\nThis process is guided by instructional prompting strategies, where\\nwe design engineered prompt templates and feed them into the\\nGPT model along with the relevant article text segments (specific\\nsections). These segments are further refined and filtered based\\non their content relevance to ensure accurate classification and\\nextraction.\\nAt the technical level, we explored and tested the capabilities\\nof two GPT models, including a local instance of EleutherAI/gpt-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='extraction.\\nAt the technical level, we explored and tested the capabilities\\nof two GPT models, including a local instance of EleutherAI/gpt-\\nneo-1.3B models and the ChatGPT-3.5 Turbo API. Our experiments\\nreveals that small models with on 1.3B parameters suffer from\\nsevere hallucination, and are unable to analyze large size of tokens.\\nThe ChatGPT-3.5 API demonstrates stable performance, particularly\\nin its ability to process large text segments efficiently and produce\\nreasoned characterizations.\\n4 PILOT STUDY\\nThis pilot study aims to demonstrate the feasibility and performance\\nof our proposed methods. For this study, we compiled a dataset\\nof 223 high-impact urban research articles published in Nature\\nCommunications, obtained through the following Scopus query:\\nTITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management”\\nOR “urban planning” ) AND SRCTITLE ( “Nature Communications”\\n) AND PUBYEAR > 2013. We preprocessed the dataset by removing'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='TITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management”\\nOR “urban planning” ) AND SRCTITLE ( “Nature Communications”\\n) AND PUBYEAR > 2013. We preprocessed the dataset by removing\\nall intermediate versions labeled as “Author Correction” or “Pub-\\nlisher Correction. ” The final dataset consists of a CSV file containing\\nbibliometric summaries with all Scopus fields selected, along with\\n223 individual PDF documents of the actual articles.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\n(a) Semantic and contextual search based on user’s query.\\n(b) Customized Literature Characterization using GPT’s reasoning capability.\\nFigure 2: Demonstration of the workflow through two use cases.\\n4.1 Use Case Demonstration\\nOur first use case on semantic and contextual search is demon-\\nstrated in Figure 2a, where the user submits an inquiry to identify\\narticles related to urban green space. The chatbot responds by visu-\\nalizing a histogram of similarity scores for all articles and displaying\\nthe GMM clusters of the articles. Additionally, a link is provided to'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA Xu, et al.\\ndownload a CSV file that ranks and clusters the articles based on\\ntheir relevance to the user’s query.\\nOur second use case builds on the output articles from the first\\nuse case and demonstrates the capability to generate customized\\nliterature characterizations, creating new metrics for bibliometric\\nanalysis. Through the chatbot interface, users specify requests and\\ninstructions via prompts to guide the GPT model in generating tai-\\nlored metrics. Examples of these prompts are illustrated in Figure\\n2b. Based on the prompts, the GPT processes the 67 retrieved arti-\\ncles and their critical content, leveraging its contextual reasoning\\nability to derive new literature characteristics, which can then be\\ndeveloped into metrics and summary statistics. Figure 2b also visu-\\nalizes the responses to the user’s queries using pie charts and box\\nplots. Users can submit additional questions and custom requests'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='alizes the responses to the user’s queries using pie charts and box\\nplots. Users can submit additional questions and custom requests\\nthrough the chatbot to extract information and develop unique\\nmetrics tailored to the needs of bibliometric analysis.\\nOur major contribution lies in the development of an autonomous\\nAI agent designed to assist researchers in automating the charac-\\nterization and information extraction of large volumes of literature,\\nincluding datasets exceeding 1,000 articles. This system enables the\\ngeneration of in-depth insights for bibliometric analysis, signifi-\\ncantly enhancing the scalability and depth of literature review and\\nresearch trend identification processes. By automating these tasks,\\nthe AI agent offers a powerful tool for efficiently managing and\\nanalyzing extensive collections of scholarly articles, ultimately fa-\\ncilitating more comprehensive and insightful bibliometric analyses.\\n4.2 Limitation and Future Work'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='analyzing extensive collections of scholarly articles, ultimately fa-\\ncilitating more comprehensive and insightful bibliometric analyses.\\n4.2 Limitation and Future Work\\nDeveloped as a prototype for a more advanced knowledge base and\\nmanagement system, our workflow still faces a few limitations, as\\nthe following:\\nToken Size Limitation: The current implementation using\\nthe ChatGPT API has a maximum token size limitation and\\nincurs service fees based on the number of tokens processed.\\nThis makes it less suitable for analyzing large volumes of\\nliterature.\\nDatabase Query Performance: The current datastore imple-\\nmentation using the Neo4j database may encounter chal-\\nlenges in querying and managing large volumes of embed-\\nding data, as Neo4j is not optimized as a dedicated vector\\ndatabase.\\nLack of Evaluation and Validation: The GPT-generated lit-\\nerature characteristics are not currently evaluated by human\\nexperts, which introduces uncertainty regarding their accu-\\nracy and reliability.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='erature characteristics are not currently evaluated by human\\nexperts, which introduces uncertainty regarding their accu-\\nracy and reliability.\\nAs future work to address these limitations, we propose several\\nexperimental solutions. These include (a) deploying a local version\\nof large language models, such as GPT-Neo, to minimize service\\nfees for large-scale data analysis, (b) fine-tuning the GPT model\\nto reduce unnecessary content and instructions sent to the model,\\nthereby mitigating token size limitations, (c) transitioning our data-\\nstore implementation to dedicated vector databases, such as FAISS\\nor Pinecone, to enhance latency and accuracy, and (d) developing a\\ncomprehensive strategy to evaluate the GPT’s performance in ana-\\nlyzing and characterizing literature. Additionally, more advanced\\nbibliometric analysis methods could be integrated into the current\\nworkflow to extend its analytical capabilities.\\n5 CONCLUSION\\nIn this paper, we have presented a novel workflow that integrates'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='bibliometric analysis methods could be integrated into the current\\nworkflow to extend its analytical capabilities.\\n5 CONCLUSION\\nIn this paper, we have presented a novel workflow that integrates\\ngenerative AI models and advanced analytical techniques through\\nthe RAG paradigm to address the limitations of traditional biblio-\\nmetric analysis methods. By leveraging the contextual reasoning\\ncapabilities of large language models and enhanced semantic search\\ntechniques, our system offers a more nuanced and insightful analy-\\nsis of research literature. This approach, demonstrated through the\\nanalysis of urban science-related articles, enables customized char-\\nacterizations and generates new metrics for bibliometric analysis,\\nproviding deeper insights into research trends, methodologies, and\\ncontributions.\\nOur pilot study demonstrates the feasibility of this workflow,\\nshowcasing its ability to facilitate advanced semantic and contex-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='contributions.\\nOur pilot study demonstrates the feasibility of this workflow,\\nshowcasing its ability to facilitate advanced semantic and contex-\\ntual searches, cluster relevant articles, and produce tailored bib-\\nliographic insights through generative AI. However, the current\\nimplementation faces challenges, including token size limitations,\\ndatabase query performance issues, and the lack of expert evalua-\\ntion for the AI-generated results.\\nTo address these limitations, future work will explore the de-\\nployment of local language models, fine-tuning of GPT models to\\noptimize token usage, and transitioning to vector databases like\\nFAISS or Pinecone to improve performance. Additionally, we aim\\nto establish a comprehensive validation framework involving hu-\\nman experts to ensure the accuracy and reliability of the gener-\\nated bibliometric insights. As advancements in AI and bibliometric\\nmethodologies continue, our workflow has the potential to serve as'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='man experts to ensure the accuracy and reliability of the gener-\\nated bibliometric insights. As advancements in AI and bibliometric\\nmethodologies continue, our workflow has the potential to serve as\\na powerful and autonomous tool for researchers and policymakers\\nseeking to analyze and interpret vast bodies of scientific literature\\nmore effectively.\\n6 ACKNOWLEDGMENTS\\nThis work was supported by the U.S. Department of Energy (U.S\\nDOE), Advanced Research Projects Agency–Energy (ARPA-E) un-\\nder the project #DE-AR0001780. We thank our collaborators from\\nthe University of Tennessee Knoxville.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG) UrbanAI’24, October 29-November 1 2024, Atlanta, GA, USA\\nREFERENCES\\n[1] Xieling Chen and Haoran Xie. 2020. A structural topic modeling-based biblio-\\nmetric study of sentiment analysis literature. Cognitive Computation 12 (2020),\\n1097–1129.\\n[2] Naveen Donthu, Satish Kumar, Debmalya Mukherjee, Nitesh Pandey, and\\nWeng Marc Lim. 2021. How to conduct a bibliometric analysis: An overview and\\nguidelines. Journal of business research 133 (2021), 285–296.\\n[3] Nino Fijačko, Ruth Masterson Creber, Benjamin S Abella, Primož Kocbek, Špela\\nMetličar, Robert Greif, and Gregor Štiglic. 2024. Using generative artificial intelli-\\ngence in bibliometric analysis: 10 years of research trends from the European\\nResuscitation Congresses. Resuscitation Plus 18 (2024), 100584.\\n[4] Ye-na Gan, Duo-duo Li, Nicola Robinson, and Jian-ping Liu. 2022. Practical guid-'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Resuscitation Congresses. Resuscitation Plus 18 (2024), 100584.\\n[4] Ye-na Gan, Duo-duo Li, Nicola Robinson, and Jian-ping Liu. 2022. Practical guid-\\nance on bibliometric analysis and mapping knowledge domains methodology–A\\nsummary. European Journal of Integrative Medicine 56 (2022), 102203.\\n[5] Yi-Ming Guo, Zhen-Ling Huang, Ji Guo, Hua Li, Xing-Rong Guo, and Mpeoane Ju-\\ndith Nkeli. 2019. Bibliometric analysis on smart cities research. Sustainability 11,\\n13 (2019), 3606.\\n[6] Jianyuan Liang, Anqi Zhao, Shuyang Hou, Fengying Jin, and Huayi Wu. 2024. A\\nGPT-enhanced framework on knowledge extraction and reuse for geographic\\nanalysis models in Google Earth Engine. International Journal of Digital Earth\\n17, 1 (2024), 2398063.\\n[7] Luis Javier Cabeza Ramírez, Sandra M Sánchez-Cañizares, and Fernando J Fuentes-\\nGarcía. 2019. Past themes and tracking research trends in entrepreneurship: A\\nco-word, cites and usage count analysis. Sustainability 11, 11 (2019), 3121.'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='García. 2019. Past themes and tracking research trends in entrepreneurship: A\\nco-word, cites and usage count analysis. Sustainability 11, 11 (2019), 3121.\\n[8] Rodrigo Romero-Silva and Sander De Leeuw. 2021. Learning from the past\\nto shape the future: A comprehensive text mining analysis of OR/MS reviews.\\nOmega 100 (2021), 102388.\\n[9] Iqra Safder and Saeed-Ul Hassan. 2019. Bibliometric-enhanced information\\nretrieval: a novel deep feature engineering approach for algorithm searching\\nfrom full-text publications. Scientometrics 119 (2019), 257–277.\\n[10] Jose Tupayachi, Haowen Xu, Olufemi A Omitaomu, Mustafa Can Camur, Aliza\\nSharmin, and Xueping Li. 2024. Towards Next-Generation Urban Decision Sup-\\nport Systems through AI-Powered Construction of Scientific Ontology Using\\nLarge Language Models—A Case in Optimizing Intermodal Freight Transporta-\\ntion. Smart Cities 7, 5 (2024), 2392–2421.\\n[11] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Large Language Models—A Case in Optimizing Intermodal Freight Transporta-\\ntion. Smart Cities 7, 5 (2024), 2392–2421.\\n[11] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\\nZhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large\\nlanguage model based autonomous agents. Frontiers of Computer Science 18, 6\\n(2024), 186345.\\n[12] Min-Hsien Weng, Shaoqun Wu, and Mark Dyer. 2022. Identification and visual-\\nization of key topics in scientific publications with transformer-based language\\nmodels and document clustering methods. Applied Sciences 12, 21 (2022), 11220.\\n[13] Haowen Xu, Femi Omitaomu, Soheil Sabri, Sisi Zlatanova, Xiao Li, and Yongze\\nSong. 2024. Leveraging Generative AI for Urban Digital Twins: A Scoping Review\\non the Autonomous Generation of Urban Data, Scenarios, Designs, and 3D City\\nModels for Smart City Advancement. arXiv preprint arXiv:2405.19464 (2024).\\nhttps://doi.org/10.48550/arXiv.2405.19464 arXiv:2405.19464 [cs.AI] Computer'),\n",
       " Document(metadata={'producer': 'iText 4.2.0 by 1T3XT', 'creator': 'PyPDF', 'creationdate': '2026-01-28T22:38:28-08:00', 'moddate': '2026-01-28T22:38:28-08:00', 'source': '../data/pdf/3681780.3697252.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': '3681780.3697252.pdf', 'file_type': 'pdf'}, page_content='Models for Smart City Advancement. arXiv preprint arXiv:2405.19464 (2024).\\nhttps://doi.org/10.48550/arXiv.2405.19464 arXiv:2405.19464 [cs.AI] Computer\\nScience > Artificial Intelligence.\\n[14] Haowen Xu, Jinghui Yuan, Anye Zhou, Guanhao Xu, Wan Li, Xinyue Ye, et al. 2024.\\nGenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities\\nand Challenges for Integrating Large Language Models (LLMs) and Retrieval-\\nAugmented Generation (RAG) with Intelligent Transportation Systems. arXiv\\npreprint arXiv:2409.00494 (2024).\\n[15] Li Zhao, Zhi-ying Tang, and Xin Zou. 2019. Mapping the knowledge domain of\\nsmart-city research: A bibliometric and scientometric analysis. Sustainability 11,\\n23 (2019), 6648.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, CurrentLandscapeandFutureDirections\\nShailjaGupta(CarnegieMellonUniversity, USA)RajeshRanjan(CarnegieMellonUniversity, USA)SuryaNarayanSingh(BITSindri, India)\\nAbstract'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing itsevolution fromfoundational concepts to the current state of theart. RAGcombinesretrieval mechanismswith generative language modelstoenhancetheaccuracyof outputs, addressingkeylimitationsof LLMs.Thestudyexploresthebasicarchitectureof RAG, focusingonhowretrieval andgenerationareintegratedto handle knowledge-intensive tasks. Adetailed reviewof the significant technological advancements inRAG is provided, including key innovations in retrieval-augmented language models and applicationsacross various domains such as question-answering, summarization, and knowledge-based tasks.Recent research breakthroughs are discussed, highlighting novel methods for improving retrievalefficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethicalconcerns in deployment. Future research directions are proposed, with a focus on improving therobustness of RAGmodels, expanding'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='examines ongoing challenges such as scalability, bias, and ethicalconcerns in deployment. Future research directions are proposed, with a focus on improving therobustness of RAGmodels, expanding the scope of application of RAGmodels, andaddressingsocietalimplications. This survey aims to serve as a foundational resource for researchers and practitioners inunderstandingthepotential of RAGanditstrajectoryinthefieldof natural languageprocessing.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 0, 'page_label': '1', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure1: TrendsinRAGcapturedfromrecent researchpapers\\nKeywords: Retrieval-Augmented Generation (RAG), InformationRetrieval, Natural LanguageProcessing(NLP), Artificial Intelligence(AI), MachineLearning(ML), LargeLanguageModel (LLM).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Introduction\\n1.1Introductionof Natural LanguageGeneration(NLG)\\nNatural Language Processing (NLP) has become a pivotal domain within artificial intelligence (AI), withapplications ranging from simple text classification to more complex tasks such as summarization,machinetranslation, andquestionanswering. Aparticularlysignificant branchof NLPisNatural LanguageGeneration (NLG), which focuses on the production of human-like language from structured orunstructured data. NLG's goal is to enable machines to generate coherent, relevant, and context-awaretext, improvinginteractionsbetweenhumansandmachines(Gatt et. al. 2018). AsAI evolves, thedemandfor more contextually awareandfactuallygroundedgeneratedcontent hasincreased, bringingabout newchallengesandinnovationsinNLG.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Traditional NLG models, especially sequence-to-sequence architectures (Sutskever et al. 2014), haveexhibited significant advancements ingeneratingfluent andcoherent text. However, thesemodelstendtorely heavily on training data, often struggling when tasked with generating factually accurate orcontextually rich content for queries that require knowledge beyond their trainingset. Asaresult, modelslike GPT (Radford et al. 2019) or BERT-based (Devlin et al. 2019) text generators are prone tohallucinations, where they produceplausiblebut incorrect or non-existent information(Ji et al. 2022). Thislimitation has prompted the exploration of hybrid models that combine retrieval mechanisms withgenerative capabilities to ensure both fluency and factual correctness in outputs. There has been asignificant rise in several research papers in this field and several new methods across the RAGcomponents have been proposed. Apart from new algorithms and methods, RAGhas also seen steepadoption across'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='rise in several research papers in this field and several new methods across the RAGcomponents have been proposed. Apart from new algorithms and methods, RAGhas also seen steepadoption across various applications. However, there is a gapinasufficient surveyof thisspacetrackingtheevolutionandrecent changesinthisspace. Thecurrent surveyintendstofill thisgap.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.2Overviewof Retrieval-AugmentedGeneration(RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address thelimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,which retrieves relevant documents or information from an external knowledge source, and (ii) ageneration module, which processesthisinformationtogeneratehuman-liketext (Lewiset al. 2020). Thiscombination allows RAG models to not only generate fluent text but also ground their outputs inreal-world, up-to-datedata.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 1, 'page_label': '2', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The retrieval module in RAG typically leverages dense vector representations to identify relevantdocuments from large datasets, such as Wikipedia or proprietary databases. Once retrieved, thesedocuments are passed to the generative module, often built using transformer-based architectures, togenerate responses grounded in the retrieved knowledge. This methodology helps mitigate thehallucination problem and ensures that the generated text is more factual and contextually appropriate(Thakur et al. 2021). Over the period, RAGmodels have seen applicationsinvariousdomains, includingopen-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), andpersonalizedrecommendations.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure2: Abasicflowof theRAGsystemalongwithitscomponent\\n1.3Evolutionof HybridModelsinNLP\\nBefore the introduction of RAG, NLPmodelsprimarilyreliedoneither retrieval or generationapproaches,each with its own set of advantages and limitations. Retrieval-based systems, such as traditionalinformation retrieval engines (Salton et al., 1975), efficiently provided relevant documents or snippets inresponse to a query but could not synthesize new information or present the results in a coherentnarrative. On the other hand, purely generative models, which became popular with the rise oftransformer architectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factualaccuracy.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The development of hybrid systems combining retrieval and generation began to gain momentum asresearchers recognizedthecomplementarystrengthsof bothapproaches. Earlyeffortsinhybridmodelingcan be traced back to works like DrQA(Chen et al. 2017), which employed retrieval techniques to fetchrelevant documents for question-answering tasks. However, the generative component in such systemswas minimal, often limited to selecting text directlyfromtheretrieveddocuments. Similarly, inmodelslikeInformationRetrieval (Dai et al. 2019), retrieval wastreatedasdistinct, independent components.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The real innovation came with the realization that retrieval and generation could be tightly integrated.Models like REALM (Guu et al., 2020) represented a key milestone, as they trained the retrieval andgenerative components jointly, enabling better alignment between the retrieved information and thegenerated output. RAG (Lewis et al. 2020) further extended this paradigm by using dense passageretrieval (Karpukhin et al., 2020) to fetch relevant documents and transformers like BART (Lewis et al.,2020) for a generation. This architecture provided a more seamless integration of retrieval andgeneration, allowingthemodel toanswer open-endedquestionswithbothfluencyandfactual grounding.\\n1.4Importanceof FactuallyGroundedLanguageGeneration'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 2, 'page_label': '3', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.4Importanceof FactuallyGroundedLanguageGeneration\\nOne of the main motivations for developing RAG is the increasing demand for factually accurate,contextually relevant, and up-to-date generated content. Inmanyapplications, suchascustomer service,medical diagnostics, or legal advisory systems, the need for reliable and grounded responses isparamount. Generative models that produce hallucinated or inaccurate information can lead to seriousconsequences, suchasspreadingmisinformationor providingincorrect advice(Ji et al. 2022).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='RAG models directly address these concerns by grounding their generative process in external,up-to-date knowledge sources. This groundingimprovesthefactual accuracyof theoutput andenhancesthe relevance of responses by incorporating real-world data that is directly tiedtothequery. Additionally,RAGmodels are less likely to propagate biases present in static training data, astheycanretrievemorediverseandbalancedinformationfromexternal sources\\n1.5Applicationsof RAGModels'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='1.5Applicationsof RAGModels\\nRAG models have been applied across a wide array of domains where factual accuracy and contextualunderstanding are critical. One of themost prominent applicationsisinopen-domainquestionanswering,where the model must generate answers based on a wide range of topics. RAGhas proven effective inimproving answer accuracy byretrievingrelevant informationandthengeneratingresponsesgroundedinthat data (Izacard et. al. 2021). Models like Dense PassageRetrieval (DPR) (Karpukhinet al., 2020) andFusion-in-Decoder (Izacardet. al. 2021) havebeenusedtogreat effect inthiscontext, showingsignificantimprovementsover traditional generativeor retrieval-onlymodels.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"In conversational AI, RAGmodels have enhanced the capabilities of dialogue systems by ensuring thatresponses are both coherent and grounded in factual information (Roller et al., 2020). For example,chatbots used in customer service can benefit fromRAG's ability to retrievespecificdetailsfromproductdatabasesor documentation, leadingtomoreaccurateanduseful responsesfor end-users.\\nOther applications include medical diagnosis systems, where RAGcan retrieve and integrate the latestresearch findings or patient-specific data togenerateaccuratediagnosticsuggestions, andlegal advisorysystems, where the model can retrieve relevant case law or statutes to provide legally sound advice.Furthermore, RAGhasfoundapplicationsinpersonalizedrecommendationsystems, whereit canretrieveuser preferencesor past interactionsandgeneratepersonalizedsuggestions.\\n1.6ChallengesandLimitationsof RAG\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Despite the promise of RAGmodels, several challenges need attention. The retrieval mechanism, whilepowerful, can still struggle with retrieving the most relevant documents, particularly when dealing withambiguous queries or niche knowledge domains. The reliance on dense vector representations, suchasthose used in DPR, can sometimes lead to irrelevant or off-topic documents being retrieved. Efforts torefine retrieval techniques, including the incorporation of more sophisticated query expansion andcontextual disambiguation, are needed to improve performance in these areas. The integration betweenretrieval and generation, while seamless in theory, can sometimes fail in practice. For instance, thegenerative module may not always effectively incorporate the retrieved information into its responses,leading to inconsistencies or incoherence between the retrieved facts and the generated text. Researchinto better alignment mechanisms, such as improved attention models or hierarchical fusion'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='to inconsistencies or incoherence between the retrieved facts and the generated text. Researchinto better alignment mechanisms, such as improved attention models or hierarchical fusion techniques,may help alleviate these issues (Izacard et. al. 2021). Additionally, the computational overhead of RAGmodels is a concern, as they require both a retrieval and a generation step for each query. This dualprocess can be resource-intensive, particularly for large-scale applications (Borgeaud et al. 2021).Techniques such as model pruning (Han et al. 2015) or knowledge distillation (Sanh et al., 2019) mayoffer ways to reduce the computational burden without sacrificing performance. Finally, there are ethicalconcerns associated with the deployment of RAGmodels, particularly in termsof biasandtransparency.Biases in AI and LLM have been a well-researched and evolving field with researchers identifyingdifferent types of biases not limited to Gender, socio-economic class, or even educational'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 3, 'page_label': '4', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='in AI and LLM have been a well-researched and evolving field with researchers identifyingdifferent types of biases not limited to Gender, socio-economic class, or even educational background(Gupta et. al. 2024; Ranjan et. al., 2024). While RAG has the potential to reduce biases by retrievingmore balanced information, there is still the risk of amplifying biases present in the retrieved sources'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='(Binns, 2018). Furthermore, ensuring transparency in how retrieval results are selected and used ingenerationiscrucial for maintainingtrust inthesesystems.\\n1.7Scopeof theSurvey\\nThis paper aims to provide a comprehensive survey of RAG models, covering their evolution, keyarchitectural components, recent research in this area, current challenges and limitations of RAG, andfutureresearchdirection.\\n2: CoreComponentsandArchitectural Overviewof RAGSystems\\n2.1Overviewof RAGModels'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2: CoreComponentsandArchitectural Overviewof RAGSystems\\n2.1Overviewof RAGModels\\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments naturallanguage generation (NLG) with external retrieval mechanisms to enhance themodel'sknowledgebase.Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vastcorpora, rely entirely on their internal representations of knowledge, making themsusceptible to issueslike hallucinations—where the models generate plausible but incorrect information. Thesemodelscannotefficiently update their knowledge bases without retraining, making them less practical for dynamic,knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al.2020). Toovercometheselimitations, thepaper (Lewiset al. 2020) proposedtheRAGarchitecture, whichretrievesreal-time, relevant external documentstogroundthegeneratedtext infactual information.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='TheRAGmodel incorporatestwokeycomponents:\\n1. Retriever: This retrieves the most relevant documents froma corpus using techniques such asdensepassageretrieval (DPR) (Karpukhinet. al. 2020) or traditional BM25algorithms.2. Generator: It synthesizes the retrieved documents into coherent, contextually relevantresponses.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='RAG’s strength lies in its ability to leverage external knowledge dynamically, allowing it to outperformgenerative models like GPT-3andknowledge-groundedsystemslikeBERT, whichrelyonstaticdatasets.In open-domain question answering, RAG has been demonstrated to be highly effective, consistentlyretrievingrelevant informationandimprovingthefactual accuracyof thegeneratedresponses(Guu, K., etal. 2020). In addition to knowledge retrieval, RAGmodels excel at updating knowledgebases. Sincethemodel fetches external documents for each query, it requires no retraining to incorporate the latestinformation. This flexibility makes RAG models particularly suitable for domains where information isconstantly evolving, such as medical research, financial news, and legal proceedings. Furthermore,studies have shown that RAGmodels achieve superior results in a varietyof knowledge-intensivetasks,includingdocument summarizationand, knowledge-groundeddialogues\\n2.2Retriever MechanismsinRAGSystems'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 4, 'page_label': '5', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2.2Retriever MechanismsinRAGSystems\\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.Effective retrieval ensures that the model's output is grounded in accurate information. Several retrievalmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticatedtechniqueslikeDensePassageRetrieval (DPR).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.1BM25'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"BM25 is a well-established informationretrieval algorithmthat usesthetermfrequency-inversedocumentfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25remains a strong baseline for many modern retrieval systems, including those used in RAG models.BM25 calculates therelevancescoreof adocument basedonhowfrequentlyaquerytermappearsinthedocument while adjusting for the document's length and the frequency of the term across the corpus(Robertson et. al. 2009). WhileBM25iseffectivefor keywordmatching, it haslimitationsinunderstandingsemantic meaning. For example, BM25 cannot capture the relationships between words and tends toperform poorly on more complex, natural language queries that require an understanding of context.Despite this limitation, BM25 is still widely used because of itssimplicityandefficiency. BM25iseffectivefor tasks involvingsimpler, keyword-basedqueries, althoughmoremodernretrieval modelslikeDPRtendtooutperformit\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='BM25 is still widely used because of itssimplicityandefficiency. BM25iseffectivefor tasks involvingsimpler, keyword-basedqueries, althoughmoremodernretrieval modelslikeDPRtendtooutperformit insemanticallycomplextasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.2DensePassageRetrieval (DPR)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Dense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modernapproach to information retrieval. It uses a dense vector space in which both the query and thedocuments are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, wherethe query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xionget. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query anddocuments, making it highly effective for open-domain question-answering tasks. The strength of DPRlies in its ability to retrieve relevant information based on semantic meaning rather than keywordmatching. By training the retriever on a large corpus of question-answer pairs, DPRcan finddocumentsthat are contextually related to the query, even when the query and the document do not share exactterms. Recent research has further improved DPRbyintegratingit withpre-trainedlanguagemodelsandanexampleisLLMadaptedfor'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='related to the query, even when the query and the document do not share exactterms. Recent research has further improved DPRbyintegratingit withpre-trainedlanguagemodelsandanexampleisLLMadaptedfor thedenseRetrievAl approach(Li et. al. 2023)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.2.3REALM(Retrieval-AugmentedLanguageModel)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Another significant advancement in retrieval mechanisms for RAGmodels is REALM(Guu et al. (2020).REALM integrates retrieval into the language model's pre-training process, ensuring that the retriever isoptimized alongside the generator for downstreamtasks. ThekeyinnovationinREALMisthat it learnstoretrieve documents that improve the model’s performance on specific tasks, suchasquestionansweringor document summarization. During training, REALM updates both the retriever and the generator,ensuring that the retrieval process is optimized for the generation task. REALM’s retriever is trained toidentify documents that are not only relevant to the query but also helpful for generating accurate andcoherent responses. As a result, REALM significantly improves the quality of generated responses,particularly in tasks that require external knowledge. Recent studies have demonstrated that REALMoutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='in tasks that require external knowledge. Recent studies have demonstrated that REALMoutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval istightlycoupledwithgeneration.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 5, 'page_label': '6', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='The core of RAG lies in the quality of retrieved passages, but many current methods rely onsimilarity-based retrieval (Mallen et al. 2022). Self-RAG (Asai et al. 2023b), and REPLUG (Shi et al.,2023) have advanced by leveraging LLMs to enhance retrieval capabilities, achieving more adaptiveretrieval. After initial retrieval, cross-encoder models are used to re-rank the retrieved results by jointlyencoding the query and each retrieved document to compute relevance scores. These models providemore context-aware retrieval at the cost of higher computational overhead. Pointwise and PairwiseRanking, often based on Learning-to-Rank (LTR) algorithms, are used to assign relevance scores to'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='retrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAGsystems utilizeself-attentionwithintheLLMtomanagecontext andrelevanceacrossdifferent partsof theinput and retrieved text. Cross-attentionmechanismsareusedwhenintegratingretrievedinformationintothe generative model, ensuring that the most relevant pieces of information are emphasized duringgeneration.\\n2.3Generator MechanismsinRAGSystems\\nIn Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role inproducing the final output by integrating retrieved information with the input query. After the retrievalcomponent pulls relevant knowledge from external sources, the generator synthesizes this informationinto coherent, contextually appropriate responses. The Large Language Model (LLM) serves as thebackbone of the generator, which ensures the generated text is fluent, accurate, and aligned with theoriginal query.\\n2.3.1T5(Text-to-Text Transfer Transformer)'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"2.3.1T5(Text-to-Text Transfer Transformer)\\nT5 (Text-to-Text Transfer Transformer) (Raffel et al. 2020) is one of the most commonlyusedmodelsforgeneration tasks in RAGsystems. T5isversatileinitsapproach, framingeveryNLPtaskasatext-to-texttask. This uniform framework allows T5 to be fine-tuned for a wide range of tasks, includingquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,T5-based RAG models have been shown to outperform traditional generative models like GPT-3 andBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.Moreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAGsystemsthat needtotackleadiverserangeof knowledge-intensivetasks.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='2.3.2BARTBART(Bidirectional andAuto-RegressiveTransformer), introducedbyLewiset al. (2020), isanotherprominent generativemodel usedinRAGsystems. BARTisparticularlywell-suitedfor tasksinvolvingtextgenerationfromnoisyinputs, suchassummarizationandopen-domainquestionanswering. Asadenoisingautoencoder, BARTcanreconstruct corruptedtext sequences, makingit robust for tasksthatrequirethegenerationof coherent, factual outputsfromincompleteor noisydata. Whenpairedwitharetriever inaRAGsystem, BARThasbeenshowntoimprovethefactual accuracyof generatedtext bygroundingit inexternal knowledge. Studieshavedemonstratedthat BART-basedRAGmodelsachievestate-of-the-art resultsinvariousknowledge-intensivetasks, includingdialoguegenerationandnewssummarization.\\n3. Retrieval-AugmentedGenerationModelsAcrossDifferent Modalities'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 6, 'page_label': '7', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='3.1 Text-Based RAG Models: Text-based RAG models represent the most mature and widelyresearched category. These models leverage textual data for both retrieval and generation tasks,enabling applications such as question-answering, summarization, and conversational agents.Transformer architectures, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), arefoundational in text-based RAG models. These models utilize self-attention mechanisms to capturecontextual relationships within text, which enhances both retrieval accuracy and generation fluency.Dense retrieval models, such as those using dense embeddings fromBERT, offer superior performancecompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leveragedense representations to retrieve relevant documents more effectively. Recent advancements focus onintegrating retrieval and generation into a single training pipeline. REALM (Guu et al., 2020) is an'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='example of such anend-to-endmodel that jointlyoptimizesretrieval andgenerationprocesses, improvingoverall taskperformance.\\n3.2 Audio-Based RAGModels: Audio-based RAGmodels extend the principles of retrieval-augmentedgeneration to the audio modality, enablingapplicationssuchasspeechrecognition, audiosummarization,and conversational agents in voice interfaces. Audiodataisoftenrepresentedusingembeddingsderivedfrom pre-trained models like Wav2Vec 2.0 (Baevski et al., 2020). These embeddings serve as input toretrieval andgenerationcomponents, enablingthemodel tohandleaudiodataeffectively.\\n3.3 Video-Based RAG Models: Video-based RAG models incorporate both visual and textualinformation to enhance performance in tasks such as video understanding, captioning, and retrieval.Video data is represented using embeddings from models like I3D (Xie et. al. 2017) or TimeSformer(Bertasius et al. 2021). These embeddings capture temporal and spatial features essential for effectiveretrieval andgeneration.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='3.4 Multimodal RAG Models: Multimodal RAG models integrate data from multiple modalities—text,audio, video, and images—to provide a more holistic approach to retrieval andgenerationtasks. Modelslike Flamingo (Alayrac et al., 2022) integrate multiple modalities into a unified framework, enablingsimultaneous processing of text, images, and videos. Techniques for cross-modal retrieval involveretrievingrelevant informationacrossdifferent modalities(Li. et. al. 2023).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 7, 'page_label': '8', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Multimodal capabilities enhance the versatility and efficiency of RAG across various applications.”Retrieval as generation” (Wang et. al. 2024) extends the Retrieval-Augmented Generation (RAG)framework tomultimodal applicationsbyincorporatingtext-to-imageandimage-to-text retrieval. Utilizingalarge dataset of pairedimagesandtext descriptions, thesystemacceleratesimagegenerationwhenuserqueries align with stored text descriptions (\"retrieval as generation\"). The image-to-text functionalityallowsuserstoengageindiscussionsbasedoninput images.\\nFigure3: Timelineof theevolutionof theRAGsystemanditscomponents'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='4. Recent Advancement inthefield:'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='There has been significant advancement in this field and this section intendstocapturekeyfindingsof afewimportant recent papers. Anovel agenticRetrieval-AugmentedGeneration(RAG) framework(Ravuruet. al. 2024) employs a hierarchical, multi-agent architecturewherespecializedsub-agents, usingsmallerpre-trained language models (SLMs), are fine-tuned for specific time series tasks. The master agentdelegates tasks to these sub-agents, who retrieve relevant prompts fromasharedknowledgerepository.In this modular, multi-agent approach, the authors achieve state-of-the-art performance demonstratingimproved flexibility and effectiveness over task-specificmethodsintimeseriesanalysis. RULE(Xiaet. al.2024), a multimodal Retrieval-Augmented Generation (RAG) framework designed to improve thefactuality of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG byintroducing a calibrated selection strategy to control factuality risk, and, by developing a preferenceoptimization'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG byintroducing a calibrated selection strategy to control factuality risk, and, by developing a preferenceoptimization strategy to balance the model’s intrinsic knowledge with retrieved contexts, proving itseffectiveness in enhancing factual accuracy in Med-LVLM systems. METRAG (Gan et. al. 2024), amulti-layered, thoughts-enhanced retrieval-augmented generation framework, integratesLLMsupervisionto generate utility-oriented thoughts and combines document similarity with utility for improvedperformance. It also incorporates a task-adaptive summarizer to produce compact thoughts. Using themulti-layered thoughts from these stages, an LLM generates knowledge-augmented content,demonstrating superior performance on knowledge-intensive tasks compared to traditional approaches.Distractor document is'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 8, 'page_label': '9', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Figure4: EvolvingTrendsinRAGcapturedfromresearchpapers'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"one of the key traits of Retrieval AugmentedFine-Tuning(RAFT) (Zhanget. al. 2024) wherethemodel istrained to disregard irrelevant, distractor documents and instead cite directly fromrelevant sources. Thisprocess, combined with a chain-of-thought reasoning style, enhances themodel'sreasoningcapabilities.RAFT demonstrates consistent performance improvements in domain-specific RAG tasks, includingPubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO(Wang et. al. 2023) , a method designed toenhancethequalityof context providedtogenerativemodelsin tasks like open-domain question answering and fact verification, addresses issues of over- orunder-relianceonretrievedpassages, whichcanleadtoproblemssuchashallucinationsinthegeneratedoutputs. The method improves context quality by identifying useful context through lexical andinformation-theoretic approaches and training context filtering models to refine retrieved contexts duringtest time.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='method improves context quality by identifying useful context through lexical andinformation-theoretic approaches and training context filtering models to refine retrieved contexts duringtest time. ReflectionTokenisakeyattributeof Self-reflectiveRetrieval Augmented-Generation(Self-RAG)(Asai et. al. 2023), anovel frameworkdesignedtoimprovethefactual accuracyof largelanguagemodels(LLMs) by combining retrieval with self-reflection. Unlike traditional methodsthat retrieveandincorporatea fixed number of passages, Self-RAGadaptively retrievesrelevant passagesandusesreflectiontokensto evaluate and refine its responses, allowing the model to adjust its behavior according to task-specificneeds and has shown superior performance in open-domain question-answering, reasoning, factverification, and long-formgenerationtasks. Intelligenceandeffectivenessof RAGaredependent alot onthe quality of retrieval and more meta-data understanding of the repository would enhance theeffectiveness of the'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='and long-formgenerationtasks. Intelligenceandeffectivenessof RAGaredependent alot onthe quality of retrieval and more meta-data understanding of the repository would enhance theeffectiveness of the RAGsystem. Anovel data-centric Retrieval-Augmented Generation (RAG) workflowadvances beyond the traditional retrieve-then-read mode and employs aprepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextuallyrelevant, time-critical, or domain-specific information. Key innovations include generating metadata,synthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MKSummary)for clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG(Chang et. al. 2024), a zero-shot framework that integrates community structures within KnowledgeGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances theaccuracy and contextual relevance of fact-checking by utilizing multi-hop connections'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='within KnowledgeGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances theaccuracy and contextual relevance of fact-checking by utilizing multi-hop connections within KGs,outperforming traditional methods without requiring additional domain-specific training. The RAPTORmodel (Sarthi et. al. 2024) introduces a hierarchical approach to retrieval-augmented language models,addressing limitations in traditional methods that retrieve only short, contiguous text chunks. RAPTORforms a summary tree to retrieve information at varying abstraction levels by recursively embedding,clustering, and summarizing text. Experiments demonstrate RAPTOR’s superior performance, especiallyin question-answering tasks requiring complex reasoning. When paired with GPT-4, RAPTORimprovesaccuracyontheQuALITYbenchmarkby20%.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"This advancement in RAGfurther provestheutilityof theRAGsystemhowever recent LLMlaunchesthatsupport long-termcontext havesignificantlyshownimprovedperformance. Arecent study(Li et. al. 2024)compared the efficiency of Retrieval Augmented Generation (RAG) and long-context (LC) LargeLanguage Models (LLMs), such as Gemini-1.5 and GPT-4. While LC models outperform RAG whenadequately resourced, RAG's cost-efficiency remains advantageous. To balance performance and cost,the paper introduces Self-Route. This method dynamically directs queries to either RAGor LCbasedonmodel self-reflection, optimizing both computation cost and performance. This study offers valuableinsights into the optimal application of RAGand LCin handling long-context tasks. Nguyen et. al., 2024introduce SFR-RAG, a small but highly efficient Retrieval AugmentedGeneration(RAG) model, whichisdesigned to enhance the integration of external contextual information into Large Language Models(LLMs) while minimizing\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 9, 'page_label': '10', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='a small but highly efficient Retrieval AugmentedGeneration(RAG) model, whichisdesigned to enhance the integration of external contextual information into Large Language Models(LLMs) while minimizing hallucinations. LA-RAG (Li et. al., 2024), a novel Retrieval-AugmentedGeneration (RAG) paradigm designed to enhance Automatic Speech Recognition (ASR) in largelanguage models (LLMs). One of the key benefits of LA-RAG is its ability to leverage fine-grainedtoken-level speech data stores alongside a speech-to-speech retrieval mechanism, improving ASR'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='accuracy by incorporating LLMin-context learning (ICL). The studyfocusesondatasetsof Mandarinandvarious Chinese dialects, demonstrating significant accuracy improvements, particularly in managingaccent variations, which have historically been a challenge for existing speech encoders. The findingshighlight LA-RAG’s potential to advance ASR technology, offering a more robust solution for diverseacoustic conditions. Large Language Models (LLMs) face challenges in AI legal and policy contextsdueto outdated knowledge and hallucinations. HyPA-RAG(Kalra et. al., 2024), aHybridParameter-AdaptiveRetrieval-Augmented Generation system, improves accuracy by using adaptive parameter tuning andhybrid retrieval strategies. Tested on NYC Local Law144 (LL144), HyPA-RAGdemonstrates enhancedcorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG(Qianet. al.,2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome thelimitations of'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='and contextual precision, addressing the complexities of legal texts. MemoRAG(Qianet. al.,2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome thelimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG’sdual-system architecture utilizes a lightweight long-range LLM to generate draft answers and guideretrieval tools, while a more powerful LLM refines the final output. This framework, optimized for bettercluing and memory capacity, significantly outperforms conventional RAG models across both complexand straightforward tasks. NLLB-E5 (Acharya et. al., 2024) introduces a scalable multilingual retrievalmodel aimed at addressing the challenges faced in supporting multiple languages, particularlylow-resource languages like Indiclanguages. ByleveragingtheNLLBencoder andadistillationapproachfromtheE5multilingual retriever, NLLB-E5enableszero-shot retrieval acrosslanguageswithout theneedfor multilingual training'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='like Indiclanguages. ByleveragingtheNLLBencoder andadistillationapproachfromtheE5multilingual retriever, NLLB-E5enableszero-shot retrieval acrosslanguageswithout theneedfor multilingual training data. Evaluations on benchmarks such as Hindi-BEIR showcase its robustperformance, highlighting task-specific challenges and advancing multilingual information access forglobal inclusivity.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='5. Current Challenges andLimitationsinRetrieval-AugmentedGeneration(RAG):\\nThissectionintendstohighlight thecurrent challengesandlimitationsof RAGconsideringthecurrentlandscapeof thesystemandthiswouldshapethefutureresearchdirectionsinthefield.\\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrievalcomponentsrelyonexternal databases, handlingvast anddynamicallygrowingdatasetsrequiresefficientretrieval algorithms. High computational costs and memory requirements also make it difficult to deployRAGmodelsinreal-timeor resource-constrainedenvironments(Shi et al. 2023), (Asai et al. 2023b).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Retrieval Quality and Relevance: Ensuring the quality andrelevanceof retrieveddocumentsremainsasignificant concern. Retrieval models can sometimes return irrelevant or outdated information, whichnegatively affects the accuracy of the generated output. Improving retrieval precision, especially forlong-formcontent generation, remainsanactiveareaof research(Mallenet al. 2022), (Shi et al. 2023).\\nBias and Fairness: Similar to other machine learning models, RAG systems can exhibit bias due tobiases present in the retrieved datasets. Retrieval-based models may amplifyharmful biasesinretrievedknowledge, leading to biased outputs in a generation. Developing bias mitigation techniquesfor retrievalandgenerationintandemisanongoingchallenge.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 10, 'page_label': '11', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Coherence: RAG models often struggle with integrating the retrieved knowledge into coherent,contextually relevant text. The alignment between retrieved passages and the generationmodel'soutputis not always seamless, leading to inconsistencies or factual hallucinations in the final response(Ji et al.2022).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Interpretability and Transparency: Like many AI systems, RAG models are often treated as blackboxes, with limited transparency in how retrieval influences generation. Improving the interpretability ofthesemodelsiscrucial tofosteringtrust, especiallyincritical applications(Roller et al. 2020).\\n6. FutureResearchDirectionsfor Retrieval-AugmentedGeneration(RAG)\\nRetrieval-augmented generation (RAG) represents a significant advancement in natural languageprocessing and related fields by combining retrieval and generative mechanisms. This section exploreskeyareasfor futureresearch, highlightingthepotential for innovationandimprovement inRAGsystems.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.1 Enhancing Multimodal Integration: The integration of text, image, audio, and video data in RAGmodels remains an evolving challenge. Future research should focus on improving multimodal fusiontechniques to enable seamless interaction between different data types. This includes developingadvanced methods for aligning and synthesizing information across modalities. Recent works (Chen et.al. 2022), (Yasunaga et. al. 2022), (Zhu et. al. 2024) have explored multimodal learning, but furtherinnovations are needed to enhance the coherence andcontextualityof multimodal outputs.Researchintocross-modal retrieval aims to improve the ability of RAGsystems to retrieve relevant information acrossdifferent modalities. For example, combining text-based queries with image or video content retrievalcould enhance applications such as visual question answering and multimedia search. This is anotherfuturedirectiontoexplorefor RAGrelatedresearch.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.2 Scaling and Efficiency: As RAG models are deployed in increasingly large-scale applications,scalability becomes a critical concern. Research should focus on developing methods toefficientlyscaleretrieval and generation processes without compromising performance. Techniques such as distributedcomputing and efficient indexing methods are essential for handling large datasets. Improving theefficiency of RAG models involves optimizing both retrieval and generation components to reducecomputational resourcesandlatency.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrievalprocesses to cater to individual user preferences and contexts. This involves developing techniques toadapt retrieval strategies based on user history, behaviour, and preferences. Enhancing the contextualadaptation of RAGmodels by deeper understandingof thecontext andsentimentsof query(Guptaet. al.2024) and the repository of ducments is crucial for improving the relevance of generated responses.Research should explore methods for dynamic adjustment of retrieval and generation processes basedontheevolvingcontext of interactions. Thisincludesincorporatinguser feedbackandcontextual cuesintotheRAGpipeline.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 11, 'page_label': '12', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.4 Ethical and PrivacyConsiderations: Addressingbiases(Shresthaet. al. 2024), (Guptaet. al. 2024)in general and specifics to RAG models is a critical area for future research. As RAG systems aredeployed in diverse applications, ensuring fairness and mitigating biases in retrieved and generatedcontent is essential. Future RAG research should focus on privacy-preserving techniques to protectsensitive information during retrieval and generation. This includes developing methods for secure datahandling and privacy-aware retrieval strategies. Interpretability of model is also a critical area to focusuponasapart of ongoingresearchinimprovingRAG.\\n6.5 Cross-Lingual and Low-Resource Languages: Expanding RAG technology to support multiplelanguages ( Chirkova et. al. 2024), especially low-resource languages, is a promising direction. Future'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='research should aimtoimprovecross-lingual retrieval andgenerationcapabilitiestoprovideaccurateandrelevant results across different languages. Enhancing RAG models to effectively support low-resourcelanguages involves developing methods to retrieve and generate content with limited training data.Research should focus on techniques for transfer learning and data augmentation to improveperformanceinunderrepresentedlanguages.\\n6.6 Advanced Retrieval Mechanisms: Future RAG research should explore dynamic retrievalmechanisms that adapt to changing query patterns and content requirements. This includes developingmodels that can dynamically update their retrieval strategiesbasedonnewinformationandevolvinguserneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, suchasdenseand sparse retrieval, could enhance the effectiveness of RAGsystems. Research shouldexplorehowtointegratedifferent retrieval methodstoachieveoptimal performancefor diversetasks.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='6.7 Integration with Emerging Technologies: Integrating RAGmodels with brain-computer interfaces(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.Research should explore how RAG systems can leverage BCI data to enhance user experience andgenerate context-aware responses.The integration of RAG with AR and VR technologies presentsopportunities for creating immersive and interactive experiences. FutureresearchshouldinvestigatehowRAG models can be used to enhance AR and VR applications by providing contextually relevantinformationandinteractions.\\n7. Conclusion'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Retrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive researchdedicated to improving retrieval effectiveness and enhancing coherent generation to minimizehallucinations. Fromitsearlyiterationstorecent advancements, RAGhasbeeninstrumental inintegratingexternal knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. Inparticular, recent domain-specific work has showcased RAG's potential in specialized areas such aslegal, medical, and low-resource language applications, highlighting its adaptability andscope. However,despite these advances, this paper identifies clear gaps that remain unresolved. Challengessuchastheintegration of ambiguous or unstructured information, effective handling of domain-specific contexts, andthe high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 12, 'page_label': '13', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments. Thefuture research directions outlined in this paper—ranging from improving retrieval mechanisms toenhancing context management and ensuring scalability—will serveasacritical guidefor thenext phaseof innovation in this space. By addressing these gaps, the next generation of RAG models has thepotential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing theboundariesof what ispossibleinretrieval-augmentedAI applications.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='References:\\nAcharya, A., Murthy, R., Kumar, V., &Sen, J. (2024). NLLB-E5: AScalable Multilingual Retrieval Model.ArXiv. /abs/2409.05401\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M.,Menick, J., Borgeaud, S., . . . Simonyan, K. (2022). Flamingo: AVisual Language Model for Few-ShotLearning. ArXiv. /abs/2204.14198\\nAsai, A., Wu, Z., Wang, Y., Sil, A., &Hajishirzi, H. (2023). Self-RAG: Learning to retrieve, generate, andcritiquethroughself-reflection. arXivpreprint arXiv:2310.11511.\\nBaevski, A., Zhou, H., Mohamed, A., &Auli, M. (2020). Wav2vec 2.0: AFramework for Self-SupervisedLearningof SpeechRepresentations. ArXiv. /abs/2006.11477\\nBertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for VideoUnderstanding?ArXiv. /abs/2102.05095'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for VideoUnderstanding?ArXiv. /abs/2102.05095\\nBinns, R. (2018). Fairness in machine learning: Lessons from political philosophy. Proceedings of the2018ConferenceonFairness, Accountability, andTransparency(pp. 149-159).\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. V., Lespiau, J.,Damoc, B., Clark, A., Casas, D. D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L.,Jones, C., Cassirer, A., . . . Sifre, L. (2021). Improving language models by retrieving from trillions oftokens. ArXiv. /abs/2112.04426\\nBrown, T., et al. (2020). \"LanguageModelsareFew-Shot Learners.\" arXivpreprint arXiv:2005.14165.\\nChang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in KnowledgeGraphsfor AdvancedRetrieval-AugmentedGenerationinFact-Checking. ArXiv. /abs/2408.08535'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Chang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in KnowledgeGraphsfor AdvancedRetrieval-AugmentedGenerationinFact-Checking. ArXiv. /abs/2408.08535\\nChen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to answer open-domainquestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume1: LongPapers) (pp. 1870-1879).\\nChen, W., Hu, H., Chen, X., Verga, P., &Cohen, W. W. (2022). MuRAG: Multimodal Retrieval-AugmentedGenerator for OpenQuestionAnsweringover ImagesandText. ArXiv. /abs/2210.02928\\nChirkova, N., Rau, D., Déjean, H., Formal, T., Clinchant, S., &Nikoulina, V. (2024). Retrieval-augmentedgenerationinmultilingual settings. ArXiv. /abs/2407.01463\\nDai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For FirstStageRetrieval. ArXiv. /abs/1910.10687'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 13, 'page_label': '14', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Dai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For FirstStageRetrieval. ArXiv. /abs/1910.10687\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectionaltransformers for language understanding. In Proceedings of the 2019 Conference of theNorthAmericanChapter of the Association for Computational Linguistics: Human Language Technologies (pp.4171-4186).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep BidirectionalTransformersfor LanguageUnderstanding. ArXiv. /abs/1810.04805\\nGan, C., Yang, D., Hu, B., Zhang, H., Li, S., Liu, Z., Shen, Y., Ju, L., Zhang, Z., Gu, J., Liang, L., &Zhou,J. (2024). Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi LayeredThoughts. ArXiv. /abs/2405.19893\\nGatt, A., &Krahmer, &E. (2018). Surveyof thestateof theart innatural languagegeneration: Coretasks,applications, andevaluation. Journal of Artificial IntelligenceResearch, 61, 65-170.\\nGupta, S., &Ranjan, R. (2024). Evaluation of LLMs Biases TowardsEliteUniversities: APersona-BasedExploration. ArXiv. /abs/2407.12801\\nGupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: FromRule-basedtomodernLLMbasedsystem. ArXiv. /abs/2409.09989'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Gupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: FromRule-basedtomodernLLMbasedsystem. ArXiv. /abs/2409.09989\\nGuu, J., Lee, K., & Pasupat, P. (2020). Retrieval-augmented generation for knowledge-intensive NLPtasks. arXivpreprint. https://arxiv.org/abs/2002.08909\\nGuu, K., Lee, K., Tung, Z., Pasupat, P., & Chang, M. (2020). REALM: Retrieval-augmented languagemodel pre-training. In Proceedings of the 37th International Conference on Machine Learning (pp.3929-3938).\\nHan, S., Pool, J., Tran, J., & Dally, W. J. (2015). Learning both weights and connections for efficientneural network. InAdvancesinNeural InformationProcessingSystems(pp. 1135-1143).\\nIzacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domainquestion answering. In Proceedings of the 16th Conference of the European Chapter of the Associationfor Computational Linguistics: MainVolume(pp. 874-880).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Chen, D., Dai, W., Chan, H. S.,Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ArXiv.https://doi.org/10.1145/3571730\\nKalra, R., Wu, Z., Gulley, A., Hilliard, A., Guan, X., Koshiyama, A., &Treleaven, P. (2024). HyPA-RAG: AHybridParameter AdaptiveRetrieval-AugmentedGenerationSystemfor AI Legal andPolicyApplications.ArXiv. /abs/2409.09046\\nKarpukhin, V., Oguz, B., Min, S., & Yih, W. (2020). Dense passage retrieval for open-domain questionanswering. arXivpreprint. https://arxiv.org/abs/2004.04906\\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. & Yih, W. T. (2020). Densepassage retrieval for open-domain question answering. In Proceedings of the 2020 Conference onEmpirical MethodsinNatural LanguageProcessing(EMNLP) (pp. 6769-6781).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 14, 'page_label': '15', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Riedel, S. (2020).Retrieval-augmented generation for knowledge-intensive NLP tasks. In Proceedings of the 34thInternational ConferenceonNeural InformationProcessingSystem( pp. 9459-9474).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Li, C., Liu, Z., Xiao, S., & Shao, Y. (2023). Making Large Language Models A Better Foundation ForDenseRetrieval. ArXiv. /abs/2312.15503\\nLi, F., Zhu, L., Wang, T., Li, J., Zhang, Z., & Shen, H. T. (2023). Cross-Modal Retrieval: ASystematicReviewof MethodsandFutureDirections. ArXiv. /abs/2308.14263\\nLi, S., Shang, H., Wei, D., Guo, J., Li, Z., He, X., Zhang, M., & Yang, H. (2024). LA-RAG:EnhancingLLM-basedASRAccuracywithRetrieval-AugmentedGeneration. ArXiv. /abs/2409.08597\\nLi, S., Park, S., Lee, I., & Bastani, O. (2023). TRAQ: Trustworthy Retrieval Augmented QuestionAnsweringviaConformal Prediction. ArXiv. /abs/2307.04642\\nLi, Z., Li, C., Zhang, M., Mei, Q., & Bendersky, M. (2024). Retrieval Augmented Generation orLong-Context LLMs?AComprehensiveStudyandHybridApproach. ArXiv. /abs/2407.16833\\nLiu, Z., Wang, H., Niu, Z., Wu, H., Che, W., &Liu, T. (2020). Towards Conversational Recommendationover Multi-TypeDialogs. ArXiv. /abs/2005.03954'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Liu, Z., Wang, H., Niu, Z., Wu, H., Che, W., &Liu, T. (2020). Towards Conversational Recommendationover Multi-TypeDialogs. ArXiv. /abs/2005.03954\\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., & Hajishirzi, H. (2022). When Not to TrustLanguage Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ArXiv./abs/2212.10511\\nMombaerts, L., Ding, T., Banerjee, A., Felice, F., Taws, J., &Borogovac, T. (2024). Meta Knowledge forRetrieval AugmentedLargeLanguageModels. ArXiv. /abs/2408.09017\\nNguyen, X., Pandit, S., Purushwalkam, S., Xu, A., Chen, H., Ming, Y., Ke, Z., Savarese, S., Xong, C., &Joty, S. (2024). SFR-RAG: TowardsContextuallyFaithful LLMs. ArXiv. /abs/2409.09916\\nNiu, C., Wu, Y., Zhu, J., Xu, S., Shum, K., Zhong, R., Song, J., & Zhang, T. (2023). RAGTruth: AHallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models. ArXiv./abs/2401.00396'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 15, 'page_label': '16', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Qian, H., Zhang, P., Liu, Z., Mao, K., &Dou, Z. (2024). MemoRAG: Moving towards Next-Gen RAGViaMemory-InspiredKnowledgeDiscovery. ArXiv. /abs/2409.05591\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models areunsupervisedmultitasklearners. OpenAI Blog, 1(8), 9.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., &Liu, P. J. (2019).ExploringtheLimitsof Transfer LearningwithaUnifiedText-to-Text Transformer. ArXiv. /abs/1910.10683\\nRanade, P., & Joshi, A. (2023). FABULA: Intelligence Report Generation Using Retrieval-AugmentedNarrativeConstruction. ArXiv. https://doi.org/10.1145/3625007.3627505\\nRanjan, R., Gupta, S., & Singh, S. N. (2024). A Comprehensive Survey of Bias in LLMs: CurrentLandscapeandFutureDirections. ArXiv. /abs/2409.16430\\nRavuru, C., Sakhinana, S. S., &Runkana, V. (2024). Agentic Retrieval-Augmented Generation for TimeSeriesAnalysis. ArXiv. /abs/2408.14484'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Robertson, S.G., & Zaragoza,H., (2009). The Probabilistic Relevance Framework: BM25 and Beyond,FoundationsandTrendsinInformationRetrieval, 3(4), pp. 333-389.\\nRoller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Shuster, K., Smith, E. M.,Boureau, Y., &Weston, J. (2020). Recipesfor buildinganopen-domainchatbot. ArXiv. /abs/2004.13637\\nSalton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic indexing.Communicationsof theACM, 18(11), 613-620.\\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller,faster, cheaper andlighter. ArXiv. /abs/1910.01108\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., &Manning, C. D. (2024). RAPTOR: RecursiveAbstractiveProcessingfor Tree-OrganizedRetrieval. ArXiv. /abs/2401.18059'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., &Manning, C. D. (2024). RAPTOR: RecursiveAbstractiveProcessingfor Tree-OrganizedRetrieval. ArXiv. /abs/2401.18059\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., & Yih, W.-T. (2023).REPLUG: Retrieval-augmentedblack-boxlanguagemodels. arXivpreprint arXiv:2301.12652.\\nShrestha, R., Zou, Y., Chen, Q., Li, Z., Xie, Y., &Deng, S. (2024). FairRAG: Fair Human GenerationviaFair Retrieval Augmentation. ArXiv. /abs/2403.19964\\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. InAdvancesinNeural InformationProcessingSystems(pp. 3104-3112).\\nThakur, N., Bonifacio, L., Zhang, X., Ogundepo, O., Kamalloo, E., Li, X., Liu, Q., Chen, B.,Rezagholizadeh, M., &Lin, J. (2023). NoMIRACL: KnowingWhenYouDon't Knowfor Robust MultilingualRetrieval-AugmentedGeneration. ArXiv. /abs/2312.11361\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content=\"Thakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A HeterogenousBenchmarkfor Zero-shot Evaluationof InformationRetrieval Models. ArXiv, abs/2104.08663.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &Polosukhin, I.(2017). Attentionisall youneed. InAdvancesinNeural InformationProcessingSystems(pp. 5998-6008).\\nWang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., Shi, T., Wang, Z., Li, S., Qian, Q., Yin, R., Lv, C.,Zheng, X., &Huang, X. (2024). Searching for Best Practices in Retrieval-Augmented Generation. ArXiv./abs/2407.01219\\nWang, Z., Araki, J., Jiang, Z., Parvez, M. R., & Neubig, G. (2023). Learning to Filter Context forRetrieval-AugmentedGeneration. ArXiv. /abs/2311.08377\\nXia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., &Yao, H. (2024). RULE: ReliableMultimodal RAGfor FactualityinMedical VisionLanguageModels. ArXiv. /abs/2407.05131\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 16, 'page_label': '17', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Xia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., &Yao, H. (2024). RULE: ReliableMultimodal RAGfor FactualityinMedical VisionLanguageModels. ArXiv. /abs/2407.05131\\nXie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2017). Rethinking Spatiotemporal Feature Learning:Speed-AccuracyTrade-offsinVideoClassification. ArXiv. /abs/1712.04851\\nXiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P., Ahmed, J., &Overwijk, A. (2020). ApproximateNearest Neighbor NegativeContrastiveLearningfor DenseText Retrieval. ArXiv. /abs/2007.00808'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'page': 17, 'page_label': '18', 'source_file': 'Comprehensive Survey of RAG.pdf', 'file_type': 'pdf'}, page_content='Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &Yih, W. (2022). Retrieval-AugmentedMultimodal LanguageModeling. ArXiv. /abs/2211.12561\\nZhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., & Gonzalez, J. E. (2024). RAFT:AdaptingLanguageModel toDomainSpecificRAG. ArXiv. /abs/2403.10131\\nZhu, Y., Ren, C., Xie, S., Liu, S., Ji, H., Wang, Z., Sun, T., He, L., Li, Z., Zhu, X., & Pan, C. (2024).REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via LargeLanguageModels. ArXiv. /abs/2402.07016'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nSUFFICIENT CONTEXT : A N EW LENS ON RETRIEVAL\\nAUGMENTED GENERATION SYSTEMS\\nHailey Joren∗\\nUC San Diego\\nhjoren@ucsd.edu\\nJianyi Zhang†\\nDuke University\\njianyi.zhang@duke.edu\\nChun-Sung Ferng\\nGoogle\\ncsferng@google.com\\nDa-Cheng Juan\\nGoogle\\ndacheng@google.com\\nAnkur Taly\\nGoogle\\nataly@google.com\\nCyrus Rashtchian\\nGoogle\\ncyroid@google.com\\nABSTRACT\\nAugmenting LLMs with context leads to improved performance across many\\napplications. Despite much research on Retrieval Augmented Generation (RAG)\\nsystems, an open question is whether errors arise because LLMs fail to utilize the\\ncontext from retrieval or the context itself is insufficient to answer the query. To\\nshed light on this, we develop a new notion of sufficient context, along with a\\nmethod to classify instances that have enough information to answer the query. We\\nthen use sufficient context to analyze several models and datasets. By stratifying'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='method to classify instances that have enough information to answer the query. We\\nthen use sufficient context to analyze several models and datasets. By stratifying\\nerrors based on context sufficiency, we find that larger models with higher baseline\\nperformance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when\\nthe context is sufficient, but often output incorrect answers instead of abstaining\\nwhen the context is not. On the other hand, smaller models with lower baseline\\nperformance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient\\ncontext. We further categorize cases when the context is useful, and improves\\naccuracy, even though it does not fully answer the query and the model errs without\\nthe context. Building on our findings, we explore ways to reduce hallucinations in\\nRAG systems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='RAG systems, including a new selective generation method that leverages sufficient\\ncontext information for guided abstention. Our method improves the fraction of\\ncorrect answers among times where the model responds by 2–10% for Gemini,\\nGPT, and Gemma. Key findings and the prompts used in our autorater analysis are\\navailable on our github.\\n1 I NTRODUCTION\\nProviding Large Language Models (LLMs) with additional context, such as in Retrieval Augmented\\nGeneration (RAG) systems, has led to major improvements in LLM factuality and verifiability when\\nadapting to new domains (Lewis et al., 2020). In the case of open-domain question answering, a\\nretrieval model provides context at inference time in the form of snippets or long-form text (Zhu\\net al., 2021). Then, the model synthesizes the query along with this added context to generate the\\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='et al., 2021). Then, the model synthesizes the query along with this added context to generate the\\nanswer. Unfortunately, current RAG-based LLMs exhibit many undesirable traits, such as confidently\\npredicting the incorrect answer with retrieved evidence (Mishra et al., 2024; Niu et al., 2024; Ru\\net al., 2024), being distracted by unrelated information (Cuconasu et al., 2024; Yoran et al., 2024),\\nand failing to properly extract answers from long text snippets (Hsieh et al., 2024; Liu et al., 2024).\\nThe ideal outcome is for the LLM to output the correct answer if the provided context contains\\nenough information to answer the question when combined with the model’s parametric knowledge.\\nOtherwise, the model should abstain from answering and/or ask for more information. One core\\nchallenge in achieving this ideal outcome is building models that can use the provided context only\\nwhen it helps answer the question correctly. Several works have investigated this issue by evaluating'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 0, 'page_label': '1', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='when it helps answer the question correctly. Several works have investigated this issue by evaluating\\n∗Work done during an internship at Google.\\n†Work done during an internship at Google.\\n1\\narXiv:2411.06037v3  [cs.CL]  23 Apr 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nmodels in the presence of irrelevant information in the context (discussed in Section 2). However,\\n“relevant information” can range from directly containing the answer to simply being topically related\\nto the question. Even “golden” or oracle documents in datasets vary in how much information they\\nprovide about the query, and whether they directly inform the ground truth answer or not. In other\\nwords, while the goal seems to be to understand how LLMs behave when they do or do not have\\nsufficient information to answer the query, prior work fails to address this head-on.\\nAs our first contribution, we put forth a new notion of sufficient context. We divide instances into two\\ncategories based on whether the context provides enough information to construct an answer to the\\nquery. The sufficient context designation is a function of an input pair consisting of one question and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='query. The sufficient context designation is a function of an input pair consisting of one question and\\nthe associated context. Crucially, it does not require a ground truth answer. Figure 1 shows examples\\nand a breakdown of model responses after splitting the data based on sufficient vs. insufficient context.\\nTo divide the dataset, we use an LLM-based autorater to classify instances as sufficient or not. Here,\\nan autorater is a model that evaluates instances based on a property, e.g., a sufficient context autorater.\\nUsing our sufficient context autorater, we uncover new insights into LLM behavior and into existing\\nbenchmark datasets. First, we find models generate incorrect answers on a non-trivial fraction of\\ninstances that have sufficient context to answer the query. In other words, open-book QA cannot\\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='instances that have sufficient context to answer the query. In other words, open-book QA cannot\\nbe solved by improving retrieval alone. Second, when given instances without sufficient context,\\nmodels tend to hallucinate more than they abstain, especially for multi-hop questions. This finding\\ncomplements prior work, which shows that LLMs are not robust to noisy retrieval (Yoran et al.,\\n2024; Wu et al., 2024). Third, models generate correct answers in many cases, even when the\\nprovided context is insufficient. Surprisingly, this remains true after we filter out questions that the\\nmodel answers correctly in a closed book (w/o RAG) setting. Together, our analysis deepens our\\nunderstanding of RAG systems by revealing nuances in how models generate responses with retrieval.\\nAs a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\\nWe implement a new selective generation framework that improves accuracy. We use a smaller,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='As a final contribution, we explore ways to use sufficient context labels to reduce model hallucinations.\\nWe implement a new selective generation framework that improves accuracy. We use a smaller,\\nintervention model to determine when the model generates or abstains, providing a controllable\\ntrade-off. Moreover, we can combine our method with any LLM, including proprietary models like\\nGemini and GPT. Our main result is that using sufficient context as an additional signal leads to\\nmuch higher accuracy over the fraction of answered queries, for most coverage levels and across\\nmultiple models/datasets. We also find that fine-tuning open-source models with sufficient context\\ninformation does not easily reduce the hallucination rate. Instead, for Mistral 3, fine-tuning can lead\\nto a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\\nour autorater analysis are available on our github.\\nTo summarize, our main contributions are'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='to a higher abstention rate at the cost of fewer correct answers. Key findings and the prompts used in\\nour autorater analysis are available on our github.\\nTo summarize, our main contributions are\\n1. We define the notion of sufficient context, unifying existing work on relevance for RAG systems.\\nThen, we design a sufficient context autorater (achieving 93% accuracy), enabling us to label\\ninstances scalably and to analyze model responses with or without sufficient context.\\n2. Our analysis leads to several new findings about retrieval-augmented model performance. One\\ntakeaway is that SOTA LLMs output correct responses 35–62% of the time with insufficient\\ncontext. Hence, intervention strategies to increase accuracy should not solely rely on sufficiency.\\n3. Building on our findings above, we develop an efficient and general method for selective generation,\\nusing both confidence and sufficient context signals. Our method improves the fraction of correct'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 1, 'page_label': '2', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='using both confidence and sufficient context signals. Our method improves the fraction of correct\\nanswers (among total model responses) by up to 2–10% for Gemini, GPT, and Gemma.\\n2 R ELATED WORK\\nMany papers have shown that reaping the benefits of RAG (e.g., better factuality) will require a deeper\\nunderstanding of how LLMs respond to variations in the queries and provided context (Asai et al.,\\n2024; Fan et al., 2024; Ram et al., 2023; Rau et al., 2024). We review two main areas. First, much\\nwork has evaluated RAG systems with poor retrieval, uncovering cases where LLMs are led astray by\\nirrelevant context. Another line of study aims to reduce LLM hallucinations in RAG settings.\\n(Ir)relevant Context. Prior studies uncover a lack of robustness to imperfect retrieval. However,\\nthese studies vary in terms of how they evaluate retrieval quality, without anchoring to a precise\\n“relevance” definition. Shi et al. (2023a) adds sentences to math questions (based on GSM8K) which\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nProprietary + Conﬁdential\\nQuestion: Who is Lya L. married to?\\nInsufficient \\nContext\\nLya L. married Tom in 2006… \\nThey divorced in 2014… Lya went \\non dates with Paul in 2018…\\nContext C\\nSufficient \\nContext\\nLya L. married Paul in 2020… They \\nlooked happy together at the \\nrecent event.\\nContext A\\nExamples: sufficient context\\nInsufficient \\nContext\\nLya L. is an astronaut, born in \\nOhio…. Lya has two children… \\nLya’s parents are lawyers…\\nContext D\\nSufficient \\nContext\\nLya L. – Wikipedia\\nBorn: October 1, 1980\\nSpouse: Paul (m. 2020)\\nContext B\\nCategorizing Model Responses (Musique Dataset)\\nFigure 1: New insights into RAG systems by looking at whether instances have sufficient context.\\nOn the left, we show examples of sufficient context; on the right, a breakdown of model responses on\\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='On the left, we show examples of sufficient context; on the right, a breakdown of model responses on\\nthe Musique dataset. Adding RAG improves the percentage of correct answers. Unfortunately, with\\nRAG, models hallucinate more than abstain, and the insufficiency of the context does not account for\\nthis major issue. Also, standard datasets have many instances with insufficient context (here, 55.4%).\\nWe include results for other datasets (FreshQA, HotPotQA) in Appendix B, showing similar trends.\\nshould not impact the answer at all (and GSM8K is designed to have sufficient context by definition).\\nXie et al. (2024) looks at having counter-memory context, by either replacing the entity name with\\nan erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='an erroneous one or using an LLM to generate a synthetic context supporting the erroneous entity.\\nRet-Robust (Yoran et al., 2024) trains a model to be robust to irrelevant context, with an NLI-based\\nentailment to determine relevance, and only uses the relevance scores to influence the training mixture\\nof relevant vs. irrelevant documents. Wu et al. (2024) looks at questions where the LLM gets the\\nanswer correct without retrieval and is non-robust to changes in the retrieval. Multiple methods use\\na model to predict relevance scores (as part of a larger pipeline), without calibration to a formal\\ndefinition (Wang et al., 2024a; Zhou et al., 2024), including for iterative retrieval (Jiang et al., 2024;\\nYan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\\nrelevant documents, but simply uses “does not contain the answer” as a proxy for irrelevant context.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Yan et al., 2024). In terms of analysis studies, Cuconasu et al. (2024) distinguishes golden and\\nrelevant documents, but simply uses “does not contain the answer” as a proxy for irrelevant context.\\nReducing Hallucinations. There have also been efforts to improve RAG factuality on open-book QA\\ntasks (Asai et al., 2023; Mineiro, 2024; Simhi et al., 2024; Wang et al., 2024b; Zhang et al., 2024b).\\nThe main theme is to improve both the generation and retrieval quality, often by fine-tuning one or\\nmore components. Also, since RAG leads to very long contexts, another issue that arises is the “lost\\nin the middle” problem (Hsieh et al., 2024; Liu et al., 2024; Yu et al., 2024). These works start with\\nthe premise that the provided query/context should be precisely answerable by the LLM, and hence,\\nonly analyze their findings in the sufficient context scenario. Independent of RAG, many papers\\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='only analyze their findings in the sufficient context scenario. Independent of RAG, many papers\\nhave studied interventions and tools for calibrating LLM confidence in their responses (Chuang\\net al., 2024; Kadavath et al., 2022; Yin et al., 2023; Zhang et al., 2024a) and performance across\\ndisaggregated subsets of data (Paes et al., 2022; Joren et al., 2023).\\n3 S UFFICIENT CONTEXT\\nAt a high level, our aim is to classify input instances based on whether the context contains enough\\ninformation to answer the query. We split possible contexts into two cases: (1) Sufficient Context.\\nThe context is sufficient to answer the query if it contains all the necessary information to provide a\\ndefinitive answer. (2) Insufficient Context. Otherwise, a context is insufficient. A context may also\\nbe insufficient if the query requires specialized knowledge that is not provided in the context or if\\nthe information in the context is incomplete, inconclusive, or contradictory. In this section, we more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 2, 'page_label': '3', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the information in the context is incomplete, inconclusive, or contradictory. In this section, we more\\nthoroughly discuss sufficient context. Then, we show how to accurately and scalably label instances.\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n3.1 D EFINITION OF SUFFICIENT CONTEXT\\nWe first set some notation for a generic open-domain question-answering setting following Trivedi\\net al. (2020). Consider a dataset D with instances of the form q = (Q, C; A), where Q is the query\\nand C is the context that consists of a set of facts. At inference time, we also consider instances\\nq′ = (Q, C) without the ground truth answer, where the goal is to predict an answer A′ from Q, C,\\nand the model’s parametric knowledge. To measure correctness, we compareA′ and A, where there\\nare many options such as exact match, F1 score, or an LLM-based assessment of answer sameness\\n(we use an LLM). Using this notation, we can now define our notion of sufficient context.\\nDefinition (Sufficient Context). An instance q′ = (Q, C) has sufficient context if and only if there\\nexists an answer A′ such that A′ is a plausible answer to the question Q given the information in C.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='exists an answer A′ such that A′ is a plausible answer to the question Q given the information in C.\\nTo understand this, we can build on the Attributable to Identified Sources (AIS) framework (Rashkin\\net al., 2023). Entailment via AIS answers a slightly different question. Namely, given an instance\\nq = ( Q, C; A), the entailment objective is to determine the truth value of the proposition: The\\nanswer to the question Q is A given the information in C. The key difference between entailment\\nand sufficient context is that for sufficient context we do not presuppose that we have the answerA′\\nin advance, only that such an answer exists. Finally, we only consider “plausible” answers, where\\nwe mean that A′ could be an answer to the question Q. For example, if the question asks about a\\nperson’s birthplace, thenA′ should be a location. We note that this allows for the possibility that the\\ncontext contains an incorrect answer to the question. This is a key requirement, because (i) we would'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='context contains an incorrect answer to the question. This is a key requirement, because (i) we would\\nlike to be able to use signal from sufficient context at inference time, where we do not have ground\\ntruth answers (see Section 5.1) and (ii) we hope to elucidate findings that are robust to ground truth\\nlabel noise.\\nRemark 1 (Multi-hop queries). In most benchmark dataset (e.g., Musique, HotPotQA), models are\\nexpected to be able to do multi-hop reasoning up to four hops, in which they must combining facts to\\nform the answer. However, they should not infer connections that are not in the context. For example,\\nif “Bob’s mother was born in New York” then this does not suffice to say Bob was born in New York.\\nBut, if the context also says “Bob’s mother is Alice...” and “... all of Alice’s children were born in\\nNew York” then this instance has sufficient context.\\nRemark 2 (Ambiguous queries). If the query is ambiguous, then the context is sufficient if and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='New York” then this instance has sufficient context.\\nRemark 2 (Ambiguous queries). If the query is ambiguous, then the context is sufficient if and\\nonly if (i) the context can disambiguate the query and (ii) the context provides an answer to the\\ndisambiguated query. For example, the question could be “What sport does Mia play?” and the\\ncontext could contain both “Mia, from New York, plays basketball. . . ” and “... Mia, from California,\\nplays volleyball.” This is sufficient because if the query is referring to either Mia from New York or\\nBob from California, then the context can answer the question.\\nRemark 3 (Ambiguous contexts). Assume the context contains multiple plausible answers to the\\nquery. Then it is sufficient if and only if it also provides enough information to distinguish between\\nqueries that would lead to each answer. For example, if the question is “What country does Ali live'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='queries that would lead to each answer. For example, if the question is “What country does Ali live\\nin?” and the context is “Ali lives in Paris” then this instance does not have sufficient context because\\nit is not clear if Ali lives in Paris, France or Paris, Texas, USA. If the context further contains “This\\nweekend, Ali took the train from Paris to Marseille.” Then this becomes sufficient because it is almost\\ncertain that Ali lives in France as one cannot take a train from Texas to France.\\n3.2 S UFFICIENT CONTEXT AUTO RATER\\nNext, we consider automating the task of labeling whether instances have sufficient context or not.\\nWe investigate two questions: (1) Can today’s models achieve high accuracy on a challenging,\\nhuman-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 3, 'page_label': '4', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='human-annotated dataset? (2) How does an entailment model compare to general-purpose LLMs? To\\nanswer these questions, we evaluate methods on human-labeled data. Table 1 shows that Gemini 1.5\\nPro can serve as an accurate autorater to label instances in terms of sufficient context. It achieves\\n93% accuracy, outperforms other methods, and operates without needing a ground truth answer.\\nSufficient Context Labeled Dataset. Using the above definition, we construct gold labels for\\neach (query, context) pair. We did not use ground truth answers or model responses. For\\nthe instances, we sample a total of 115 instances (queries, contexts, and answers) from standard\\nbenchmarks (PopQA, FreshQA, Natural Questions, EntityQuestions). We design the dataset to\\nbe very challenging, including single- and multi-hop questions, as well as adding highly related\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 1: Sufficient Context AutoRater.Evaluating model ability to classify sufficient context on a\\ngold-labeled dataset of 115 (query, context; answer) instances. Gemini 1.5 Pro (1-shot)\\nperforms the best, while FLAMe can be a cheaper alternative. TRUE-NLI and Contains GT need\\nground truth (GT) answers, while others only use (query, context) . Best in column in bold.\\nMetrics: F1 Score Accuracy Precision Recall No GT Answer\\nMethods\\nGemini 1.5 Pro (1-shot) 0.935 0.930 0.935 0.935 ✓\\nGemini 1.5 Pro (0-shot) 0.878 0.870 0.885 0.871 ✓\\nFLAMe (fine-tune PaLM 24B) 0.892 0.878 0.853 0.935 ✓\\nTRUE-NLI (fine-tune T5 11B) 0.818 0.826 0.938 0.726\\nContains GT 0.810 0.809 0.870 0.758\\ninformation in the context even if it is not sufficient (e.g., a named entity from the question often\\nappears in the context). We evaluate methods’ abilities to classify sufficient context (binary labels).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='appears in the context). We evaluate methods’ abilities to classify sufficient context (binary labels).\\nMethods: Operating on Query-Context pairs. We use Gemini 1.5 Pro with either instructions\\n(0-shot) or both instructions and a 1-shot example, held out from our dataset. FLAMe 24B is a\\ngeneral autorater model (Vu et al., 2024), but it has a small context window. For FLAMe, we divide\\nthe contexts into 1600 token chunks and ask whether each chunk is sufficient. If any chunk is labeled\\nsufficient, we consider the instance to have sufficient context; otherwise, it’s labeled as insufficient.\\nWe design prompts (in Appendix C) for both models based on the sufficient context definition above.\\nMethods: When a Ground Truth (GT) Answer is Available. For two baselines (TRUE-NLI,\\nContains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Contains GT), we use answers as an additional input to classify sufficient context. TRUE-NLI is a\\nfine-tuned entailment model (Honovich et al., 2022) that checks if the context entails one of the GT\\nanswers. Contains GT checks if a GT answer appears in the context. Comparing to entailment is\\nparticularly interesting because if a given answer A is entailed by (Q, C), then the context is also\\nsufficient. On the other hand, the reverse is not true, since the answer A is only one possible choice\\nfor A′. As one consequence, if the ground truth answer A is incorrect, then it may not be entailed by\\nthe context. This happens when a named entity is ambiguous (e.g., two people with the same name),\\nand the GT answer is based on one of the people while the context describes the other.\\nResults. Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Results. Table 1 shows that Gemini 1.5 Pro (1-shot) performs the best overall in terms of F1 score\\nand accuracy. As expected, TRUE-NLI has higher precision and lower recall: it measures entailment,\\nwhich implies sufficient context. FLAMe outperforms TRUE-NLI in F1 and accuracy, but lags behind\\nGemini (1-shot), likely because it is a smaller model. The Contains GT method works surprisingly\\nwell, indicating that the presence of a ground truth answer correlates with context sufficiency.\\nDiscussion. In real-world scenarios, we cannot expect candidate answers when evaluating model\\nperformance. Hence, it is desirable to use a method that works using only the query and context.\\nAmong these methods, Gemini 1.5 Pro (1-shot) has high accuracy and balanced precision and recall.\\nTherefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Therefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\\nsufficient context is a different (and more complex) task than determining entailment.\\n4 A N EW LENS ON RAG P ERFORMANCE\\nWe set out to understand RAG performance by looking at sufficient context. We first analyze datasets\\n(Section 4.1), then we investigate model performance with/without sufficient context (Section 4.2). We\\nqualitatively discuss cases where insufficient context leads to correct model responses (Section 4.3).\\n4.1 D O BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT ?\\nWe introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 4, 'page_label': '5', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='We introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we\\ndo not aim to optimize the retrieval methods (which could increase the sufficient context percentage).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreshQA HotpotQA Musique\\n0\\n20\\n40\\n60\\n80\\n100% of Dataset with Sufficient Context\\n77.4\\n46.2 44.6\\n77.4\\n46.2 44.6\\n63.7\\n45.4\\n33.4\\nMeasuring Sufficiency by Context Length\\n10000 T okens\\n6000 T okens\\n2000 T okens\\nFigure 2: We compare the % of instances that our autorater labels as sufficient across datasets,\\neither with the first 10k, 6k, or 2k tokens of the provided sources. FreshQA has hand-curated URLs\\nthat support the answers and exhibits high sufficient context. HotPotQA and Musique have lower\\nsufficient context (and even lower with 2000 tokens). We use 6000 token contexts in the remainder.\\nThis is not the focus of our work, as we wish to understand how models perform with or without\\nsufficient context. Having a mix of both is inevitable in generic RAG systems.\\nDatasets. We consider FreshQA, Musique-Ans, and HotpotQA as a representative spread of open'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='sufficient context. Having a mix of both is inevitable in generic RAG systems.\\nDatasets. We consider FreshQA, Musique-Ans, and HotpotQA as a representative spread of open\\nbook QA datasets. FreshQA (Vu et al., 2023) evaluates time-sensitive information and has up-to-date\\nURLs that should support an answer to the queries, which we use to construct the context (see\\nAppendix A.3 for details on the retrieval). We use the ‘True Premise’ setting (452 instances), skipping\\n‘False Premise’ questions that mislead by design. Musique-Ans (Trivedi et al., 2022) is a multi-hop\\nQA benchmark, created by composing two to four single-hop interconnected questions. Here, ‘Ans’\\nis the standard ‘answerable’ subset. Musique instances have 20 supporting text snippets as sources,\\nwhich we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='which we use as the context. HotPotQA (Yang et al., 2018) is a Wikipedia-based QA dataset, with\\nsingle- and multi-hop questions. The corpus is a large set of snippets; we retrieve the top 5 as the\\ncontext (via REPLUG (Shi et al., 2023b) from FlashRAG (Jin et al., 2024)). We randomly sample\\n500 instances from the development sets of Musique-Ans and HotPotQA for evaluation.\\nSufficient Context % of Datasets. Figure 2 shows the fraction of instances that our autorater\\nclassifies as having sufficient context. We explore three context lengths, ranging from a maximum\\nof 2000 to maximum of 10000 tokens. The motivation behind this is to assess if there is a large\\nchange in sufficient context if we were to simply truncate the retrieval (e.g., for models that have\\nsmall context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='small context windows). In general, we see a modest difference from 2000 to 6000 token limit, but\\neffectively none from 6000 to 10000 tokens. FreshQA has the highest sufficient context percentage,\\nwhich makes sense as the context comes from oracle supporting documents. The lower sufficient\\ncontext in Musique is perhaps surprising, given that the retrieval is fixed as part of the dataset. From\\nthe results in Figure 2, we truncate at 6000 tokens for all three datasets in the remainder of the paper.\\n4.2 I NITIAL FINDINGS BASED ON SUFFICIENT CONTEXT\\nIn general, the ideal behavior for a language generation model is to answer questions correctly when\\npossible and to otherwise abstain. RAG seeks to move models towards this desired behavior, such\\nthat the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\\nanalyze several cases to assess how far we are from this ideal trade-off.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='that the provided context shifts hallucinations to correct answers, or to abstentions if needed. We\\nanalyze several cases to assess how far we are from this ideal trade-off.\\nExperimental Set-up and LLMEval. We employed a basic chain of thought (CoT) prompting\\napproach, with the prompt structure and further information detailed in Appendix C.4. We then\\nprocessed the outputted answers to identify matches between the response and any of the ground truth\\nanswers. Responses where a clear correct match could not be determined were processed through\\nthe LLMEval pipeline using a zero-shot approach, with the prompt based on Krishna et al. (2024)\\n(see Appendix C.3). Then, for each example, we can rate it as “correct” or “abstain” or “hallucinate”\\ndepending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 5, 'page_label': '6', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='depending on the LLMEval output. We use an LLM for evaluation instead of checking for an exact\\nmatch because it is more robust to syntactic variations. See Appendix B.3 for details and examples.\\nModels Abstain Less with RAG. While overall performance improves with RAG, the introduction\\nof additional context paradoxically reduces the model’s ability to abstain from answering when\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCorrect Abstain Halluc.\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nGemini 1.5 Pro\\nCorrect Abstain Halluc.\\nGPT 4o\\nCorrect Abstain Halluc.\\nClaude 3.5 Sonnet\\nCorrect Abstain Halluc.\\nGemma 27B\\nCategorizing RAG Responses: Sufficient vs Insufficient Context\\nSufficient ContextInsufficient Context\\nFreshQA Musique HotpotQA\\nFigure 3: Model Performance on Datasets Stratified by Sufficient Context. Given sufficient\\ncontext, models have a higher correct percentage on these challenging datasets. Performance drops,\\nbut the models are still able to answer a large portion of questions correct without sufficient context.\\nOne prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='One prevailing issue is that all models hallucinate rather than abstain in many cases with insufficient\\ncontext. The smallest model Gemma 27B struggles to avoid hallucinations given insufficient context.\\nappropriate. Without RAG, Claude 3.5 Sonnet abstains on 84.1% questions, while with RAG, the\\nfraction of abstentions drops to 52%. Similarly, GPT 4o’s abstention fraction moves from 34.4%\\nto 31.2% and Gemini 1.5 Pro’s drops from 100% to 18.6%. This phenomenon may arise from the\\nmodel’s increased confidence in the presence of any contextual information, leading to a higher\\npropensity for hallucination rather than abstention.\\nModels Hallucinate with Both Sufficient and Insufficient Context. Considering Figure 3, models\\ngenerally achieve higher accuracy with sufficient context (highergreen bars, top row) than without\\nsufficient context (lower green bars, bottom row). However, looking at each row separately, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='generally achieve higher accuracy with sufficient context (highergreen bars, top row) than without\\nsufficient context (lower green bars, bottom row). However, looking at each row separately, we\\ndiscover several findings. First, in the sufficient context case (top row), we see that models hallucinate\\nmore than they abstain ( red bars are higher than blue bars, usually). The trend holds across all\\nthree datasets. Moving to insufficient context (bottom row), we find a different distribution of model\\nresponses, with more abstentions and hallucinations. This tendency varies notably across different\\nmodels. For instance, Claude abstains more (higher blue bars) with insufficient context, but answers\\nfewer questions correctly (lower green bars) than Gemini and GPT. These differences underscore\\nthe potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\\nmuch more hallucinations (higher red bars) than the other models, except for HotPotQA, where we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the potential for improvement in both retrieval and reasoning capabilities. Overall, Gemma has\\nmuch more hallucinations (higher red bars) than the other models, except for HotPotQA, where we\\nattribute the higher accuracy to the smaller retrieved contexts.\\n4.3 Q UALITATIVELY ANALYZING RESPONSES WITH INSUFFICIENT CONTEXT\\nOne curious observation in our analysis is the ability of models to sometimes provide correct answers\\neven when presented with insufficient context. For example, from Figure 3, all three models are able\\nto correctly answer upwards of 35% of instances with insufficient context on HotpotQA. A natural\\nassumption is that the models already know the answer from pre-training, and they can generate a\\ncorrect response from parametric memory. However, this only explains part of the story.\\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 6, 'page_label': '7', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='correct response from parametric memory. However, this only explains part of the story.\\nLooking deeper, we provide a qualitative categorization in Table 2 of instance types where our\\nautorater labels an instance as insufficient context, while the LLM evaluator marks the model answer\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nInstance type Why model may be correct Example\\nYes/No question 50% chance of correct Q: Is there a total eclipse in the United States this year?\\nLimited choice Some chance of correct Q: Which band has more members, Chvrches or\\nGoodbye Mr. Mackenzie?\\nMulti-hop: fragment Use parametric inference\\nQ: Who did the original voice for the character\\nwhose series Mickey’s Safari in Letterland is from?\\nContext says Mickey’s Safari is a video game\\nand Walt Disney voices Mickey Mouse in cartoons.\\nMust infer the game is in the Mickey Mouse series.\\nMulti-hop: partial Use parametric knowledge\\nQ: Claudine’s Return starred the actress who played\\nwhich role on “Married...with Children”?\\nContext lists actresses but not their roles in\\n“Married...with Children”. Must know extra facts.\\nToo many hops Execute complex reasoning\\nQ: How many cyclists have won all three of women’s\\ncycling Grand Tours equivalents in the same year?'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='“Married...with Children”. Must know extra facts.\\nToo many hops Execute complex reasoning\\nQ: How many cyclists have won all three of women’s\\ncycling Grand Tours equivalents in the same year?\\nContext requires cross-referencing lists of events\\nand lists of winners while tracking winners by year .\\nAmbiguous query Guess right interpretation\\nQ: Who is the spouse of a cast member from King\\nof the Mountain?\\nContext has many cast members and query/context do\\nnot specify which spouse to answer about.\\nRater error Mislabel insuff. or correct —\\nClosed-book correct Known from pre-training —\\nTable 2: Qualitative Analysis of Correct Answer & Insufficient Context. Examining model\\nresponses across datasets, we identify common cases where the model generates a correct answer\\neven though our autorater labels the instance as insufficient. We categorize such instances into eight\\ntypes, as well as provide examples. Given that models are also correct on many questions in the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='even though our autorater labels the instance as insufficient. We categorize such instances into eight\\ntypes, as well as provide examples. Given that models are also correct on many questions in the\\nclosed-book setting, we believe this mostly explains the 35–62% correct rate with insufficient context.\\nas correct. For example, one type accounts for when the provided context is not sufficient to answer the\\nquery, but it bridges gaps in the model’s knowledge. Another type is when the retrieved information\\nclarifies ambiguities inherent in the question (without answering the question). Finally, we also have\\nthe times where either the autorater or the evaluator model makes an error. We note that our analysis\\nexpands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\\nis correct with RAG (but not without) even though the context does not contain the answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='expands on prior work by Yoran et al. (2024), who also find a large fraction of cases where the model\\nis correct with RAG (but not without) even though the context does not contain the answer.\\nWe additionally investigated cases where the autorater labels an instance as having sufficient context\\nwhile the LLM evaluator marks the answer as incorrect. One source of these discrepancies occurs\\nwhen the ground truth answer conflicts with the answer provided in the source. This represents a key\\ndifference from methods that measure entailment, where context is evaluated relative to a specific\\nground truth answer (see Table 1). Another source of errors arises when the autorater correctly\\nidentifies that the necessary information is present, but the model fails to properly compose the\\ninformation (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\\ncases, however, determining the source of the error proves challenging.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='information (e.g., in multihop questions or questions requiring arithmetic). In a substantial number of\\ncases, however, determining the source of the error proves challenging.\\n5 T ECHNIQUES TO REDUCE HALLUCINATIONS WITH RAG\\nFrom our previous analysis, we have seen that models may hallucinate rather than abstain and that\\nthis happens more with RAG than in a closed-book setting. A natural next question is whether we\\ncan prompt or fine-tune a model to perform closer to the ideal case. Can we steer the model to either\\noutput the correct answer or abstain, while hallucinating an incorrect answer as little as possible?\\n5.1 S ELECTIVE RAG U SING SUFFICIENT CONTEXT SIGNAL\\nOne simple solution to improving RAG performance would be to use the sufficient context autorater\\nto abstain given insufficient context. However, this heavy-handed approach can lower overall\\nperformance, since all models answer some questions correctly even with insufficient context, as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 7, 'page_label': '8', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='to abstain given insufficient context. However, this heavy-handed approach can lower overall\\nperformance, since all models answer some questions correctly even with insufficient context, as\\ndescribed in Table 2 and demonstrated in Figure 3. Instead, we propose a method for combining\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nthe sufficient context autorater outputs with model self-rated confidence scores to tune a selective\\naccuracy-coverage trade-off, where “coverage” denotes the portion of inputs on which the model does\\nnot abstain. Specifically, we use these signals to train a simple linear model to predict hallucinations,\\nand then use it to set coverage-accuracy trade-off thresholds.\\nThis mechanism differs from other strategies for improving abstention in two key ways. First, because\\nit operates independently from generation, it mitigates unintended downstream effects, whereas\\nstrategies like fine-tuning to improve abstention can inadvertently worsen performance on certain\\ninputs (see Section 5.2). Second, it offers a controllable mechanism for tuning abstention, which\\nallows for different operating settings in differing applications, such as strict accuracy compliance in\\nmedical domains or maximal coverage on creative generation tasks.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='allows for different operating settings in differing applications, such as strict accuracy compliance in\\nmedical domains or maximal coverage on creative generation tasks.\\nAbstention Signals We utilize two main signals for abstention: the self-rated probabilities as\\nin Li et al. (2024); Kadavath et al. (2022) and the sufficient context autorater. For the self-rated\\nprobabilities, we use two strategies: P(True) and P(Correct). P(True) requires sampling answers\\nfrom the model multiple times, and then prompting the model multiple times to label each model as\\ncorrect or incorrect, resulting in a final probability of correctness associated with each question as in\\nKadavath et al. (2022). For proprietary models, where extensive querying is prohibitively expensive,\\nwe use P(Correct) instead. We adapt the probability-generating prompt from Li et al. (2024) to obtain\\nthe model’s response and its estimated probability of correctness. For the sufficient context signal,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='the model’s response and its estimated probability of correctness. For the sufficient context signal,\\nwe use the binary label from an autorater. Our hypothesis is that combining these signals should lead\\nto more effective abstention, particularly in cases where the context is insufficient.\\nMethods. We calculate P(True) by sampling 20 responses for each question and querying the model\\n5 times to evaluate whether the answer is correct or incorrect (without using the ground truth) as\\nin Kadavath et al. (2022). For P(Correct), the prompt requests the most likely and second most\\nlikely answers along with their probabilities. We use string matching to extract the response and\\nself-predicted probability, keeping the one with the highest probability. To determine sufficient\\ncontext, we use FLAMe, a small and efficient model for determining the sufficient context label. We\\ndivide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='divide the retrievals into chunks of 1600 tokens to fit in the context window and label the context as\\nsufficient if any of these chunks are sufficient.\\nWe combine the binary sufficient context label with the self-rated answer probability (P(True) for\\nopen-source models or P(Correct) for proprietary models) in a simple logistic regression model to\\npredict hallucinations with 100 iterations of random hyperparameter search. At inference time, we\\nuse the logistic regression model scores to threshold the outputs, abstaining when the score is below\\na chosen threshold as in Joren et al. (2024). We measure the added value for selective accuracy\\nof the sufficient context signal (purple line in Figure 4) by comparing it with the model self-rated\\nconfidence alone (gray line).\\nResults. We find that our approach leads to a better selective accuracy-coverage trade-off compared\\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Results. We find that our approach leads to a better selective accuracy-coverage trade-off compared\\nto using model confidence alone. In particular, see gains of over 10% for Gemma 27B on HotpotQA\\nin the highest accuracy regions, and gains of over 5% for Gemini 1.5 Pro on the same dataset near the\\n70% coverage region. These gains are less pronounced on datasets with lower overall accuracy, such\\nas when using Gemma 27B on Musique. In this scenario, the low overall performance (18.4%) likely\\nmeans that most of the predictive gains are seen by using the self-rated confidence to predict errors\\nfor the majority of samples. As a result, there is no added benefit from the sufficient context signal.\\nDiscussion. As expected, we see a downward trend in which higher coverage leads to lower selective\\naccuracy for both methods. We conclude that the selective generation mechanism with sufficient\\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 8, 'page_label': '9', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='accuracy for both methods. We conclude that the selective generation mechanism with sufficient\\ncontext has an added benefit for accuracy-coverage trade-offs compared to self-rated confidence\\nalone. As a prerequisite for our method, models should have a non-trivial accuracy on sufficient and\\ninsufficient context instances. Then, intuitively, we can prioritize answering questions the model is\\nlikely to get right before those the model struggles with. While ordering examples is impossible in\\nreal settings, we can estimate a coverage level and use a threshold to choose when to answer.\\n5.2 F INE -TUNING\\nWe also consider fine-tuning models to increase their ability to abstain instead of outputting an\\nincorrect answer. To do so, we train the models with some examples that contain “I don’t know”\\ninstead of their original ground truth answer. The intuition here is that training explicitly on such\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - HotpotQA\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n Gemma 27B - HotpotQA\\nP(True)\\nP(True) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100Selective Accuracy (%)\\nGemini 1.5 Pro - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n60\\n70\\n80\\n90\\n100\\n GPT 4o - Musique\\nP(Correct)\\nP(Correct) + Suff. Context\\n0 20 40 60 80 100\\nCoverage (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n Gemma 27B - Musique\\nP(True)\\nP(True) + Suff. Context\\nFigure 4: Selective Generation: Coverage vs. Selective Accuracy. For selective generation, we\\nuse a linear combination of sufficient context and self-rated confidence (purple) or confidence alone'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Figure 4: Selective Generation: Coverage vs. Selective Accuracy. For selective generation, we\\nuse a linear combination of sufficient context and self-rated confidence (purple) or confidence alone\\n(gray). The x-axis shows coverage (% of questions answered); the y-axis shows accuracy at each\\ncoverage (# correct / # answered). The combined approach matches or outperforms the baseline\\nconfidence-only method, especially on HotpotQA, where our method improves accuracy for most\\ncoverages. For Gemma 27B on Musique, the methods are identical (coeff. for stuff. context is 0).\\ninputs could encourage the model to abstain instead of hallucinating. We also consider multiple\\nsettings, such as only changing the answers when the example has insufficient context. We present\\nfull details in Appendix B.1. The main takeaways are that fine-tuned models (i) have a higher rate of\\ncorrect answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='correct answers in many cases, but (ii) still hallucinate quite often and more than they abstain. Overall,\\nit is likely possible to use fine-tuning to steer the model towards better abstention and correctness, but\\nmore work is needed to determine develop a reliable strategy that can balance these objectives.\\n6 C ONCLUSION\\nOur work provided a new lens on LLM responses in RAG systems centered around our notion of\\nsufficient context. We constructed a sufficient context autorater, which enabled scalable insights into\\nmodel performance on different types of instances. Our analysis revealed that even with sufficient\\ncontext, LLMs frequently hallucinate answers. We also found, surprisingly, many cases where\\na model will output a correct answer with access to only insufficient context. Qualitatively, we\\ncategorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='categorized such instances, leading to a fuller picture of ways context can be useful. Finally, we\\ndemonstrated a general-purpose selective generation method, which applies to Gemini, GPT, and\\nGemma, and can reduce hallucinations by 2–10% on queries that the model answers.\\nLimitations. Our analysis focuses on QA datasets, but summarization tasks also utilize context,\\nwhich may or may not be sufficient. For example, models may behave differently on the prompt\\n“Summarize the reviews of 5-star hotels in Mallorca” depending on whether the context mentions the\\nhotel reviews, whether they are for 5-star hotels, etc. Another shortcoming is an exploration of how\\noften different retrieval methods lead to sufficient context. Also to achieve the best performance, we\\ncould have used our autorater to iteratively judge whether to retrieve more or answer the question.\\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 9, 'page_label': '10', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='could have used our autorater to iteratively judge whether to retrieve more or answer the question.\\nFuture Work. One direction is a fine-grained sufficient context autorater, which outputs a score\\ninstead of a binary label. This could be useful for ranking contexts after the retrieval step. Another\\ndirection is to extend the definition of sufficient context to multi-modal RAG settings, such as for\\nvisual QA (images) or document QA (pdf files). Finally, our selective generation results suggest that\\nthere is room for improvement in reducing hallucinations by using auxiliary signals from the inputs.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nACKNOWLEDGMENTS\\nWe thank Hrishikesh Garud, Vikram Gopali, Xun Sun, and Bruce Wang for annotating data. We\\nthank Ranjay Krishna and Jacob Eisenstein for helpful discussions. We also thank Alyshia Olsen for\\nhelp with the figure design and color palette. We thank the anonymous reviewers for suggestions to\\nimprove the presentation.\\nREFERENCES\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint\\narXiv:2303.08774, 2023.\\nAnthropic. Claude 3.5 sonnet model card addendum, 2024.\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve,\\ngenerate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023.\\nAkari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, and Wen-tau\\nYih. Reliable, adaptable, and attributable language models with retrieval. arXiv preprint arXiv:2403.03187,\\n2024.\\nYung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, and James Glass. Lookback lens:\\nDetecting and mitigating contextual hallucinations in large language models using only attention maps. arXiv\\npreprint arXiv:2407.07071, 2024.\\nFlorin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone Filice, Cesare Campagnano, Yoelle Maarek,\\nNicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems. In\\nProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information\\nRetrieval, pp. 719–729, 2024.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Retrieval, pp. 719–729, 2024.\\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A\\nsurvey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th\\nACM SIGKDD Conference on Knowledge Discovery and Data Mining , pp. 6491–6501, 2024.\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\\narXiv preprint arXiv:2312.11805, 2023.\\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\\nLéonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open\\nlanguage models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='language models at a practical size. arXiv preprint arXiv:2408.00118, 2024.\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas\\nScialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. True: Re-evaluating factual consistency\\nevaluation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, pp. 3905–3920, 2022.\\nCheng-Yu Hsieh, Yung-Sung Chuang, Chun-Liang Li, Zifeng Wang, Long Le, Abhishek Kumar, James Glass,\\nAlexander Ratner, Chen-Yu Lee, Ranjay Krishna, and Tomas Pfister. Found in the middle: Calibrating\\npositional attention bias improves long context utilization. In Lun-Wei Ku, Andre Martins, and Vivek\\nSrikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024 , pp. 14982–14995,\\nBangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 10, 'page_label': '11', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi:\\n10.18653/v1/2024.findings-acl.890. URL https://aclanthology.org/2024.findings-acl.\\n890.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\\nChen. Lora: Low-rank adaptation of large language models. In International Conference on Learning\\nRepresentations (ICLR), 2022.\\nAQ Jiang, A Sablayrolles, A Mensch, C Bamford, DS Chaplot, D de las Casas, F Bressand, G Lengyel, G Lample,\\nL Saulnier, et al. Mistral 7b (2023). arXiv preprint arXiv:2310.06825, 2023.\\nZhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. Retrieve, summarize, plan: Advancing multi-hop\\nquestion answering with an iterative approach. arXiv preprint arXiv:2407.13101, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for\\nefficient retrieval-augmented generation research. CoRR, abs/2405.13576, 2024. URL https://arxiv.\\norg/abs/2405.13576.\\nHailey Joren, Chirag Nagpal, Katherine A Heller, and Berk Ustun. Participatory person-\\nalization in classification. In Advances in Neural Information Processing Systems , vol-\\nume 36, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\\nfile/2dbb8bfe4cd3875609b23799830ee865-Paper-Conference.pdf.\\nHailey Joren, Charles Marx, and Berk Ustun. Classification with conceptual safeguards. In The Twelfth Inter-\\nnational Conference on Learning Representations (ICLR) , 2024. URL https://iclr.cc/virtual/\\n2024/poster/17625.\\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,\\nZac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they\\nknow. arXiv preprint arXiv:2207.05221, 2022.\\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay,\\nand Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. arXiv\\npreprint arXiv:2409.12941, 2024.\\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-\\nintensive nlp tasks. Advances in Neural Information Processing Systems , 33:9459–9474, 2020.\\nMoxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua. Think twice before assure:\\nConfidence estimation for large language models through reflection on multiple answers. arXiv preprint\\narXiv:2403.09972, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Confidence estimation for large language models through reflection on multiple answers. arXiv preprint\\narXiv:2403.09972, 2024.\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy\\nLiang. Lost in the middle: How language models use long contexts. Transactions of the Associ-\\nation for Computational Linguistics , 12:157–173, 2024. doi: 10.1162/tacl_a_00638. URL https:\\n//aclanthology.org/2024.tacl-1.9.\\nPaul Mineiro. Online joint fine-tuning of multi-agent flows. arXiv preprint arXiv:2406.04516, 2024.\\nAbhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and\\nHannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. In COLM 2024,\\n2024.\\nCheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Cheng Niu, Yuanhao Wu, Juno Zhu, Siliang Xu, KaShun Shum, Randy Zhong, Juntong Song, and Tong\\nZhang. RAGTruth: A hallucination corpus for developing trustworthy retrieval-augmented language models.\\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting\\nof the Association for Computational Linguistics (V olume 1: Long Papers) , pp. 10862–10878, Bangkok,\\nThailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.585.\\nURL https://aclanthology.org/2024.acl-long.585.\\nLucas Monteiro Paes, Carol Xuan Long, Berk Ustun, and Flavio Calmon. On the epistemic limits of personalized\\nprediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural\\nInformation Processing Systems, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Information Processing Systems, 2022. URL https://openreview.net/forum?id=Snp3iEj7NJ.\\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\\nShoham. In-context retrieval-augmented language models. Transactions of the Association for Computational\\nLinguistics, 11:1316–1331, 2023.\\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov,\\nGaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation\\nmodels. Computational Linguistics, 49(4):777–840, 2023.\\nDavid Rau, Hervé Déjean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, and\\nStéphane Clinchant. Bergen: A benchmarking library for retrieval-augmented generation. arXiv preprint\\narXiv:2407.01102, 2024.\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 11, 'page_label': '12', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='arXiv:2407.01102, 2024.\\nDongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang\\nWang, Shichao Sun, Huanyu Li, et al. Ragchecker: A fine-grained framework for diagnosing retrieval-\\naugmented generation. arXiv preprint arXiv:2408.08067, 2024.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny\\nZhou. Large language models can be easily distracted by irrelevant context. In International Conference on\\nMachine Learning, pp. 31210–31227. PMLR, 2023a.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652,\\n2023b.\\nAdi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Constructing benchmarks and interventions\\nfor combating hallucinations in llms. arXiv preprint arXiv:2404.09971, 2024.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Is multihop qa in dire condition?\\nmeasuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , pp. 8846–8863, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='measuring and reducing disconnected reasoning. In Proceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing (EMNLP) , pp. 8846–8863, 2020.\\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions\\nvia single-hop question composition. Transactions of the Association for Computational Linguistics , 10:\\n539–554, 2022.\\nTu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny\\nZhou, Quoc Le, et al. Freshllms: Refreshing large language models with search engine augmentation. arXiv\\npreprint arXiv:2310.03214, 2023.\\nTu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational\\nautoraters: Taming large language models for better automatic evaluation. arXiv preprint arXiv:2407.10817,\\n2024.\\nYuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='2024.\\nYuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware\\nretrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497 ,\\n2024a.\\nZilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang,\\nAnush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing retrieval augmented generation\\nthrough drafting. arXiv preprint arXiv:2407.08223, 2024b.\\nSiye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do irrelevant inputs\\nskew the responses of large language models? In COLM 2024, 2024.\\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealing\\nthe behavior of large language models in knowledge conflicts. In International Conference on Learning\\nRepresentations (ICLR), 2024.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Representations (ICLR), 2024.\\nShi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. Corrective retrieval augmented generation. arXiv\\npreprint arXiv:2401.15884, 2024.\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-\\npher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings\\nof the 2018 Conference on Empirical Methods in Natural Language Processing . Association for Computa-\\ntional Linguistics, 2018.\\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. Do large language\\nmodels know what they don’t know? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),\\nFindings of the Association for Computational Linguistics: ACL 2023 , pp. 8653–8665, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.551. URL\\nhttps://aclanthology.org/2023.findings-acl.551.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 12, 'page_label': '13', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.551. URL\\nhttps://aclanthology.org/2023.findings-acl.551.\\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models robust\\nto irrelevant context. In International Conference on Learning Representations (ICLR) , 2024.\\nTan Yu, Anbang Xu, and Rama Akkiraju. In defense of rag in the era of long-context language models. arXiv\\npreprint arXiv:2409.01666, 2024.\\nHanning Zhang, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and\\nTong Zhang. R-tuning: Instructing large language models to say ‘i don’t know’. In Proceedings of the\\n2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies (V olume 1: Long Papers), pp. 7106–7132, 2024a.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 13, 'page_label': '14', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nJiahao Zhang, Haiyang Zhang, Dongmei Zhang, Liu Yong, and Shen Huang. End-to-end beam retrieval for\\nmulti-hop question answering. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of\\nthe 2024 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies (V olume 1: Long Papers) , pp. 1718–1731, Mexico City, Mexico, June\\n2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.96. URL https:\\n//aclanthology.org/2024.naacl-long.96.\\nYujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large\\nlanguage models. In Proceedings of the ACM on Web Conference 2024 , pp. 1453–1463, 2024.\\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and\\nreading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774,\\n2021.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nA S UPPORTING EXPERIMENT INFORMATION\\nWe provide sufficient details to reproduce all of our experiments. We list all of the prompts that we\\nuse in the next section.\\nA.1 M ODELS\\nGPT 4o . We use the API-accessible gpt-4o-2024-08-06 model, released on August 6,\\n2024 (Achiam et al., 2023).\\nGemini 1.5 Pro. We use the API-accessible gemini-1.5-pro-0514 model, released on May\\n14, 2024 (Gemini Team et al., 2023).\\nClaude 3.5 Sonnet. We use the only API-accessible claude-3-5-sonnet-20240620 model,\\nreleased on June 20, 2024 (Anthropic, 2024).\\nGemma 2 27B. We use the publicly available instruction tunedgemma-2-27b-it model, released\\non Jun 27, 2024 (Gemma Team et al., 2024).\\nMistral 3 7B. We use the publicly available instruction tuned Mistral-7B-Instruct-v0.3\\nmodel, released on May 22, 2024 (Jiang et al., 2023).\\nFLAMe. We use the published FLAMe-RM-24B model (Vu et al., 2024).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='model, released on May 22, 2024 (Jiang et al., 2023).\\nFLAMe. We use the published FLAMe-RM-24B model (Vu et al., 2024).\\nTRUE-NLI model. Calculated the maximum probability over the chunks in the context. Use a\\nthreshold of 0.05, where if the maximum probability is higher then this, then we classify as ‘sufficient\\ncontext’. The threshold of 0.05 achieved the highest F1 score on our human labeled dataset. We use\\nthe t5_xxl_true_nli_mixture version of their model (Honovich et al., 2022).\\nA.2 F INE -TUNING SETTINGS\\nIn our fine-tuning setup, we employed the LoRA adaptation technique (Hu et al., 2022) to fine-tune\\nMistral-7B-Instruct-v0.31. We used either a 2,000-example random subset sampled from the training\\nset of the Musique-Ans dataset or from the development set of the HotPotQA data 2. The prompt\\ntemplate for finetuning is provided in Appendix D.\\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='template for finetuning is provided in Appendix D.\\nFor the LoRA parameters, we set the rank to 4 and alpha to 8 for all experiments. The models were\\nfine-tuned over 2 epochs with a batch size of 16 and a learning rate of 1 × 10−5. We note that the\\ntraining was not smooth, and different checkpoints led to very different results. To be systematic,\\nwe chose the best checkpoint in terms of Correct % after either 1 or 2 epochs (where for Musique it\\nturned out to be after 1 epoch, and for HotPotQA we found that 2 epochs was better).\\nA.3 D ATASETS\\nWe sample 500 examples from HotPotQA and Musique-Ans dev sets, following prior work. We use\\nall ‘True Premise’ questions from FreshQA.\\nRetrieval for HotpotQA. We adopt the FlashRAG framework (Jin et al., 2024) to implement our\\nRetrieval-Augmented Generation (RAG) process. Our retrieval corpus is based on the wiki-18 dataset,\\nutilizing ‘intfloat/e5-base-v2‘ from Hugging Face’s model hub as a Dense Retriever3. For each query,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 14, 'page_label': '15', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='utilizing ‘intfloat/e5-base-v2‘ from Hugging Face’s model hub as a Dense Retriever3. For each query,\\nwe retrieved the top 5 documents, which are subsequently concatenated with the query and placed\\nwithin a prompt template for inference.\\nTo explore advanced retrieval techniques, we also evaluated the REPLUG (Shi et al., 2023b) method.\\nREPLUG enhances the generation quality by prepending each retrieved document individually to the\\ninput context and ensembling output probabilities across different passes. The REPLUG method is\\nalso implemented based on the FlashRAG framework (Jin et al., 2024).\\n1Available at huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\\n2We use dev set for HotPotQA since the training set had a much different distribution of sufficient context\\nexamples. Namely, we found the train set to be over 88% sufficient context, while the dev set was only 44%.\\n3huggingface.co/intfloat/e5-base-v2\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 15, 'page_label': '16', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nRetrieval for FreshQA We use the urls provided in the FreshQA dataset as retrieval for the\\ncontext. We scraped each url and discarded extra HTML content such as headers, footers, and\\nnavigation. We include the title of each webpage in the text, convert any included tables to markdown,\\nand include the table title immediately before the table in the text. When splitting the tables and text\\nfor smaller context windows, we keep tables and sentences intact when possible. For large tables that\\nrequire splitting, we duplicate the table row column headers to include them in each chunk.\\n16'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 3: Fine-tuned (FT) Mistral 3 7B Instruct. We compare closed book and vanilla RAG with\\nthree FT settings, measuring % Correct ( %C), % Abstain (%A), and % Hallucinate ( %H). Also,\\n“idk” means we change the answer in training samples to be “I don’t know” instead of the given\\nanswer (either for 20% of random examples, or 20% of examples with insufficient context). Best\\n%C for each model/dataset in bold.\\nMusique HotPotQA\\nModel Variant RAG %C %A %H %C %A %H\\nMistral Closed Book 6.6 29.8 63.6 32 7.6 60.4\\n\" Vanilla RAG ✓ 28.8 11.8 59.4 46.6 9.2 44.2\\n\" FT GT answer (Data Mix 1) ✓ 31.4 0 68.6 43.4 0 56.6\\n\" FT idk 20% rand. (Data Mix 2) ✓ 23 1.2 75.8 41.6 0.8 57.6\\n\" FT idk 20% insuff. (Data Mix 3) ✓ 23 2.2 74.8 41.2 2 56.8\\nB A DDITIONAL RESULTS\\nB.1 F INE -TUNING FULL RESULTS\\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='B A DDITIONAL RESULTS\\nB.1 F INE -TUNING FULL RESULTS\\nOne aspect of our selective generation framework is that we use FLAMe, a 24B model, to provide\\nsufficient context labels. However, we would incur significant overhead if we used a 24B model\\nto improve the generation of a much smaller LLM. Instead, we try directly fine-tuning Mistral 3\\n7B to increase accuracy with retrieval. Specifically, we experiment with different data mixtures to\\nencourage the model to output “I don’t know” instead of generating an incorrect response.\\nFine-tuning Data. We repeat the following process separately for each of the Musique-Ans and\\nHotPotQA datasets to create three mixtures of training data with different answers for each dataset.\\nFirst, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='First, we sample 2000 instances. Then, for Data Mix 1, we fine-tune on these instances and keep their\\ngiven ground truth answer. For Data Mix 2, we choose 400 examples (20%) at random and change\\nthe answer to “I don’t know” before fine-tuning. For Data Mix 3, we instead randomly choose 400\\nexamples (20%) that our autorater labels as insufficient context and change their answer to “I don’t\\nknow” while keeping the other answers as the ground truth. Our hypothesis is that fine-tuning on\\nData Mix 2 and 3 should steer the model to abstain more and hallucinate less than with Data Mix 1.\\nModels, Methods, Metrics. Using the three data mixtures described above, we fine-tune the Mistral\\n3 7B Instruct model model with LoRA (details in Appendix A.2). At inference time, we use the\\nstandard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='standard RAG setup where we add context to the prompt. As baselines, we also evaluate the model\\nwithout fine-tuning in both the closed-book setting (w/o RAG) and the open-book setting (Vanilla\\nRAG). Consistent with the prior experiments, we use an LLM with ground truth answers to classify\\nresponses as Correct (%C), Abstention (%A), or Hallucination (%H).\\nFine-tuning Results and Discussion. Table 3 shows our experimental results. We verify that the FT\\nvariants have a higher rate of generating correct answers (%C) compared to closed-book and Vanilla\\nRAG for Musique but not for HotPotQA. On the other hand, refuting our hypothesis, Data Mix 2 and\\n3 do not lead to more abstentions than Vanilla RAG. But, they do abstain more than with Data Mix 1,\\nshowing the impact of adding “I don’t know” in the training set. In general, FT models using RAG\\noutput incorrect answers (%H) much of the time, and often more than they abstain (%A).\\nB.2 P ERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 16, 'page_label': '17', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='output incorrect answers (%H) much of the time, and often more than they abstain (%A).\\nB.2 P ERFORMANCE BREAKDOWN BY SUFFICIENT CONTEXT\\nWe explore RAG performance by different models for various RAG benchmark datasets. Here,\\nthe first column shows performance without RAG (closed-book) while the second column shows\\nperformance with RAG (open-book). To better understand RAG performance, we use our sufficient\\ncontext autorater to stratify the retrieval augmented generation (RAG) datasets into sufficient and\\ninsufficient context. The third and fourth columns show the performance of the second column\\nstratified by sufficient vs insufficient context respectively.\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n100%\\n11.7%\\n10.0%\\n78.3%\\n1.7%\\n9.1%\\n89.1%\\n46.1%\\n12.7%\\n41.2%\\n40.3%\\n8.6%\\n51.1%\\n11.7%\\n9.3%\\n79.0%\\n4.0%\\n6.9%\\n89.1%\\n38.2%\\n17.6%\\n44.1%\\n53.2%\\n11.8%\\n35.0%\\nWithout RAG\\n23.1%\\n54.1%\\n22.8%\\n    With RAG\\n18.3%54.2%\\n27.5%\\n    With RAG\\nSuff. Context\\n77.4% of Dataset\\n39.2%\\n53.9%\\n6.9%\\n    With RAG\\nInsuff. Context\\n22.6% of Dataset\\nCategorizing model responses (FreshQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 5: Correct, hallucination, and abstention fractions across models for dataset FreshQA, stratified\\nby sufficient context. FreshQA includes hand-curated source URLs, which explains the larger\\npercentage of sufficient context (77.4%). FreshQA also specifically explores questions with answers\\nthat change based on the question’s timestamp, which may explain the frequent abstentions without\\nRAG (100% for Gemini 1.5 Pro).\\n68.4%\\n8.2%\\n23.4%\\n12.0%\\n30.2%\\n57.8%\\n6.5%\\n26.0%\\n67.5%\\n16.7%\\n33.8%\\n49.4%\\n26.2%25.8%\\n48.0%\\n10.0%\\n24.8%\\n65.2%\\n8.2%'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 17, 'page_label': '18', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='RAG (100% for Gemini 1.5 Pro).\\n68.4%\\n8.2%\\n23.4%\\n12.0%\\n30.2%\\n57.8%\\n6.5%\\n26.0%\\n67.5%\\n16.7%\\n33.8%\\n49.4%\\n26.2%25.8%\\n48.0%\\n10.0%\\n24.8%\\n65.2%\\n8.2%\\n19.9%\\n71.9%\\n11.5%\\n29.0%\\n59.5%\\n54.8%\\n18.0% 27.2%\\nWithout RAG\\n7.2%\\n42.8%\\n50.0%\\n    With RAG\\n1.7%\\n34.2%\\n64.1%\\n    With RAG\\nSuff. Context\\n46.2% of Dataset\\n11.9%\\n50.2%\\n37.9%\\n    With RAG\\nInsuff. Context\\n53.8% of Dataset\\nCategorizing model responses (HotpotQA dataset)\\nGemini\\n 1.5 Pro\\nGPT 4o\\nGemma\\n 27B\\nAbstain Hallucinate Correct\\nFigure 6: Correct, hallucination, and abstention fractions across models for dataset HotpotQA,\\nstratified by sufficient context. HotpotQA includes questions that are more likely to be answerable\\nwithout context (e.g., yes or no questions, multiple choice questions, or questions with answers that\\nmight be answerable due to pre-training). This explains the higher fraction of correct answers without\\nRAG (e.g., 48.0% for GPT 4o).\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nTable 4: Performance Analysis of RAG Systems Using Human-Annotated Sufficient Context\\nLabels. These tables include results on a curated set of challenging context-dependent questions.\\nTable (a) shows that while larger models generally achieve higher accuracy with sufficient context\\n(present in 54.8% of cases), even top performers exhibit a 14-16% error rate. Table (b) reveals that\\nwith insufficient context (45.2% of cases), models predominantly abstain from answering (50-73%\\nof instances), though significant hallucination rates (15-40%) persist. These patterns of context-\\ndependent performance and hallucination risk are consistent with our analyses of HotpotQA, FreshQA,\\nand Musique datasets, despite variations in absolute performance due to different task complexities.\\n(a) Performance with Sufficient Context (54.8% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 84.1 1.6 14.3\\nGPT 4o 82.5 4.8 12.7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 18, 'page_label': '19', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='(a) Performance with Sufficient Context (54.8% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 84.1 1.6 14.3\\nGPT 4o 82.5 4.8 12.7\\nClaude 3.5 Sonnet 85.7 11.1 3.2\\nGemini 1.5 Flash 77.8 4.8 17.5\\nGemma 27B 71.4 3.2 25.4\\n(b) Performance with Insufficient Context (45.2% of Dataset)\\nModel % Correct % Abstain % Hallucinate\\nGemini 1.5 Pro 9.6 50.0 40.4\\nGPT 4o 23.1 61.5 15.4\\nClaude 3.5 Sonnet 9.6 53.8 36.5\\nGemini 1.5 Flash 7.7 73.1 19.2\\nGemma 27B 9.6 55.8 34.6\\n19'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nB.3 C OMPARISON OF QA E VALUATION METRICS\\nWe compare two the LLM-based QA Evaluator (LLMEval) used in the paper with a deterministic\\nlexical matching metric (Contains Answer). The Contains Answer metric labels responses based on\\nwhether they contain the exact ground truth answer, while LLMEval uses an LLM to assess semantic\\ncorrectness.\\nTable 5 presents model performance across three datasets (FreshQA, Musique, HotpotQA), split by\\nour sufficient context autorater. The results show Contains Answer is generally stricter than LLMEval,\\nthough both metrics reveal similar patterns in model behavior.\\nTable 5: Comparison of evaluation metrics across models and datasets. We show results for\\nchecking whether the response contains one of the ground truth answer strings (\"Contains\"), where\\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='checking whether the response contains one of the ground truth answer strings (\"Contains\"), where\\nwe report the % of responses that contain an answer. We compare this to our LLMEval method that\\nuses an LLM to evaluate if the response is correct, abstain, or hallucinated, and we report % correct.\\nFreshQA Musique HotpotQA\\nModel Context Contains LLMEval Contains LLMEval Contains LLMEval\\nGemini 1.5 Pro Suff 80.3% 89.1% 60.1% 83.4% 47.6% 67.5%\\nInsuff 31.4% 41.2% 33.6% 49.5% 34.2% 49.4%\\nGPT-4 Suff 84.3% 89.1% 64.6% 83.4% 52.4% 71.9%\\nInsuff 36.3% 44.1% 44.4% 61.4% 46.1% 59.5%\\nGemma 27B Suff 26.9% 27.5% 10.8% 23.3% 40.7% 64.1%\\nInsuff 11.8% 6.9% 7.2% 10.1% 22.7% 37.9%\\nClaude 3.5 Suff 67.9% 73.1% 48.9% 74.0% 46.3% 66.7%\\nSonnet Insuff 26.5% 33.3% 19.9% 40.4% 29.0% 38.3%\\nThe Contains Answer metric exhibits several characteristics when compared to LLMEval:\\n1. Different formatting affects matching:\\nQ: What date did the creator of Autumn Leaves die?\\nGround Truth: 13 August 1896'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 19, 'page_label': '20', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='1. Different formatting affects matching:\\nQ: What date did the creator of Autumn Leaves die?\\nGround Truth: 13 August 1896\\nResponse: August 13, 1896.\\nContains Answer: False\\nLLMEval: Correct\\n2. Semantic equivalents are not captured:\\nQ: What former Los Angeles Lakers majority owner is the\\nfather of Jeanie Marie Buss?\\nGround Truth: Gerald Hatten Buss\\nResponse: Jerry Buss.\\nContains Answer: False\\nLLMEval: Correct\\n3. Partial matches can be marked as correct:\\nQ: What is Amazon Prime Video’s most watched premiere ever?\\nGround Truth: The Rings of Power\\nResponse: The series explores the forging of the Rings of Power,\\nthe rise of Sauron...\\nContains Answer: True\\nLLMEval: Hallucinate\\nThe LLM QA evaluator provides several practical advantages:\\n20'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 20, 'page_label': '21', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\n• Handles variations in model verbosity and formatting\\n• Distinguishes between correct, abstain, and incorrect responses\\n• Enables efficient evaluation across multiple datasets\\nOur analysis shows two key findings that are consistent across both metrics: LLMs (i) exhibit\\nhallucination even with sufficient context and (ii) struggle to abstain with insufficient context.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC P ROMPTS\\nC.1 S UFFICIENT CONTEXT AUTORATER PROMPT\\nYou are an expert LLM evaluator that excels at evaluating a QUESTION and REFERENCES.\\nConsider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp <TIMESTAMP>.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION.\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\nEXAMPLE:\\n### QUESTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Output the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\nEXAMPLE:\\n### QUESTION\\nIn which year did the publisher of Roald Dahl’s Guide to Railway Safety cease to exist?\\n### References\\nRoald Dahl’s Guide to Railway Safety was published in 1991 by the British Railways Board.\\nThe British Railways Board had asked Roald Dahl to write the text of the booklet, and\\nQuentin Blake to illustrate it, to help young people enjoy using the railways safely. The\\nBritish Railways Board (BRB) was a nationalised industry in the United Kingdom that\\noperated from 1963 to 2001. Until 1997 it was responsible for most railway services in Great\\nBritain, trading under the brand name British Railways and, from 1965, British Rail. It\\ndid not operate railways in Northern Ireland, where railways were the responsibility of the\\nGovernment of Northern Ireland.\\n### EXPLANATION\\nThe context mentions that Roald Dahl’s Guide to Railway Safety was published by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Government of Northern Ireland.\\n### EXPLANATION\\nThe context mentions that Roald Dahl’s Guide to Railway Safety was published by the\\nBritish Railways Board. It also states that the British Railways Board operated from 1963 to\\n2001, meaning the year it ceased to exist was 2001. Therefore, the context does provide a\\nprecise answer to the question.\\n### JSON\\n{\"Sufficient Context\": 1}\\nRemember the instructions: You are an expert LLM evaluator that excels at evaluating a\\nQUESTION and REFERENCES. Consider the following criteria:\\nSufficient Context: 1 IF the CONTEXT is sufficient to infer the answer to the question and 0\\nIF the CONTEXT cannot be used to infer the answer to the question\\nAssume the queries have timestamp TIMESTAMP.\\nFirst, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 21, 'page_label': '22', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='First, output a list of step-by-step questions that would be used to arrive at a label for the\\ncriteria. Make sure to include questions about assumptions implicit in the QUESTION\\nInclude questions about any mathematical calculations or arithmetic that would be required.\\nNext, answer each of the questions. Make sure to work step by step through any required\\nmathematical calculations or arithmetic. Finally, use these answers to evaluate the criteria.\\nOutput the ### EXPLANATION (Text). Then, use the EXPLANATION to output the ###\\nEV ALUATION (JSON)\\n### QUESTION\\n<question>\\n### REFERENCES\\n<context>\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.2 FLAM E PROMPT\\nINSTRUCTIONS:\\ntitle: Is the context sufficient to infer the answer to the question?\\ndescription: In this task, you will be provided with documents and a question. Use one of the\\nfollowing labels under ’judgment’:\\n1. sufficient: The documents are not sufficient to infer the answer to the question.\\n2. insufficient: The documents are sufficient to infer the answer to the question.\\noutput_fields: judgment\\nCONTEXT:\\ndocuments:<references> question: <question>\\nC.3 LLME VAL PROMPT\\nSince the questions in our datasets ask for free form answers, the LLM responses may not exactly\\nmatch the GT answers. Hence, we use an LLM to determine: the answers are the same (Correct)\\nor the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='or the LLM does not answer the question (Abstain) or the answer is incorrect (Hallucinate). We\\nnote that prior work has shown that Gemini 1.5 Pro has very high accuracy and correlation with\\nhuman judgments for this evaluating free form responses (Krishna et al., 2024). Responses resulting\\nin empty strings were classified as \"missing,\" while variations of \"I don’t know\" were also treated\\nas missing. We normalized both the ground truth answers and the model’s responses by removing\\npunctuation, converting to lowercase, and eliminating stop words.\\n===Task===\\nI need your help in evaluating an answer provided by an LLM against ground truth answers.\\nYour task is to determine if the LLM’s response matches the ground truth answers. Please\\nanalyze the provided data and make a decision.\\n===Instructions===\\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answers\". 2. Consider\\nthe substance of the answers – look for equivalent information or correct answers. Do not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='===Instructions===\\n1. Carefully compare the \"Predicted Answer\" with the \"Ground Truth Answers\". 2. Consider\\nthe substance of the answers – look for equivalent information or correct answers. Do not\\nfocus on exact wording unless the exact wording is crucial to the meaning.\\n3. Your final decision should be based on whether the meaning and the vital facts of the\\n\"Ground Truth Answers\" are present in the \"Predicted Answer.\" 4. Categorize the answer as\\none of the following:\\n- \"perfect\": The answer is completely correct and matches the ground truth.\\n- \"acceptable\": The answer is partially correct or contains the main idea of the ground truth.\\n- \"incorrect\": The answer is wrong or contradicts the ground truth.\\n- \"missing\": The answer is \"I don’t know\", \"invalid question\", or similar responses indicating\\nlack of knowledge.\\n===Input Data===\\n- Question: What 1876 battle featured the Other Magpie?\\n- Predicted Answer: The Other Magpie fought in the Battle of the Rosebud.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 22, 'page_label': '23', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='lack of knowledge.\\n===Input Data===\\n- Question: What 1876 battle featured the Other Magpie?\\n- Predicted Answer: The Other Magpie fought in the Battle of the Rosebud.\\n- Ground Truth Answers: Battle of the Rosebud\\n===Output Format===\\nProvide your evaluation in the following format:\\nExplanation: (How you made the decision)\\nDecision: (One of \"perfect\", \"acceptable\", \"incorrect\", or \"missing\")\\nPlease proceed with the evaluation.\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nC.4 D ATASET QUESTION ANSWER PROMPTS\\nThe CoT prompt instructs the model to provide an accurate and concise answer based solely on\\nthe given search results, using an unbiased and journalistic tone. The prompt includes an example\\nquestion, references, and answer to guide the model’s response format. To extract the final answer,\\nwe implemented a pattern matching technique on the model’s response, specifically targeting the text\\nfollowing \"The answer is:\" for CoT prompts.\\nChain of Thought (CoT)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Start with an accurate, engaging, and concise\\nexplanation based only on the provided documents. Must end with \"The answer is:\". Use an\\nunbiased and journalistic tone.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 23, 'page_label': '24', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='unbiased and journalistic tone.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nAnswer Only (AO)\\nWrite an accurate and concise answer for the given question using only the provided search\\nresults (some of which might be irrelevant). Do not say anything other than the answer itself.\\nEXAMPLE:\\n### Question\\n<example question>\\n### References\\n<example references>\\n### Answer\\n<example answer>\\n### Question\\n<question>\\n### References\\n<references>\\n### Answer\\nD F INE -TUNING AND RAG P ROMPTS FOR MISTRAL\\nFinetuning Prompt (FT)\\n24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Published as a conference paper at ICLR 2025\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n<answer>\\nEvaluation Without RAG Prompt\\nAnswer the question based on your own knowledge. Only give me the answer and do not\\noutput any other words. Please follow the following guideline when formulating your answer:\\nif you are uncertain or don’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\nEvaluation With RAG Prompt\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-04-24T00:18:16+00:00', 'author': '', 'keywords': '', 'moddate': '2025-04-24T00:18:16+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf/2411.06037v3.pdf', 'total_pages': 25, 'page': 24, 'page_label': '25', 'source_file': '2411.06037v3.pdf', 'file_type': 'pdf'}, page_content='Evaluation With RAG Prompt\\nAnswer the question based on the given document. Only give me the answer and do not\\noutput any other words. The following are given references.\\n### References\\n<references>\\nPlease follow the following guideline when formulating your answer: if you are uncertain or\\ndon’t know the answer, respond with “I don’t know”.\\n### Question\\n<question>\\n### Answer\\n25')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d87f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 230 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 8/8 [00:02<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (230, 384)\n",
      "Adding 230 documents to vector store...\n",
      "Successfully added 230 documents to vector store\n",
      "Total documents in collection: 460\n"
     ]
    }
   ],
   "source": [
    "### Convert text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "## now we generate our embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## store in the Vector Database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff35721",
   "metadata": {},
   "source": [
    "### RAG Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3500955",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                print(\"Distances:\", distances[:5])\n",
    "\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9d0278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection count: 460\n",
      "Retrieving documents for query: 'Retrieval-Augmented Generation combines retrieval with generation'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [0.42710837721824646, 0.42710837721824646, 0.5356273651123047, 0.5356273651123047, 0.543084979057312]\n",
      "Retrieved 5 documents (after filtering)\n",
      "Returned: 5\n",
      "Top result distance: 0.42710837721824646\n",
      "Top result score: 0.5728916227817535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Collection count:\", vectorstore.collection.count())\n",
    "\n",
    "docs = rag_retriever.retrieve(\n",
    "    query=\"Retrieval-Augmented Generation combines retrieval with generation\",\n",
    "    top_k=5,\n",
    "    score_threshold=0.0\n",
    ")\n",
    "\n",
    "print(\"Returned:\", len(docs))\n",
    "if docs:\n",
    "    print(\"Top result distance:\", docs[0][\"distance\"])\n",
    "    print(\"Top result score:\", docs[0][\"similarity_score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a9fec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x10475c050>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51d6e5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'foundational concepts'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [1.4232573509216309, 1.4232573509216309, 1.4698652029037476, 1.4698652029037476, 1.4792640209197998]\n",
      "Retrieved 0 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"foundational concepts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5cb5316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. '\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [0.3334968686103821, 0.3334968686103821, 0.4511657953262329, 0.4511657953262329, 0.6015252470970154]\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_8569f0d4_44',\n",
       "  'content': 'A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, CurrentLandscapeandFutureDirections\\nShailjaGupta(CarnegieMellonUniversity, USA)RajeshRanjan(CarnegieMellonUniversity, USA)SuryaNarayanSingh(BITSindri, India)\\nAbstract',\n",
       "  'metadata': {'doc_index': 44,\n",
       "   'producer': 'Skia/PDF m131 Google Docs Renderer',\n",
       "   'source': '../data/pdf/Comprehensive Survey of RAG.pdf',\n",
       "   'total_pages': 18,\n",
       "   'page_label': '1',\n",
       "   'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions',\n",
       "   'content_length': 241,\n",
       "   'page': 0,\n",
       "   'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'Comprehensive Survey of RAG.pdf'},\n",
       "  'similarity_score': 0.6665031313896179,\n",
       "  'distance': 0.3334968686103821,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_a991f1c4_44',\n",
       "  'content': 'A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, CurrentLandscapeandFutureDirections\\nShailjaGupta(CarnegieMellonUniversity, USA)RajeshRanjan(CarnegieMellonUniversity, USA)SuryaNarayanSingh(BITSindri, India)\\nAbstract',\n",
       "  'metadata': {'creationdate': '',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '../data/pdf/Comprehensive Survey of RAG.pdf',\n",
       "   'content_length': 241,\n",
       "   'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions',\n",
       "   'source_file': 'Comprehensive Survey of RAG.pdf',\n",
       "   'page_label': '1',\n",
       "   'total_pages': 18,\n",
       "   'producer': 'Skia/PDF m131 Google Docs Renderer',\n",
       "   'creator': 'PyPDF',\n",
       "   'doc_index': 44,\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.6665031313896179,\n",
       "  'distance': 0.3334968686103821,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_d282ea62_51',\n",
       "  'content': '1.2Overviewof Retrieval-AugmentedGeneration(RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address thelimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,which retrieves relevant documents or information from an external knowledge source, and (ii) ageneration module, which processesthisinformationtogeneratehuman-liketext (Lewiset al. 2020). Thiscombination allows RAG models to not only generate fluent text but also ground their outputs inreal-world, up-to-datedata.',\n",
       "  'metadata': {'content_length': 559,\n",
       "   'page': 1,\n",
       "   'total_pages': 18,\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'Comprehensive Survey of RAG.pdf',\n",
       "   'creationdate': '',\n",
       "   'producer': 'Skia/PDF m131 Google Docs Renderer',\n",
       "   'source': '../data/pdf/Comprehensive Survey of RAG.pdf',\n",
       "   'page_label': '2',\n",
       "   'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions',\n",
       "   'creator': 'PyPDF',\n",
       "   'doc_index': 51},\n",
       "  'similarity_score': 0.5488342046737671,\n",
       "  'distance': 0.4511657953262329,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_e35dbeb1_51',\n",
       "  'content': '1.2Overviewof Retrieval-AugmentedGeneration(RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address thelimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,which retrieves relevant documents or information from an external knowledge source, and (ii) ageneration module, which processesthisinformationtogeneratehuman-liketext (Lewiset al. 2020). Thiscombination allows RAG models to not only generate fluent text but also ground their outputs inreal-world, up-to-datedata.',\n",
       "  'metadata': {'doc_index': 51,\n",
       "   'creator': 'PyPDF',\n",
       "   'page': 1,\n",
       "   'creationdate': '',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 559,\n",
       "   'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions',\n",
       "   'source_file': 'Comprehensive Survey of RAG.pdf',\n",
       "   'total_pages': 18,\n",
       "   'source': '../data/pdf/Comprehensive Survey of RAG.pdf',\n",
       "   'page_label': '2',\n",
       "   'producer': 'Skia/PDF m131 Google Docs Renderer'},\n",
       "  'similarity_score': 0.5488342046737671,\n",
       "  'distance': 0.4511657953262329,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_5583a956_108',\n",
       "  'content': \"Retrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive researchdedicated to improving retrieval effectiveness and enhancing coherent generation to minimizehallucinations. Fromitsearlyiterationstorecent advancements, RAGhasbeeninstrumental inintegratingexternal knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. Inparticular, recent domain-specific work has showcased RAG's potential in specialized areas such aslegal, medical, and low-resource language applications, highlighting its adaptability andscope. However,despite these advances, this paper identifies clear gaps that remain unresolved. Challengessuchastheintegration of ambiguous or unstructured information, effective handling of domain-specific contexts, andthe high computational overhead of complex retrieval tasks still persist. These limitations constrain thebroader applicability of RAG systems, particularly in diverse and dynamic real-world environments.\",\n",
       "  'metadata': {'source_file': 'Comprehensive Survey of RAG.pdf',\n",
       "   'page': 12,\n",
       "   'source': '../data/pdf/Comprehensive Survey of RAG.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'PyPDF',\n",
       "   'producer': 'Skia/PDF m131 Google Docs Renderer',\n",
       "   'page_label': '13',\n",
       "   'creationdate': '',\n",
       "   'total_pages': 18,\n",
       "   'doc_index': 108,\n",
       "   'content_length': 995,\n",
       "   'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions'},\n",
       "  'similarity_score': 0.3984747529029846,\n",
       "  'distance': 0.6015252470970154,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cd97d",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2591444",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"gemma2-9b-it\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b395a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is RAG?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [0.7435992360115051, 0.8802915811538696, 0.9652455449104309]\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `gemma2-9b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m answer = \u001b[43mrag_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhat is RAG?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_retriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mrag_simple\u001b[39m\u001b[34m(query, retriever, llm, top_k)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m## generate the answwer using GROQ LLM\u001b[39;00m\n\u001b[32m     21\u001b[39m prompt=\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mUse the following context to answer the question concisely.\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[33m    Context:\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[33m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[33m    Answer:\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m response=\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/langchain_groq/chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/rag-learn/rag-1/.venv/lib/python3.13/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'The model `gemma2-9b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"what is RAG?\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1fb375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5x/94mftlfx1tsdlgx5d79s3s1r0000gn/T/ipykernel_40546/1413814251.py:12: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.1:8b\", temperature=0.1)\n"
     ]
    }
   ],
   "source": [
    "### Simple RAG pipeline with Ollama LLM (LOCAL, FREE)\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the Ollama LLM (make sure `ollama serve` is running)\n",
    "# Pick one:\n",
    "# - \"llama3.1:8b\" (better, heavier)\n",
    "# - \"llama3.2:3b\" (lighter, faster)\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0.1)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    ## retrieve the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "\n",
    "    ## generate the answer using Ollama LLM\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)   # ✅ Ollama: pass a string (not [string])\n",
    "    return response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3ec73a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is Retrieval Augmented Generation?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [0.5441585779190063, 0.6871664524078369, 0.7346802353858948]\n",
      "Retrieved 3 documents (after filtering)\n",
      "Retrieval-Augmented Generation (RAG) is a hybrid architecture that integrates retrieval and generative mechanisms to generate human-like text grounded in real-world data.\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "answer = rag_simple(\"What is Retrieval Augmented Generation?\", rag_retriever, llm)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd202b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c344a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'DO BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Distances: [0.6075935959815979, 0.6075935959815979, 0.7356294393539429]\n",
      "Retrieved 3 documents (after filtering)\n",
      "Answer: No.\n",
      "Sources: [{'source': '2411.06037v3.pdf', 'page': 4, 'score': 0.3924064040184021, 'preview': 'Therefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\\nsufficien...'}, {'source': '2411.06037v3.pdf', 'page': 4, 'score': 0.3924064040184021, 'preview': 'Therefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\\nLater, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\\nselective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\\nsufficien...'}, {'source': '2411.06037v3.pdf', 'page': 4, 'score': 0.26437056064605713, 'preview': 'We introduce the datasets that we use for our analysis. Then, we investigate the percentage of\\ninstances in these datasets that have sufficient context (according to our autorater). For our study, we\\ndo not aim to optimize the retrieval methods (which could increase the sufficient context percentage...'}]\n",
      "Confidence: 0.3924064040184021\n",
      "Context Preview: Therefore, we use it in Section 4 as our main method for analyzing datasets and model responses.\n",
      "Later, in Section 5.1, we use FLAMe as a computationally efficient autorater to provide a signal for\n",
      "selective generation. Our comparison with TRUE-NLI and Contains GT confirms that classifying\n",
      "sufficien\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "result = rag_advanced(\"DO BENCHMARK DATASETS HAVE HIGH SUFFICIENT CONTEXT?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
