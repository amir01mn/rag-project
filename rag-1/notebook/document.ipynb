{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846a96ac",
   "metadata": {},
   "source": [
    "### DATA INGESTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b16bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DocumentStructure\n",
    "\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3786e7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Amirhossein Mansouri', 'date_created': '2026-01-28'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = Document(\n",
    "    page_content = \"this is the main text content I am using to create RAG\", \n",
    "    metadata = {\n",
    "        \"source\" : \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Amirhossein Mansouri\",\n",
    "        \"date_created\": \"2026-01-28\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4b3c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create a simple txt file \n",
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd718a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe59bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amirmansouri/Dev/rag-learn/rag-1/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5702798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "###Textloader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python_intro.txt\", encoding = \"utf-8\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dddf92d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.'),\n",
       " Document(metadata={'source': '../data/text_files/machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory Loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", ## Pattern to match files  \n",
    "    loader_cls= TextLoader, ##loader class to use\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06f4c8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0}, page_content='A\\nComprehensive\\nSurvey\\nof\\nRetrieval-Augmented\\nGeneration\\n(RAG):\\nEvolution,\\nCurrent\\nLandscape and Future Directions\\nShailja Gupta (Carnegie Mellon University, USA)\\nRajesh Ranjan (Carnegie Mellon University, USA)\\nSurya Narayan Singh (BIT Sindri, India)\\nAbstract\\nThis paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its\\nevolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms\\nwith generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs.\\nThe study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated\\nto handle knowledge-intensive tasks. A detailed review of the significant technological advancements in\\nRAG is provided, including key innovations in retrieval-augmented language models and applications\\nacross various domains such as question-answering, summarization, and knowledge-based tasks.\\nRecent research breakthroughs are discussed, highlighting novel methods for improving retrieval\\nefficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical\\nconcerns in deployment. Future research directions are proposed, with a focus on improving the\\nrobustness of RAG models, expanding the scope of application of RAG models, and addressing societal\\nimplications. This survey aims to serve as a foundational resource for researchers and practitioners in\\nunderstanding the potential of RAG and its trajectory in the field of natural language processing.\\nFigure 1: Trends in RAG captured from recent research papers\\nKeywords: Retrieval-Augmented Generation (RAG), Information Retrieval, Natural Language Processing\\n(NLP), Artificial Intelligence (AI), Machine Learning (ML), Large Language Model (LLM).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 1}, page_content=\"Introduction\\n1.1 Introduction of Natural Language Generation (NLG)\\nNatural Language Processing (NLP) has become a pivotal domain within artificial intelligence (AI), with\\napplications ranging from simple text classification to more complex tasks such as summarization,\\nmachine translation, and question answering. A particularly significant branch of NLP is Natural Language\\nGeneration (NLG), which focuses on the production of human-like language from structured or\\nunstructured data. NLG's goal is to enable machines to generate coherent, relevant, and context-aware\\ntext, improving interactions between humans and machines (Gatt et. al. 2018). As AI evolves, the demand\\nfor more contextually aware and factually grounded generated content has increased, bringing about new\\nchallenges and innovations in NLG.\\nTraditional NLG models, especially sequence-to-sequence architectures (Sutskever et al. 2014), have\\nexhibited significant advancements in generating fluent and coherent text. However, these models tend to\\nrely heavily on training data, often struggling when tasked with generating factually accurate or\\ncontextually rich content for queries that require knowledge beyond their training set. As a result, models\\nlike GPT (Radford et al. 2019) or BERT-based (Devlin et al. 2019) text generators are prone to\\nhallucinations, where they produce plausible but incorrect or non-existent information (Ji et al. 2022). This\\nlimitation has prompted the exploration of hybrid models that combine retrieval mechanisms with\\ngenerative capabilities to ensure both fluency and factual correctness in outputs. There has been a\\nsignificant rise in several research papers in this field and several new methods across the RAG\\ncomponents have been proposed. Apart from new algorithms and methods, RAG has also seen steep\\nadoption across various applications. However, there is a gap in a sufficient survey of this space tracking\\nthe evolution and recent changes in this space. The current survey intends to fill this gap.\\n1.2 Overview of Retrieval-Augmented Generation (RAG)\\nRetrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the\\nlimitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism,\\nwhich retrieves relevant documents or information from an external knowledge source, and (ii) a\\ngeneration module, which processes this information to generate human-like text (Lewis et al. 2020). This\\ncombination allows RAG models to not only generate fluent text but also ground their outputs in\\nreal-world, up-to-date data.\\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant\\ndocuments from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these\\ndocuments are passed to the generative module, often built using transformer-based architectures, to\\ngenerate responses grounded in the retrieved knowledge. This methodology helps mitigate the\\nhallucination problem and ensures that the generated text is more factual and contextually appropriate\\n(Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including\\nopen-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and\\npersonalized recommendations.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 2}, page_content='Figure 2: A basic flow of the RAG system along with its component\\n1.3 Evolution of Hybrid Models in NLP\\nBefore the introduction of RAG, NLP models primarily relied on either retrieval or generation approaches,\\neach with its own set of advantages and limitations. Retrieval-based systems, such as traditional\\ninformation retrieval engines (Salton et al., 1975), efficiently provided relevant documents or snippets in\\nresponse to a query but could not synthesize new information or present the results in a coherent\\nnarrative. On the other hand, purely generative models, which became popular with the rise of\\ntransformer architectures (Vaswani et al. 2017), offered fluency and creativity but often lacked factual\\naccuracy.\\nThe development of hybrid systems combining retrieval and generation began to gain momentum as\\nresearchers recognized the complementary strengths of both approaches. Early efforts in hybrid modeling\\ncan be traced back to works like DrQA (Chen et al. 2017), which employed retrieval techniques to fetch\\nrelevant documents for question-answering tasks. However, the generative component in such systems\\nwas minimal, often limited to selecting text directly from the retrieved documents. Similarly, in models like\\nInformation Retrieval (Dai et al. 2019), retrieval was treated as distinct, independent components.\\nThe real innovation came with the realization that retrieval and generation could be tightly integrated.\\nModels like REALM (Guu et al., 2020) represented a key milestone, as they trained the retrieval and\\ngenerative components jointly, enabling better alignment between the retrieved information and the\\ngenerated output. RAG (Lewis et al. 2020) further extended this paradigm by using dense passage\\nretrieval (Karpukhin et al., 2020) to fetch relevant documents and transformers like BART (Lewis et al.,\\n2020) for a generation. This architecture provided a more seamless integration of retrieval and\\ngeneration, allowing the model to answer open-ended questions with both fluency and factual grounding.\\n1.4 Importance of Factually Grounded Language Generation\\nOne of the main motivations for developing RAG is the increasing demand for factually accurate,\\ncontextually relevant, and up-to-date generated content. In many applications, such as customer service,\\nmedical diagnostics, or legal advisory systems, the need for reliable and grounded responses is\\nparamount. Generative models that produce hallucinated or inaccurate information can lead to serious\\nconsequences, such as spreading misinformation or providing incorrect advice (Ji et al. 2022).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 3}, page_content=\"RAG models directly address these concerns by grounding their generative process in external,\\nup-to-date knowledge sources. This grounding improves the factual accuracy of the output and enhances\\nthe relevance of responses by incorporating real-world data that is directly tied to the query. Additionally,\\nRAG models are less likely to propagate biases present in static training data, as they can retrieve more\\ndiverse and balanced information from external sources\\n1.5 Applications of RAG Models\\nRAG models have been applied across a wide array of domains where factual accuracy and contextual\\nunderstanding are critical. One of the most prominent applications is in open-domain question answering,\\nwhere the model must generate answers based on a wide range of topics. RAG has proven effective in\\nimproving answer accuracy by retrieving relevant information and then generating responses grounded in\\nthat data (Izacard et. al. 2021). Models like Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and\\nFusion-in-Decoder (Izacard et. al. 2021) have been used to great effect in this context, showing significant\\nimprovements over traditional generative or retrieval-only models.\\nIn conversational AI, RAG models have enhanced the capabilities of dialogue systems by ensuring that\\nresponses are both coherent and grounded in factual information (Roller et al., 2020). For example,\\nchatbots used in customer service can benefit from RAG's ability to retrieve specific details from product\\ndatabases or documentation, leading to more accurate and useful responses for end-users.\\nOther applications include medical diagnosis systems, where RAG can retrieve and integrate the latest\\nresearch findings or patient-specific data to generate accurate diagnostic suggestions, and legal advisory\\nsystems, where the model can retrieve relevant case law or statutes to provide legally sound advice.\\nFurthermore, RAG has found applications in personalized recommendation systems, where it can retrieve\\nuser preferences or past interactions and generate personalized suggestions.\\n1.6 Challenges and Limitations of RAG\\nDespite the promise of RAG models, several challenges need attention. The retrieval mechanism, while\\npowerful, can still struggle with retrieving the most relevant documents, particularly when dealing with\\nambiguous queries or niche knowledge domains. The reliance on dense vector representations, such as\\nthose used in DPR, can sometimes lead to irrelevant or off-topic documents being retrieved. Efforts to\\nrefine retrieval techniques, including the incorporation of more sophisticated query expansion and\\ncontextual disambiguation, are needed to improve performance in these areas. The integration between\\nretrieval and generation, while seamless in theory, can sometimes fail in practice. For instance, the\\ngenerative module may not always effectively incorporate the retrieved information into its responses,\\nleading to inconsistencies or incoherence between the retrieved facts and the generated text. Research\\ninto better alignment mechanisms, such as improved attention models or hierarchical fusion techniques,\\nmay help alleviate these issues (Izacard et. al. 2021). Additionally, the computational overhead of RAG\\nmodels is a concern, as they require both a retrieval and a generation step for each query. This dual\\nprocess can be resource-intensive, particularly for large-scale applications (Borgeaud et al. 2021).\\nTechniques such as model pruning (Han et al. 2015) or knowledge distillation (Sanh et al., 2019) may\\noffer ways to reduce the computational burden without sacrificing performance. Finally, there are ethical\\nconcerns associated with the deployment of RAG models, particularly in terms of bias and transparency.\\nBiases in AI and LLM have been a well-researched and evolving field with researchers identifying\\ndifferent types of biases not limited to Gender, socio-economic class, or even educational background\\n(Gupta et. al. 2024; Ranjan et. al., 2024). While RAG has the potential to reduce biases by retrieving\\nmore balanced information, there is still the risk of amplifying biases present in the retrieved sources\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 4}, page_content=\"(Binns, 2018). Furthermore, ensuring transparency in how retrieval results are selected and used in\\ngeneration is crucial for maintaining trust in these systems.\\n1.7 Scope of the Survey\\nThis paper aims to provide a comprehensive survey of RAG models, covering their evolution, key\\narchitectural components, recent research in this area, current challenges and limitations of RAG, and\\nfuture research direction.\\n2: Core Components and Architectural Overview of RAG Systems\\n2.1 Overview of RAG Models\\nRetrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural\\nlanguage generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base.\\nTraditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast\\ncorpora, rely entirely on their internal representations of knowledge, making them susceptible to issues\\nlike hallucinations—where the models generate plausible but incorrect information. These models cannot\\nefficiently update their knowledge bases without retraining, making them less practical for dynamic,\\nknowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al.\\n2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which\\nretrieves real-time, relevant external documents to ground the generated text in factual information.\\nThe RAG model incorporates two key components:\\n1.\\nRetriever: This retrieves the most relevant documents from a corpus using techniques such as\\ndense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms.\\n2.\\nGenerator:\\nIt\\nsynthesizes\\nthe\\nretrieved\\ndocuments\\ninto\\ncoherent,\\ncontextually\\nrelevant\\nresponses.\\nRAG’s strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform\\ngenerative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets.\\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently\\nretrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et\\nal. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the\\nmodel fetches external documents for each query, it requires no retraining to incorporate the latest\\ninformation. This flexibility makes RAG models particularly suitable for domains where information is\\nconstantly evolving, such as medical research, financial news, and legal proceedings. Furthermore,\\nstudies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks,\\nincluding document summarization and, knowledge-grounded dialogues\\n2.2 Retriever Mechanisms in RAG Systems\\nThe retriever in RAG systems is essential for fetching relevant documents from an external corpus.\\nEffective retrieval ensures that the model's output is grounded in accurate information. Several retrieval\\nmechanisms are commonly used, ranging from traditional methods like BM25 to more sophisticated\\ntechniques like Dense Passage Retrieval (DPR).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 5}, page_content=\"2.2.1 BM25\\nBM25 is a well-established information retrieval algorithm that uses the term frequency-inverse document\\nfrequency (TF-IDF) to rank documents according to relevance. Despite being a classical method, BM25\\nremains a strong baseline for many modern retrieval systems, including those used in RAG models.\\nBM25 calculates the relevance score of a document based on how frequently a query term appears in the\\ndocument while adjusting for the document's length and the frequency of the term across the corpus\\n(Robertson et. al. 2009). While BM25 is effective for keyword matching, it has limitations in understanding\\nsemantic meaning. For example, BM25 cannot capture the relationships between words and tends to\\nperform poorly on more complex, natural language queries that require an understanding of context.\\nDespite this limitation, BM25 is still widely used because of its simplicity and efficiency. BM25 is effective\\nfor tasks involving simpler, keyword-based queries, although more modern retrieval models like DPR tend\\nto outperform it in semantically complex tasks.\\n2.2.2 Dense Passage Retrieval (DPR)\\nDense Passage Retrieval (DPR), introduced by Karpukhin et al. (2020), represents a more modern\\napproach to information retrieval. It uses a dense vector space in which both the query and the\\ndocuments are encoded into high-dimensional vectors. DPR employs a bi-encoder architecture, where\\nthe query and documents are encoded separately, allowing for efficient nearest-neighbor search (Xiong\\net. al. 2020). Unlike BM25, DPR excels at capturing semantic similarity between the query and\\ndocuments, making it highly effective for open-domain question-answering tasks. The strength of DPR\\nlies in its ability to retrieve relevant information based on semantic meaning rather than keyword\\nmatching. By training the retriever on a large corpus of question-answer pairs, DPR can find documents\\nthat are contextually related to the query, even when the query and the document do not share exact\\nterms. Recent research has further improved DPR by integrating it with pre-trained language models and\\nan example is LLM adapted for the dense RetrievAl approach (Li et. al. 2023)\\n2.2.3 REALM (Retrieval-Augmented Language Model)\\nAnother significant advancement in retrieval mechanisms for RAG models is REALM (Guu et al. (2020).\\nREALM integrates retrieval into the language model's pre-training process, ensuring that the retriever is\\noptimized alongside the generator for downstream tasks. The key innovation in REALM is that it learns to\\nretrieve documents that improve the model’s performance on specific tasks, such as question answering\\nor document summarization. During training, REALM updates both the retriever and the generator,\\nensuring that the retrieval process is optimized for the generation task. REALM’s retriever is trained to\\nidentify documents that are not only relevant to the query but also helpful for generating accurate and\\ncoherent responses. As a result, REALM significantly improves the quality of generated responses,\\nparticularly in tasks that require external knowledge. Recent studies have demonstrated that REALM\\noutperforms both BM25 and DPR in certain knowledge-intensive tasks, particularly when retrieval is\\ntightly coupled with generation.\\nThe core of RAG lies in the quality of retrieved passages, but many current methods rely on\\nsimilarity-based retrieval (Mallen et al. 2022). Self-RAG (Asai et al. 2023b), and REPLUG (Shi et al.,\\n2023) have advanced by leveraging LLMs to enhance retrieval capabilities, achieving more adaptive\\nretrieval. After initial retrieval, cross-encoder models are used to re-rank the retrieved results by jointly\\nencoding the query and each retrieved document to compute relevance scores. These models provide\\nmore context-aware retrieval at the cost of higher computational overhead. Pointwise and Pairwise\\nRanking, often based on Learning-to-Rank (LTR) algorithms, are used to assign relevance scores to\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 6}, page_content=\"retrieved documents, either independently (pointwise) or by comparing document pairs (pairwise). RAG\\nsystems utilize self-attention within the LLM to manage context and relevance across different parts of the\\ninput and retrieved text. Cross-attention mechanisms are used when integrating retrieved information into\\nthe generative model, ensuring that the most relevant pieces of information are emphasized during\\ngeneration.\\n2.3 Generator Mechanisms in RAG Systems\\nIn Retrieval-Augmented Generation (RAG) systems, the generator mechanism plays a crucial role in\\nproducing the final output by integrating retrieved information with the input query. After the retrieval\\ncomponent pulls relevant knowledge from external sources, the generator synthesizes this information\\ninto coherent, contextually appropriate responses. The Large Language Model (LLM) serves as the\\nbackbone of the generator, which ensures the generated text is fluent, accurate, and aligned with the\\noriginal query.\\n2.3.1 T5 (Text-to-Text Transfer Transformer)\\nT5 (Text-to-Text Transfer Transformer) (Raffel et al. 2020) is one of the most commonly used models for\\ngeneration tasks in RAG systems. T5 is versatile in its approach, framing every NLP task as a text-to-text\\ntask. This uniform framework allows T5 to be fine-tuned for a wide range of tasks, including\\nquestion-answering, summarization, and dialogue generation. By integrating retrieval with generation,\\nT5-based RAG models have been shown to outperform traditional generative models like GPT-3 and\\nBART on several benchmarks, including the Natural Questions dataset and the TriviaQA dataset.\\nMoreover, T5's ability to handle complex multi-task learning makes it a popular choice for RAG systems\\nthat need to tackle a diverse range of knowledge-intensive tasks.\\n2.3.2 BART\\nBART (Bidirectional and Auto-Regressive Transformer), introduced by Lewis et al. (2020), is another\\nprominent generative model used in RAG systems. BART is particularly well-suited for tasks involving text\\ngeneration from noisy inputs, such as summarization and open-domain question answering. As a\\ndenoising autoencoder, BART can reconstruct corrupted text sequences, making it robust for tasks that\\nrequire the generation of coherent, factual outputs from incomplete or noisy data. When paired with a\\nretriever in a RAG system, BART has been shown to improve the factual accuracy of generated text by\\ngrounding it in external knowledge. Studies have demonstrated that BART-based RAG models achieve\\nstate-of-the-art results in various knowledge-intensive tasks, including dialogue generation and news\\nsummarization.\\n3. Retrieval-Augmented Generation Models Across Different Modalities\\n3.1 Text-Based RAG Models: Text-based RAG models represent the most mature and widely\\nresearched category. These models leverage textual data for both retrieval and generation tasks,\\nenabling\\napplications\\nsuch\\nas\\nquestion-answering,\\nsummarization,\\nand\\nconversational\\nagents.\\nTransformer architectures, such as BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020), are\\nfoundational in text-based RAG models. These models utilize self-attention mechanisms to capture\\ncontextual relationships within text, which enhances both retrieval accuracy and generation fluency.\\nDense retrieval models, such as those using dense embeddings from BERT, offer superior performance\\ncompared to traditional sparse methods like TF-IDF. Dense retrievers (Karpukhin et al. 2020), leverage\\ndense representations to retrieve relevant documents more effectively. Recent advancements focus on\\nintegrating retrieval and generation into a single training pipeline. REALM (Guu et al., 2020) is an\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 7}, page_content='example of such an end-to-end model that jointly optimizes retrieval and generation processes, improving\\noverall task performance.\\n3.2 Audio-Based RAG Models: Audio-based RAG models extend the principles of retrieval-augmented\\ngeneration to the audio modality, enabling applications such as speech recognition, audio summarization,\\nand conversational agents in voice interfaces. Audio data is often represented using embeddings derived\\nfrom pre-trained models like Wav2Vec 2.0 (Baevski et al., 2020). These embeddings serve as input to\\nretrieval and generation components, enabling the model to handle audio data effectively.\\n3.3\\nVideo-Based\\nRAG\\nModels: Video-based RAG models incorporate both visual and textual\\ninformation to enhance performance in tasks such as video understanding, captioning, and retrieval.\\nVideo data is represented using embeddings from models like I3D (Xie et. al. 2017) or TimeSformer\\n(Bertasius et al. 2021). These embeddings capture temporal and spatial features essential for effective\\nretrieval and generation.\\n3.4 Multimodal RAG Models: Multimodal RAG models integrate data from multiple modalities—text,\\naudio, video, and images—to provide a more holistic approach to retrieval and generation tasks. Models\\nlike Flamingo (Alayrac et al., 2022) integrate multiple modalities into a unified framework, enabling\\nsimultaneous processing of text, images, and videos. Techniques for cross-modal retrieval involve\\nretrieving relevant information across different modalities (Li. et. al. 2023).\\nMultimodal capabilities enhance the versatility and efficiency of RAG across various applications.”\\nRetrieval as generation” (Wang et. al. 2024) extends the Retrieval-Augmented Generation (RAG)\\nframework to multimodal applications by incorporating text-to-image and image-to-text retrieval. Utilizing a\\nlarge dataset of paired images and text descriptions, the system accelerates image generation when user\\nqueries align with stored text descriptions (\"retrieval as generation\"). The image-to-text functionality\\nallows users to engage in discussions based on input images.\\nFigure 3: Timeline of the evolution of the RAG system and its components'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 8}, page_content='4. Recent Advancement in the field:\\nThere has been significant advancement in this field and this section intends to capture key findings of a\\nfew important recent papers. A novel agentic Retrieval-Augmented Generation (RAG) framework (Ravuru\\net. al. 2024) employs a hierarchical, multi-agent architecture where specialized sub-agents, using smaller\\npre-trained language models (SLMs), are fine-tuned for specific time series tasks. The master agent\\ndelegates tasks to these sub-agents, who retrieve relevant prompts from a shared knowledge repository.\\nIn this modular, multi-agent approach, the authors achieve state-of-the-art performance demonstrating\\nimproved flexibility and effectiveness over task-specific methods in time series analysis. RULE (Xia et. al.\\n2024), a multimodal Retrieval-Augmented Generation (RAG) framework designed to improve the\\nfactuality of medical Vision-Language Models (Med-LVLM), addresses challenges in medical RAG by\\nintroducing a calibrated selection strategy to control factuality risk, and, by developing a preference\\noptimization strategy to balance the model’s intrinsic knowledge with retrieved contexts, proving its\\neffectiveness in enhancing factual accuracy in Med-LVLM systems. METRAG (Gan et. al. 2024), a\\nmulti-layered, thoughts-enhanced retrieval-augmented generation framework, integrates LLM supervision\\nto\\ngenerate\\nutility-oriented\\nthoughts\\nand\\ncombines document similarity with utility for improved\\nperformance. It also incorporates a task-adaptive summarizer to produce compact thoughts. Using the\\nmulti-layered\\nthoughts\\nfrom\\nthese\\nstages,\\nan\\nLLM\\ngenerates\\nknowledge-augmented\\ncontent,\\ndemonstrating superior performance on knowledge-intensive tasks compared to traditional approaches.\\nDistractor document is\\nFigure 4: Evolving Trends in RAG captured from research papers'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 9}, page_content=\"one of the key traits of Retrieval Augmented Fine-Tuning (RAFT) (Zhang et. al. 2024) where the model is\\ntrained to disregard irrelevant, distractor documents and instead cite directly from relevant sources. This\\nprocess, combined with a chain-of-thought reasoning style, enhances the model's reasoning capabilities.\\nRAFT demonstrates consistent performance improvements in domain-specific RAG tasks, including\\nPubMed, HotpotQA, and Gorilla datasets, serving as a post-training enhancement for LLMs. FILCO\\n(Wang et. al. 2023) , a method designed to enhance the quality of context provided to generative models\\nin tasks like open-domain question answering and fact verification, addresses issues of over- or\\nunder-reliance on retrieved passages, which can lead to problems such as hallucinations in the generated\\noutputs. The method improves context quality by identifying useful context through lexical and\\ninformation-theoretic approaches and training context filtering models to refine retrieved contexts during\\ntest time. Reflection Token is a key attribute of Self-reflective Retrieval Augmented-Generation (Self-RAG)\\n(Asai et. al. 2023), a novel framework designed to improve the factual accuracy of large language models\\n(LLMs) by combining retrieval with self-reflection. Unlike traditional methods that retrieve and incorporate\\na fixed number of passages, Self-RAG adaptively retrieves relevant passages and uses reflection tokens\\nto evaluate and refine its responses, allowing the model to adjust its behavior according to task-specific\\nneeds and has shown superior performance in open-domain question-answering, reasoning, fact\\nverification, and long-form generation tasks. Intelligence and effectiveness of RAG are dependent a lot on\\nthe quality of retrieval and more meta-data understanding of the repository would enhance the\\neffectiveness of the RAG system. A novel data-centric Retrieval-Augmented Generation (RAG) workflow\\nadvances\\nbeyond\\nthe\\ntraditional\\nretrieve-then-read\\nmode\\nand\\nemploys\\na\\nprepare-then-rewrite-then-retrieve-then-read framework, enhancing LLMs by integrating contextually\\nrelevant, time-critical, or domain-specific information. Key innovations include generating metadata,\\nsynthetic Questions and Answers (QA), and introducing the Meta Knowledge Summary (MK Summary)\\nfor clusters of documents (Mombaerts et. al. 2024). A recent paper introduces CommunityKG-RAG\\n(Chang et. al. 2024), a zero-shot framework that integrates community structures within Knowledge\\nGraphs (KGs) into Retrieval-Augmented Generation (RAG) systems. This approach enhances the\\naccuracy and contextual relevance of fact-checking by utilizing multi-hop connections within KGs,\\noutperforming traditional methods without requiring additional domain-specific training. The RAPTOR\\nmodel (Sarthi et. al. 2024) introduces a hierarchical approach to retrieval-augmented language models,\\naddressing limitations in traditional methods that retrieve only short, contiguous text chunks. RAPTOR\\nforms a summary tree to retrieve information at varying abstraction levels by recursively embedding,\\nclustering, and summarizing text. Experiments demonstrate RAPTOR’s superior performance, especially\\nin question-answering tasks requiring complex reasoning. When paired with GPT-4, RAPTOR improves\\naccuracy on the QuALITY benchmark by 20%.\\nThis advancement in RAG further proves the utility of the RAG system however recent LLM launches that\\nsupport long-term context have significantly shown improved performance. A recent study (Li et. al. 2024)\\ncompared the efficiency of Retrieval Augmented Generation (RAG) and long-context (LC) Large\\nLanguage Models (LLMs), such as Gemini-1.5 and GPT-4. While LC models outperform RAG when\\nadequately resourced, RAG's cost-efficiency remains advantageous. To balance performance and cost,\\nthe paper introduces Self-Route. This method dynamically directs queries to either RAG or LC based on\\nmodel self-reflection, optimizing both computation cost and performance. This study offers valuable\\ninsights into the optimal application of RAG and LC in handling long-context tasks. Nguyen et. al., 2024\\nintroduce SFR-RAG , a small but highly efficient Retrieval Augmented Generation (RAG) model, which is\\ndesigned to enhance the integration of external contextual information into Large Language Models\\n(LLMs) while minimizing hallucinations.\\nLA-RAG (Li et. al., 2024), a novel Retrieval-Augmented\\nGeneration (RAG) paradigm designed to enhance Automatic Speech Recognition (ASR) in large\\nlanguage models (LLMs). One of the key benefits of LA-RAG is its ability to leverage fine-grained\\ntoken-level speech data stores alongside a speech-to-speech retrieval mechanism, improving ASR\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 10}, page_content=\"accuracy by incorporating LLM in-context learning (ICL). The study focuses on datasets of Mandarin and\\nvarious Chinese dialects, demonstrating significant accuracy improvements, particularly in managing\\naccent variations, which have historically been a challenge for existing speech encoders. The findings\\nhighlight LA-RAG’s potential to advance ASR technology, offering a more robust solution for diverse\\nacoustic conditions. Large Language Models (LLMs) face challenges in AI legal and policy contexts due\\nto outdated knowledge and hallucinations. HyPA-RAG (Kalra et. al., 2024), a Hybrid Parameter-Adaptive\\nRetrieval-Augmented Generation system, improves accuracy by using adaptive parameter tuning and\\nhybrid retrieval strategies. Tested on NYC Local Law 144 (LL144), HyPA-RAG demonstrates enhanced\\ncorrectness and contextual precision, addressing the complexities of legal texts. MemoRAG (Qian et. al.,\\n2024) introduces a novel Retrieval-Augmented Generation (RAG) paradigm designed to overcome the\\nlimitations of traditional RAG systems in handling ambiguous or unstructured knowledge. MemoRAG’s\\ndual-system architecture utilizes a lightweight long-range LLM to generate draft answers and guide\\nretrieval tools, while a more powerful LLM refines the final output. This framework, optimized for better\\ncluing and memory capacity, significantly outperforms conventional RAG models across both complex\\nand straightforward tasks. NLLB-E5 (Acharya et. al., 2024)\\nintroduces a scalable multilingual retrieval\\nmodel\\naimed\\nat\\naddressing\\nthe\\nchallenges faced in supporting multiple languages, particularly\\nlow-resource languages like Indic languages. By leveraging the NLLB encoder and a distillation approach\\nfrom the E5 multilingual retriever, NLLB-E5 enables zero-shot retrieval across languages without the need\\nfor multilingual training data. Evaluations on benchmarks such as Hindi-BEIR showcase its robust\\nperformance, highlighting task-specific challenges and advancing multilingual information access for\\nglobal inclusivity.\\n5. Current Challenges and Limitations in Retrieval-Augmented Generation (RAG):\\nThis section intends to highlight the current challenges and limitations of RAG considering the current\\nlandscape of the system and this would shape the future research directions in the field.\\nScalability and Efficiency: One of the primary challenges for RAG models is scalability. As retrieval\\ncomponents rely on external databases, handling vast and dynamically growing datasets requires efficient\\nretrieval algorithms. High computational costs and memory requirements also make it difficult to deploy\\nRAG models in real-time or resource-constrained environments (Shi et al. 2023), (Asai et al. 2023b).\\nRetrieval Quality and Relevance: Ensuring the quality and relevance of retrieved documents remains a\\nsignificant concern. Retrieval models can sometimes return irrelevant or outdated information, which\\nnegatively affects the accuracy of the generated output. Improving retrieval precision, especially for\\nlong-form content generation, remains an active area of research (Mallen et al. 2022), (Shi et al. 2023).\\nBias and Fairness: Similar to other machine learning models, RAG systems can exhibit bias due to\\nbiases present in the retrieved datasets. Retrieval-based models may amplify harmful biases in retrieved\\nknowledge, leading to biased outputs in a generation. Developing bias mitigation techniques for retrieval\\nand generation in tandem is an ongoing challenge.\\nCoherence: RAG models often struggle with integrating the retrieved knowledge into coherent,\\ncontextually relevant text. The alignment between retrieved passages and the generation model's output\\nis not always seamless, leading to inconsistencies or factual hallucinations in the final response (Ji et al.\\n2022).\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 11}, page_content='Interpretability and Transparency: Like many AI systems, RAG models are often treated as black\\nboxes, with limited transparency in how retrieval influences generation. Improving the interpretability of\\nthese models is crucial to fostering trust, especially in critical applications (Roller et al. 2020).\\n6. Future Research Directions for Retrieval-Augmented Generation (RAG)\\nRetrieval-augmented generation (RAG) represents a significant advancement in natural language\\nprocessing and related fields by combining retrieval and generative mechanisms. This section explores\\nkey areas for future research, highlighting the potential for innovation and improvement in RAG systems.\\n6.1 Enhancing Multimodal Integration: The integration of text, image, audio, and video data in RAG\\nmodels remains an evolving challenge. Future research should focus on improving multimodal fusion\\ntechniques to enable seamless interaction between different data types. This includes developing\\nadvanced methods for aligning and synthesizing information across modalities. Recent works (Chen et.\\nal. 2022), (Yasunaga et. al. 2022), (Zhu et. al. 2024) have explored multimodal learning, but further\\ninnovations are needed to enhance the coherence and contextuality of multimodal outputs.Research into\\ncross-modal retrieval aims to improve the ability of RAG systems to retrieve relevant information across\\ndifferent modalities. For example, combining text-based queries with image or video content retrieval\\ncould enhance applications such as visual question answering and multimedia search. This is another\\nfuture direction to explore for RAG related research.\\n6.2 Scaling and Efficiency: As RAG models are deployed in increasingly large-scale applications,\\nscalability becomes a critical concern. Research should focus on developing methods to efficiently scale\\nretrieval and generation processes without compromising performance. Techniques such as distributed\\ncomputing and efficient indexing methods are essential for handling large datasets. Improving the\\nefficiency of RAG models involves optimizing both retrieval and generation components to reduce\\ncomputational resources and latency.\\n6.3 Personalization and Adaptation: Future RAG models should focus on personalizing retrieval\\nprocesses to cater to individual user preferences and contexts. This involves developing techniques to\\nadapt retrieval strategies based on user history, behaviour, and preferences. Enhancing the contextual\\nadaptation of RAG models by deeper understanding of the context and sentiments of query (Gupta et. al.\\n2024) and the repository of ducments is crucial for improving the relevance of generated responses.\\nResearch should explore methods for dynamic adjustment of retrieval and generation processes based\\non the evolving context of interactions. This includes incorporating user feedback and contextual cues into\\nthe RAG pipeline.\\n6.4 Ethical and Privacy Considerations: Addressing biases (Shrestha et. al. 2024), (Gupta et. al. 2024)\\nin general and specifics to RAG models is a critical area for future research. As RAG systems are\\ndeployed in diverse applications, ensuring fairness and mitigating biases in retrieved and generated\\ncontent is essential. Future RAG research should focus on privacy-preserving techniques to protect\\nsensitive information during retrieval and generation. This includes developing methods for secure data\\nhandling and privacy-aware retrieval strategies. Interpretability of model is also a critical area to focus\\nupon as a part of on going research in improving RAG.\\n6.5 Cross-Lingual and Low-Resource Languages: Expanding RAG technology to support multiple\\nlanguages ( Chirkova et. al. 2024), especially low-resource languages, is a promising direction. Future'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 12}, page_content=\"research should aim to improve cross-lingual retrieval and generation capabilities to provide accurate and\\nrelevant results across different languages. Enhancing RAG models to effectively support low-resource\\nlanguages involves developing methods to retrieve and generate content with limited training data.\\nResearch\\nshould\\nfocus\\non\\ntechniques for transfer learning and data augmentation to improve\\nperformance in underrepresented languages.\\n6.6\\nAdvanced\\nRetrieval\\nMechanisms:\\nFuture RAG research should explore dynamic retrieval\\nmechanisms that adapt to changing query patterns and content requirements. This includes developing\\nmodels that can dynamically update their retrieval strategies based on new information and evolving user\\nneeds. Investigating hybrid retrieval approaches that combine various retrieval strategies, such as dense\\nand sparse retrieval, could enhance the effectiveness of RAG systems. Research should explore how to\\nintegrate different retrieval methods to achieve optimal performance for diverse tasks.\\n6.7 Integration with Emerging Technologies: Integrating RAG models with brain-computer interfaces\\n(BCIs) could lead to novel applications in human-computer interaction and assistive technologies.\\nResearch should explore how RAG systems can leverage BCI data to enhance user experience and\\ngenerate context-aware responses.The integration of RAG with AR and VR technologies presents\\nopportunities for creating immersive and interactive experiences. Future research should investigate how\\nRAG models can be used to enhance AR and VR applications by providing contextually relevant\\ninformation and interactions.\\n7. Conclusion\\nRetrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive research\\ndedicated\\nto\\nimproving\\nretrieval\\neffectiveness\\nand\\nenhancing\\ncoherent\\ngeneration\\nto\\nminimize\\nhallucinations. From its early iterations to recent advancements, RAG has been instrumental in integrating\\nexternal knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. In\\nparticular, recent domain-specific work has showcased RAG's potential in specialized areas such as\\nlegal, medical, and low-resource language applications, highlighting its adaptability and scope. However,\\ndespite these advances, this paper identifies clear gaps that remain unresolved. Challenges such as the\\nintegration of ambiguous or unstructured information, effective handling of domain-specific contexts, and\\nthe high computational overhead of complex retrieval tasks still persist. These limitations constrain the\\nbroader applicability of RAG systems, particularly in diverse and dynamic real-world environments. The\\nfuture research directions outlined in this paper—ranging from improving retrieval mechanisms to\\nenhancing context management and ensuring scalability—will serve as a critical guide for the next phase\\nof innovation in this space. By addressing these gaps, the next generation of RAG models has the\\npotential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing the\\nboundaries of what is possible in retrieval-augmented AI applications.\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 13}, page_content='References:\\nAcharya, A., Murthy, R., Kumar, V., & Sen, J. (2024). NLLB-E5: A Scalable Multilingual Retrieval Model.\\nArXiv. /abs/2409.05401\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\\nReynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M.,\\nMenick, J., Borgeaud, S., . . . Simonyan, K. (2022). Flamingo: A Visual Language Model for Few-Shot\\nLearning. ArXiv. /abs/2204.14198\\nAsai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). Self-RAG: Learning to retrieve, generate, and\\ncritique through self-reflection. arXiv preprint arXiv:2310.11511.\\nBaevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). Wav2vec 2.0: A Framework for Self-Supervised\\nLearning of Speech Representations. ArXiv. /abs/2006.11477\\nBertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video\\nUnderstanding? ArXiv. /abs/2102.05095\\nBinns, R. (2018). Fairness in machine learning: Lessons from political philosophy. Proceedings of the\\n2018 Conference on Fairness, Accountability, and Transparency (pp. 149-159).\\nBorgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Driessche, G. V., Lespiau, J.,\\nDamoc, B., Clark, A., Casas, D. D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L.,\\nJones, C., Cassirer, A., . . . Sifre, L. (2021). Improving language models by retrieving from trillions of\\ntokens. ArXiv. /abs/2112.04426\\nBrown, T., et al. (2020). \"Language Models are Few-Shot Learners.\" arXiv preprint arXiv:2005.14165.\\nChang, R., & Zhang, J. (2024). CommunityKG-RAG: Leveraging Community Structures in Knowledge\\nGraphs for Advanced Retrieval-Augmented Generation in Fact-Checking. ArXiv. /abs/2408.08535\\nChen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to answer open-domain\\nquestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) (pp. 1870-1879).\\nChen, W., Hu, H., Chen, X., Verga, P., & Cohen, W. W. (2022). MuRAG: Multimodal Retrieval-Augmented\\nGenerator for Open Question Answering over Images and Text. ArXiv. /abs/2210.02928\\nChirkova, N., Rau, D., Déjean, H., Formal, T., Clinchant, S., & Nikoulina, V. (2024). Retrieval-augmented\\ngeneration in multilingual settings. ArXiv. /abs/2407.01463\\nDai, Z., & Callan, J. (2019). Context-Aware Sentence/Passage Term Importance Estimation For First\\nStage Retrieval. ArXiv. /abs/1910.10687\\nDevlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional\\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American\\nChapter\\nof\\nthe\\nAssociation\\nfor\\nComputational\\nLinguistics:\\nHuman\\nLanguage\\nTechnologies\\n(pp.\\n4171-4186).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 14}, page_content='Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. ArXiv. /abs/1810.04805\\nGan, C., Yang, D., Hu, B., Zhang, H., Li, S., Liu, Z., Shen, Y., Ju, L., Zhang, Z., Gu, J., Liang, L., & Zhou,\\nJ. (2024). Similarity is Not All You Need: Endowing Retrieval Augmented Generation with Multi Layered\\nThoughts. ArXiv. /abs/2405.19893\\nGatt, A., & Krahmer, & E. (2018). Survey of the state of the art in natural language generation: Core tasks,\\napplications, and evaluation. Journal of Artificial Intelligence Research, 61, 65-170.\\nGupta, S., & Ranjan, R. (2024). Evaluation of LLMs Biases Towards Elite Universities: A Persona-Based\\nExploration. ArXiv. /abs/2407.12801\\nGupta, S., Ranjan, R., & Singh, S. N. (2024). Comprehensive Study on Sentiment Analysis: From\\nRule-based to modern LLM based system. ArXiv. /abs/2409.09989\\nGuu, J., Lee, K., & Pasupat, P. (2020). Retrieval-augmented generation for knowledge-intensive NLP\\ntasks. arXiv preprint. https://arxiv.org/abs/2002.08909\\nGuu, K., Lee, K., Tung, Z., Pasupat, P., & Chang, M. (2020). REALM: Retrieval-augmented language\\nmodel pre-training. In Proceedings of the 37th International Conference on Machine Learning (pp.\\n3929-3938).\\nHan, S., Pool, J., Tran, J., & Dally, W. J. (2015). Learning both weights and connections for efficient\\nneural network. In Advances in Neural Information Processing Systems (pp. 1135-1143).\\nIzacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domain\\nquestion answering. In Proceedings of the 16th Conference of the European Chapter of the Association\\nfor Computational Linguistics: Main Volume (pp. 874-880).\\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Chen, D., Dai, W., Chan, H. S.,\\nMadotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ArXiv.\\nhttps://doi.org/10.1145/3571730\\nKalra, R., Wu, Z., Gulley, A., Hilliard, A., Guan, X., Koshiyama, A., & Treleaven, P. (2024). HyPA-RAG: A\\nHybrid Parameter Adaptive Retrieval-Augmented Generation System for AI Legal and Policy Applications.\\nArXiv. /abs/2409.09046\\nKarpukhin, V., Oguz, B., Min, S., & Yih, W. (2020). Dense passage retrieval for open-domain question\\nanswering. arXiv preprint. https://arxiv.org/abs/2004.04906\\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D. & Yih, W. T. (2020). Dense\\npassage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP) (pp. 6769-6781).\\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Riedel, S. (2020).\\nRetrieval-augmented\\ngeneration\\nfor knowledge-intensive NLP tasks. In Proceedings of the 34th\\nInternational Conference on Neural Information Processing System ( pp. 9459-9474).'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 15}, page_content='Li, C., Liu, Z., Xiao, S., & Shao, Y. (2023). Making Large Language Models A Better Foundation For\\nDense Retrieval. ArXiv. /abs/2312.15503\\nLi, F., Zhu, L., Wang, T., Li, J., Zhang, Z., & Shen, H. T. (2023). Cross-Modal Retrieval: A Systematic\\nReview of Methods and Future Directions. ArXiv. /abs/2308.14263\\nLi, S., Shang, H., Wei, D., Guo, J., Li, Z., He, X., Zhang, M., & Yang, H. (2024). LA-RAG:Enhancing\\nLLM-based ASR Accuracy with Retrieval-Augmented Generation. ArXiv. /abs/2409.08597\\nLi, S., Park, S., Lee, I., & Bastani, O. (2023). TRAQ: Trustworthy Retrieval Augmented Question\\nAnswering via Conformal Prediction. ArXiv. /abs/2307.04642\\nLi, Z., Li, C., Zhang, M., Mei, Q., & Bendersky, M. (2024). Retrieval Augmented Generation or\\nLong-Context LLMs? A Comprehensive Study and Hybrid Approach. ArXiv. /abs/2407.16833\\nLiu, Z., Wang, H., Niu, Z., Wu, H., Che, W., & Liu, T. (2020). Towards Conversational Recommendation\\nover Multi-Type Dialogs. ArXiv. /abs/2005.03954\\nMallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., & Hajishirzi, H. (2022). When Not to Trust\\nLanguage Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. ArXiv.\\n/abs/2212.10511\\nMombaerts, L., Ding, T., Banerjee, A., Felice, F., Taws, J., & Borogovac, T. (2024). Meta Knowledge for\\nRetrieval Augmented Large Language Models. ArXiv. /abs/2408.09017\\nNguyen, X., Pandit, S., Purushwalkam, S., Xu, A., Chen, H., Ming, Y., Ke, Z., Savarese, S., Xong, C., &\\nJoty, S. (2024). SFR-RAG: Towards Contextually Faithful LLMs. ArXiv. /abs/2409.09916\\nNiu, C., Wu, Y., Zhu, J., Xu, S., Shum, K., Zhong, R., Song, J., & Zhang, T. (2023). RAGTruth: A\\nHallucination\\nCorpus\\nfor\\nDeveloping\\nTrustworthy\\nRetrieval-Augmented\\nLanguage\\nModels.\\nArXiv.\\n/abs/2401.00396\\nQian, H., Zhang, P., Liu, Z., Mao, K., & Dou, Z. (2024). MemoRAG: Moving towards Next-Gen RAG Via\\nMemory-Inspired Knowledge Discovery. ArXiv. /abs/2409.05591\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are\\nunsupervised multitask learners. OpenAI Blog, 1(8), 9.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019).\\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. ArXiv. /abs/1910.10683\\nRanade, P., & Joshi, A. (2023). FABULA: Intelligence Report Generation Using Retrieval-Augmented\\nNarrative Construction. ArXiv. https://doi.org/10.1145/3625007.3627505\\nRanjan, R., Gupta, S., & Singh, S. N. (2024). A Comprehensive Survey of Bias in LLMs: Current\\nLandscape and Future Directions. ArXiv. /abs/2409.16430\\nRavuru, C., Sakhinana, S. S., & Runkana, V. (2024). Agentic Retrieval-Augmented Generation for Time\\nSeries Analysis. ArXiv. /abs/2408.14484'),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 16}, page_content=\"Robertson, S.G., & Zaragoza,H., (2009). The Probabilistic Relevance Framework: BM25 and Beyond,\\nFoundations and Trends in Information Retrieval, 3(4), pp. 333-389.\\nRoller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott, M., Shuster, K., Smith, E. M.,\\nBoureau, Y., & Weston, J. (2020). Recipes for building an open-domain chatbot. ArXiv. /abs/2004.13637\\nSalton,\\nG.,\\nWong,\\nA.,\\n&\\nYang,\\nC.\\nS.\\n(1975).\\nA vector space model for automatic indexing.\\nCommunications of the ACM, 18(11), 613-620.\\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller,\\nfaster, cheaper and lighter. ArXiv. /abs/1910.01108\\nSarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., & Manning, C. D. (2024). RAPTOR: Recursive\\nAbstractive Processing for Tree-Organized Retrieval. ArXiv. /abs/2401.18059\\nShi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., & Yih, W.-T. (2023).\\nREPLUG: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652.\\nShrestha, R., Zou, Y., Chen, Q., Li, Z., Xie, Y., & Deng, S. (2024). FairRAG: Fair Human Generation via\\nFair Retrieval Augmentation. ArXiv. /abs/2403.19964\\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. In\\nAdvances in Neural Information Processing Systems (pp. 3104-3112).\\nThakur,\\nN.,\\nBonifacio,\\nL.,\\nZhang,\\nX., Ogundepo, O., Kamalloo, E., Li, X., Liu, Q., Chen, B.,\\nRezagholizadeh, M., & Lin, J. (2023). NoMIRACL: Knowing When You Don't Know for Robust Multilingual\\nRetrieval-Augmented Generation. ArXiv. /abs/2312.11361\\nThakur, N., Reimers, N., Ruckl'e, A., Srivastava, A., & Gurevych, I. (2021). BEIR: A Heterogenous\\nBenchmark for Zero-shot Evaluation of Information Retrieval Models. ArXiv, abs/2104.08663.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I.\\n(2017). Attention is all you need. In Advances in Neural Information Processing Systems (pp. 5998-6008).\\nWang, X., Wang, Z., Gao, X., Zhang, F., Wu, Y., Xu, Z., Shi, T., Wang, Z., Li, S., Qian, Q., Yin, R., Lv, C.,\\nZheng, X., & Huang, X. (2024). Searching for Best Practices in Retrieval-Augmented Generation. ArXiv.\\n/abs/2407.01219\\nWang, Z., Araki, J., Jiang, Z., Parvez, M. R., & Neubig, G. (2023). Learning to Filter Context for\\nRetrieval-Augmented Generation. ArXiv. /abs/2311.08377\\nXia, P., Zhu, K., Li, H., Zhu, H., Li, Y., Li, G., Zhang, L., & Yao, H. (2024). RULE: Reliable Multimodal RAG\\nfor Factuality in Medical Vision Language Models. ArXiv. /abs/2407.05131\\nXie, S., Sun, C., Huang, J., Tu, Z., & Murphy, K. (2017). Rethinking Spatiotemporal Feature Learning:\\nSpeed-Accuracy Trade-offs in Video Classification. ArXiv. /abs/1712.04851\\nXiong, L., Xiong, C., Li, Y., Tang, K., Liu, J., Bennett, P., Ahmed, J., & Overwijk, A. (2020). Approximate\\nNearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. ArXiv. /abs/2007.00808\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m131 Google Docs Renderer', 'creator': '', 'creationdate': '', 'source': '../data/pdf/Comprehensive Survey of RAG.pdf', 'file_path': '../data/pdf/Comprehensive Survey of RAG.pdf', 'total_pages': 18, 'format': 'PDF 1.4', 'title': 'A Comprehensive Review of Retrieval-Augmented Generation (RAG): Key Challenges and Future Directions', 'author': '', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 17}, page_content='Yasunaga, M., Aghajanyan, A., Shi, W., James, R., Leskovec, J., Liang, P., Lewis, M., Zettlemoyer, L., &\\nYih, W. (2022). Retrieval-Augmented Multimodal Language Modeling. ArXiv. /abs/2211.12561\\nZhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia, M., Stoica, I., & Gonzalez, J. E. (2024). RAFT:\\nAdapting Language Model to Domain Specific RAG. ArXiv. /abs/2403.10131\\nZhu, Y., Ren, C., Xie, S., Liu, S., Ji, H., Wang, Z., Sun, T., He, L., Li, Z., Zhu, X., & Pan, C. (2024).\\nREALM: RAG-Driven Enhancement of Multimodal Electronic Health Records Analysis via Large\\nLanguage Models. ArXiv. /abs/2402.07016')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "## load all the text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"**/*.pdf\", ## Pattern to match files  \n",
    "    loader_cls= PyMuPDFLoader, ##loader class to use\n",
    "    show_progress=False\n",
    "\n",
    ")\n",
    "\n",
    "pdf_documents=dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69703a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
